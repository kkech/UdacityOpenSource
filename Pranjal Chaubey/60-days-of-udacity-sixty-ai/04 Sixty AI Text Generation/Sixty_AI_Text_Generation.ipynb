{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Sixty AI - Text Generation",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pranjalchaubey/60-days-of-udacity-sixty-ai/blob/master/04%20Sixty%20AI%20Text%20Generation/Sixty_AI_Text_Generation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q3FoSZ8vZJlz",
        "colab_type": "text"
      },
      "source": [
        "#  Sixty AI - Text Generation\n",
        "\n",
        "<br/> Now that we have trained the OpenAI's GPT-2 Model on our text corpus, we can use this notebook to generate text whenever we want. \n",
        "<br/> We do not have to go through the process of _Finetuning_ it again and again.  \n",
        "<br/> In this notebook, we will load a pretrained GPT-2 model from our Google Drive and use it to generate text. \n",
        "\n",
        "<br/> We are using  _**Max Woolf's**_ `gpt-2-simple` library. For more information about `gpt-2-simple`, you can visit [this GitHub repository](https://github.com/minimaxir/gpt-2-simple).\n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jHXVX58aamCt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Install the GPT-2 Simple library \n",
        "!pip install -q gpt-2-simple\n",
        "\n",
        "# Import Export business \n",
        "import gpt_2_simple as gpt2\n",
        "from datetime import datetime\n",
        "from google.colab import files"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YJSEl6GNbBFe",
        "colab_type": "text"
      },
      "source": [
        "## Downloading GPT-2\n",
        "\n",
        "We have\n",
        "to download the GPT-2 model first. \n",
        "<br/>There are two sizes of GPT-2:\n",
        "\n",
        "* `117M` (default): the \"small\" model, 500MB on disk.\n",
        "* `345M`: the \"medium\" model, 1.5GB on disk.\n",
        "\n",
        "<br/>_**Note :**_ We will be downloading the same _type_ of GPT-2 model that we trained in the previous notebook. \n",
        "<br/>In our case, we trained a GPT-2 _117M_ model. So let's download that!  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QSiucGBlapBA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "outputId": "d62459e6-e4ed-4d54-c9fe-c0fce12dc38d"
      },
      "source": [
        "# model will be saved in the Colaboratory VM at `/models/<model_name>`\n",
        "gpt2.download_gpt2(model_name=\"117M\")"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fetching checkpoint: 1.05Mit [00:00, 558Mit/s]                                                      \n",
            "Fetching encoder.json: 1.05Mit [00:00, 115Mit/s]                                                    \n",
            "Fetching hparams.json: 1.05Mit [00:00, 344Mit/s]                                                    \n",
            "Fetching model.ckpt.data-00000-of-00001: 498Mit [00:04, 111Mit/s]                                   \n",
            "Fetching model.ckpt.index: 1.05Mit [00:00, 354Mit/s]                                                \n",
            "Fetching model.ckpt.meta: 1.05Mit [00:00, 184Mit/s]                                                 \n",
            "Fetching vocab.bpe: 1.05Mit [00:00, 181Mit/s]                                                       \n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cN5Hc8itcrxC",
        "colab_type": "text"
      },
      "source": [
        "## Mounting Google Drive\n",
        "\n",
        "Let's mount our Google Drive in the Colab so that we can fetch the trained model. \n",
        "<br/>In case you're a little skeptical about putting down the Google Drive auth code (you should be!), I suggest you check out what is going on under the hood in the `gpt-2-simple` library. Simply [click this link](https://github.com/minimaxir/gpt-2-simple/blob/master/gpt_2_simple/gpt_2.py \"click this link\"). \n",
        "\n",
        "<br/>TL;DR: _It's Safe!_\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T44pr3JoccCU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "96cc7447-5b26-4bc4-8bbd-517e65e514cc"
      },
      "source": [
        "gpt2.mount_gdrive()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6volP_EMc3BC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Using the default model name \n",
        "model_name = 'run1'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FhcrlxoOdGGG",
        "colab_type": "text"
      },
      "source": [
        "## Load a Trained Model Checkpoint\n",
        "\n",
        "Trained models are saved as `.rar` file. \n",
        "<br/>Copy the `.rar` checkpoint file from the Google Drive into the Colaboratory VM."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vo0n8FUQdA1L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "gpt2.copy_checkpoint_from_gdrive(run_name=model_name)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "meRA-NdCeA6T",
        "colab_type": "text"
      },
      "source": [
        "Load the retrained model checkpoint + metadata necessary to generate text.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HP-b5y44d5Hw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        },
        "outputId": "3ab4d097-6119-4515-ab26-745fbede08ef"
      },
      "source": [
        "sess = gpt2.start_tf_sess()\n",
        "gpt2.load_gpt2(sess, run_name=model_name)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0801 18:36:39.249646 140008411981696 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to check for files with this prefix.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Loading checkpoint checkpoint/run1/model-1000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yeGU8q0oeHgs",
        "colab_type": "text"
      },
      "source": [
        "## Generate Text!!!\n",
        "\n",
        "You know what you need to do üòâ"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "629Httq4eDU5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "1923d0a3-5cc6-4dae-d838-1f39bdbaa40e"
      },
      "source": [
        "# Yes, I just put an emoji in the Jupyter Notebook Markdown üòé\n",
        "\n",
        "'''\n",
        "prefix: force the text to start with a given character sequence and generate \n",
        "  text from there \n",
        "nsamples: generate multiple texts at a time\n",
        "batch_size: generate multiple samples in parallel, giving a massive speedup \n",
        "  (in Colaboratory, set a maximum of 20 for batch_size)\n",
        "length: Number of tokens to generate (default 1023, the maximum)\n",
        "temperature: The higher the temperature, the crazier the text \n",
        "  (default 0.7, recommended to keep between 0.7 and 1.0)\n",
        "top_k: Limits the generated guesses to the top k guesses \n",
        "  (default 0 which disables the behavior; if the generated output is \n",
        "  super crazy, you may want to set top_k=40)\n",
        "top_p: Nucleus sampling: limits the generated guesses to a \n",
        "  cumulative probability. (gets good results on a dataset with top_p=0.9)\n",
        "truncate: Truncates the input text until a given sequence, excluding that\n",
        "  sequence (e.g. if truncate='<|endoftext|>', the returned text will include \n",
        "  everything before the first <|endoftext|>). It may be useful to combine this\n",
        "  with a smaller length if the input texts are short.\n",
        "include_prefix: If using truncate and include_prefix=False, the specified \n",
        "  prefix will not be included in the returned text.\n",
        "'''\n",
        "\n",
        "\n",
        "gpt2.generate(sess,\n",
        "              length=512,\n",
        "              temperature=0.8,\n",
        "              prefix=None,\n",
        "              nsamples=10,\n",
        "              batch_size=10\n",
        "             )"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\"hi, kid, just starting in 6th grade. i‚Äôm starting in 1st grade. i taught my class that digit recognition is actually slightly slow, but that‚Äôs nothing to take---‚Äùyou‚Äô‚Äô‚Äô‚Äùcan‚Äôt wait, or learn a new tool for this skill!‚Äù\" #60daysofudacity <|endoftext|>\n",
            "<|startoftext|> day 4 (july 3rd) #60daysofudacity 1. started with lesson 6: \"deep learning with pytorch\" 2. worked on my project: project demo for \"intro to pytorch\" 3. read \"matplotlib free\" paper 4.i encourage @lexie, @dishingoyani1996 and @elimumichael9 to go forward with the challenge <|endoftext|>\n",
            "<|startoftext|> day 4 : \"ai is the new electricity\" :bananadance: :nerd_face: :female-technologist: <|endoftext|>\n",
            "<|startoftext|> day 2: #60daysofudacity 1. completed lesson 3 2. revised lesson 2 3. i would like to encourage @harshima.u and @ayushjain.tea to give their best in the challenge. <|endoftext|>\n",
            "<|startoftext|> day 4: #60daysofudacity - completed videos \"introducing local and global differential privacy\" - learned about pate analysis - completed lesson 4 of spai course <|endoftext|>\n",
            "<|startoftext|> day 4: 1. completed lesson 3 (differentially  privacy :classifying:privatea) of this course 2. completed lesson 4 of this course 3. read articles about backpropagation, formal definition of differential privacy <|endoftext|>\n",
            "<|startoftext|> #day4 - #60daysofudacity -took the quiz - had a virtual meet up with the community -read an article about effective back propagation -started working on my project of *federated learning* i encourage @jule.akimoto@jule.akimoto@priya97a@elimumichael9 to complete this challenge <|endoftext|>\n",
            "<|startoftext|> day 2 of #60daysofudacity\n",
            "====================\n",
            "<|startoftext|> *day 11* of #60daysofudacity 1. filled the \"create a differentially private query\" form. 2. learned about alexnet, pysyft and what it does and how it works. 3. took a quiz regarding the basics of data analysis on the #sg_novice-ai channel. 4. created a spreadsheet on how to analyze data in gcp/ml/ai. <|endoftext|>\n",
            "<|startoftext|> day 11: #60daysofudacity 1. studied chapter 1 of deep learning by ian goodfellow 2. completed lesson 4 of the course 3. started reading the gan paper 4. read through its code 5. iptab for yesterday's #busy_or_overwhelmed see images on project #sg_wrldwde-women-cdrs with @sanskriti.bajaj98@evigiannakou@shashi.gharti@aysha.kamal7 :male-technologist: i was also at #sg_project-t-shirt if you're not me @evigiannakou@shashi.gharti@aysha.kamal7@ivolinengong@sarahhelena.barmer@learn556 <|endoftext|>\n",
            "<|startoftext|> *day 9* of #60daysofudacity : - learn as a beginner how to do pandas library for kaggle projects. - find and understand the basic of kaggle project with examples and code written in python. <|endoftext|>\n",
            "<|startoftext|> day 7 of 60: started with the lesson 7 (differential privacy) <|endoftext|>\n",
            "<|startoftext|> day 7: garbage collection in pysyft (started) encouraging @fabiana.clmnt@ziad.esam.ezat to take part in #60daysofudacity <|endoftext|>\n",
            "<|startoftext|> *day 11* :torch_heart_big:   :torch_heart_big: *day 10*: completed lesson 6: differential privacy for deep learning  :torch_heart_big: *day : 9* :torch_heart_big: participated in the *boom with the basics*\n",
            "====================\n",
            "<|startoftext|> day 1: 1. i took the pledge 2. finished lesson 1 3. rewatched videos from lesson 2 from youtube. <|endoftext|>\n",
            "<|startoftext|> day 1: took the pledge completed lesson 4 also started reading paper for a job interview in kaggle attended virtual meet up organised by the team @intisarnaheen@sharmin.cse1.bu :female-student: happy 4th of july to be selected as a guest lecturer on #60daysofudacity@abhishek.sawala.sharma.riday. i would like to encourage @khandelwalcibaca@dishaishoswapna to take the pledge and join us in the challenge <|endoftext|>\n",
            "<|startoftext|> *day 1:* : 1. pledge taken 2. finished lesson 2 of andrew ng's machine learning course on coursera( was an eye opening introduction for myself in deep learning/ai) 3. started reading the next chapter \"deep learning with pytorch\"course link:  <|endoftext|>\n",
            "<|startoftext|> day 1: completed till lesson 4 full time !! <|endoftext|>\n",
            "<|startoftext|> *day 1:* 1. pledged to the #60daysofudacity 2. completed the first 3 videos of introducing differential privacy in the secure and private ai scholarship challenge mlnd lecture series. <|endoftext|>\n",
            "<|startoftext|> day 1: took the pledge completed lesson 4 completed the first 3 videos of lesson 5 had a virtual meet up with @sukhwal.niki@puja.hit01@shivamraisharma@sivainfotech looking to advance our learning in a study group !!! :penguin_dance: :bat_parrot: <|endoftext|>\n",
            "<|startoftext|> day 1 took the pledge: read some resources related to nlp and working on the dogs/cats kaggle project notebook. <|endoftext|>\n",
            "<|startoftext|> i just pledged #60daysofudacity  :penguin_dance: :bat_parrot: <|endoftext|>\n",
            "<|startoftext|> day 1 took the\n",
            "====================\n",
            "text | edited | 2015-06-21 3:06pm ------------------------------------- i am a bit late to start this challenge but i am so grateful to have started in the past! i am doing my best to be active and contribute in this challenge. i also joined a study group and i am studying some datasets/datasets that will help solve the problems i am facing. i have also started working on the interview questions to be asked during the interview day. i also encourage my wonderful encouragement friends @learn556@mikaelaysanchez@evigiannakou@learn556@michael092001 to keep up to pace. also @ewotawa@chocolate.coffee@khandelwalcibaca@sarahhelena.barmer@d.pandey11585 <|endoftext|>\n",
            "<|startoftext|> day 10: 1. i updated my #60daysofudacity project in github:  2. i finished lesson 6.6 and started working on the final project 6.7 which is going to be implemented using pytorch. <|endoftext|>\n",
            "<|startoftext|> day 7: completed #l5_diff_privacy_dl. my overall progress for this challenge is 100%. so, now i'm focused on the remaining 5 concepts. i finished the topics from lesson 1, part 5, 'evaluating the privacy of a function'. i also completed the first chapter from the algorithmic foundations of differential privacy book. i also completed the second chapter from the deep learning book. i also completed the third chapter from the deep learning book. i also created my github repo for this challenge. i'm tagging @davida.mikhail<|endoftext|>\"if only we could make a difference\" -    @evalatagworking  i forgot to take notes during coding, never mind take initiative . every day is a learning opportunity for us all.@george.christ1987@arkachkrbrty@manishajhunjhunwala7  keep up the good work. are you ready to lead this group? <|endoftext|>\n",
            "<|startoftext|> day 8: 1. i am still working on lesson 6 project 2. i have written a medium article on differential privacy 3. i read pytorch documentation on lightweight tensors 4. i read pysyft documentation on ruby on rails <|endoftext|>\n",
            "\n",
            "====================\n",
            "> #60daysofudacity privacy  function :   :after_the_house_of_stars: :udacity: <|endoftext|>\n",
            "<|startoftext|> day 11 of #60daysofudacity : 1) continued with deep learning with pytorch 2) finished the remaining 3 chapters of the course. :female-student: 3) continued with the lesson 2 notebooks of deep learning with pytorch. :pytorch: encouraging @sol.vriksh@nishant.bharatindia@sfmajors373 to take this challenge. <|endoftext|>\n",
            "<|startoftext|> *day 2* - completed till *deep learning with pytorch* section 4 - completed 2 more steps and coded a second *image recognition and classification* notebook - continuing the *deep learning with differential privacy* lesson :female-student: i encourage @casabiancadenny and @fridarode00 today <|endoftext|>\n",
            "<|startoftext|> day 11: completed four parts in lesson 7: securing federated learning. also completed the first three projects of lesson 9. also completed the project: federated learning #60daysofudacity <|endoftext|>\n",
            "<|startoftext|> day 11 of #60daysofudacity 1) completed the first 5 chapters in lesson 2 2) started with the differential privacy course 3) exploring deep learning theories 4) working on a kaggle project <|endoftext|>\n",
            "<|startoftext|> hi everyone! i couldn't be more active in the community this past weekend, so here's a link to meet-up resource i got through slack:   i would like to start a thread if anyone has questions or needs help about in-app purchasing or other types of smartwatches. <|endoftext|>\n",
            "<|startoftext|> *day 11* of #60daysofudacity : - continued with the lesson 5: introduction to differential privacy - started working on the project: build an encrypted database <|endoftext|>\n",
            "<|startoftext|> day 8 of #60daysofudacity 1) complete 4 lessons from intro to deep learning using pytorch 2)increased my accumulation of pending pending messages in google alert history for the #sg_cognitive-learners\n",
            "====================\n",
            "<|startoftext|> day 3: i am starting again lesson 2, but i am concerned about the numbering of assignments. maybe it is website admin day and all i have to do is change the github repo to github.org days of code <|endoftext|>\n",
            "<|startoftext|> day 4 #60daysofudacity 1. completed lesson 5 2. attended k-pop meetup . <|endoftext|>\n",
            "<|startoftext|> day 4: 1) finished lesson 4 2) started lesson 5 3) discussing with @jenashubhangi20 about our project. 4) completed solving all the exercises in lesson 5, i am starting to make notes for upcoming lessons. <|endoftext|>\n",
            "<|startoftext|> *day 4*  - completed lesson 2 part 1 - *\"differentially private federated learning: a client level perspective\"* { \"cells\": [ { \"cell_type\": \"markdown\", \"metadata\": {}, \"source\": [ \"## lesson: toy federated learning\" ] }, { \"cell_type\": \"markdown\", \"metadata\": {}, \"source\": [ \"the goal of the lesson is to introduce a toy example of how federated learning can be done, in a way similar to the original udacity model.\" ] }, { \"cell_type\": \"markdown\", \"metadata\": {}, \"source\": [ \"the lesson description and the notes related to it are in the repo\\n\", \"\\n\", \"## i.f, say you're a data scientist working in a data analytics application, you may be wondering how you can integrate federated learning with the actual data analyst. well, that's simple enough. simply put, the toy example will analyze the data analyst query and create a pointer to the actual dataset. this will then be passed to a worker which represents the analyst.\\n\", \"       each time the worker performs the query, a new record is created.\\n\", \"       each time the worker performs the update, a new record is created.\\n\", \"\\n\", \"       each time the worker updates the dataset, a new record is created.\\n\", \"\\n\", \"       each time the worker updates the database, a new record is created.\\n\", \"\\n\", \"   \n",
            "====================\n",
            "<|startoftext|> day 3: 1. finished lesson 4 2. completed the first project of lesson 5 3. started with lesson 6 4. had a virtual meet up with awesome women who code awesome@sarahhelena.barmer@mahak.bansal97@seeratpal91@obijoyce <|endoftext|>\n",
            "<|startoftext|> day 4: 1. completed lesson 4 2. revision the notes i have taken in lesson 2 3. joined first virtual meet up with #wmn_who_code#60daysofudacity <|endoftext|>\n",
            "<|startoftext|> day 4 of 60-something : today i worked on my project and i was working on a wedding <|endoftext|>\n",
            "<|startoftext|> *day 3/60 [june 30, 2019]*#60daysofudacity completed lesson 3 of intro to deep learning with pytorch course by andrew ng; learned more about activation functions in pytorch [intro to deep learning with pytorch] learned about two different types of softmax function: softmax and overfitting successfully fit multiple models implemented ai-style classification on images [convolutional neural networks (cnn) for easy readability [u-value discrete logistic regression]  revised the basic knowledge of pytorch [deep learning with differential privacy] *day 4* [july 30, 2019]#60daysofudacity completed lesson 2 of intro to deep learning with pytorch course by andrew ng; completed 2 more courses from udemy; read more about pytorch [federated learning] *day 5* [july 30, 2019]#60daysofudacity completed lesson 3 of intro to deep learning with pytorch course by andrew ng; completed the following 2 quizzes: ‚Ä¢ answered all of the questions asked in the challenge: federated learning   ‚Ä¢ i have also added information about studying differential privacy:  two videos from lesson 2: deep learning with pytorch: learning differential privacy (softmax function) collaborated with @sabrinajodexnis and @sauravkumarsct on a project:  a collaborative work language for computer vision and deep learning. used in: python and deep learning <|endoftext|>\n",
            "<|startoftext|> #60daysofud\n",
            "====================\n",
            "\"male starstruck:'s going to be the best time of your career\" - health ecigsuity accession number 11560 (digit 12). <|endoftext|>\n",
            "<|startoftext|> day 11: continued with federated learning. revised lesson 2 encrypted deep learning. <|endoftext|>\n",
            "<|startoftext|> day 9: completed lesson 4, started with intro to additive secret sharing. <|endoftext|>\n",
            "<|startoftext|> day 11: - completed till lesson 5 - training a network with transfer learning. - working on a toy class project. <|endoftext|>\n",
            "<|startoftext|> day 11 - #60daysofudacity review lesson 3 - varying amount of noise - joined the first challenge @arkachkrbrty <|endoftext|>\n",
            "<|startoftext|> day 11: 1. got started with nlp with tensorflow 2. reviewing videos of lesson 6 (intro to deep learning) 3. working on my capstone project. 4. had a very productive i/o session with #sg_wrldwde-women-cdrs encouraging @sarahhelena.barmer@jenashubhangi20@minassouane@sharmin.cse1.bu@vincivenv@ashishiva3@sankalpdayal5@adventuroussrv@hawk.praxs533@moshi.iit@ikhushpatel@munniomer and many more. <|endoftext|>\n",
            "<|startoftext|> day 11: 1. completed till 4.7. 2. working slowly on pate analysis. <|endoftext|>\n",
            "<|startoftext|> day 9 of #60daysofudacity: - restarted the deep learning with pytorch course. - continued reading chapter 2 of the book \"the algorithmic foundations of differential privacy\" (i read it twice) - followed the suggested reading by google cloud, i was able to make some progress. i have already started with the intro to dl with pytorch course. i will continue tomorrow. - read \"the algorithmic foundations of differential privacy\" (i almost finished it!) - read \"chapter 1: the promise of differential privacy\" by cynthia dwork <|endoftext|>\n",
            "<|start\n",
            "====================\n",
            "by\n",
            "\n",
            "link to the #60daysofudacity report :   from:andreiliphd@eagle_phoenix2@labknr98 to:labknr98@alexdomla <|endoftext|>\n",
            "\n",
            "<|startoftext|> day 8:  #60daysofudacity 1. finished the lesson 5. 2. started pate analysis. 3. worked on the paper on dp for machine learning. 4. started reading the algorithmic foundation of differential privacy. <|endoftext|>\n",
            "<|startoftext|> *day 9* of #60daysofudacity : -watched videos 1-10 from lesson 2: intro to differential privacy -watched videos from lesson 2: deep learning with pytorch -watched some youtube videos about local and global diff privacy and finally implemented a diff privacy algorithm -watched some videos about improving performance of my apn model:   <|endoftext|>\n",
            "<|startoftext|> *day 9:* ‚Ä¢ i continued working on the course. ‚Ä¢ i spent about an hour with the udacity *day 8*: 1. i worked and did research on some differential privacy concepts. 2. i worked and did research and implemented some federated learning project. ‚Ä¢ i continued working on the course. ‚Ä¢ i spent an hour with the udacity *titanic*: 1. i solved some math problem involving ml and dl. 2. i solved some math problem involving data analysis. ‚Ä¢ i continued working on the course. ‚Ä¢ i spent an hour with the udacity *day 8*: i watched videos on lesson 5, learned about some common errors in ml and dl, and pate analysis. 3. i watched some mathematical videos on lesson 5. <|endoftext|>\n",
            "<|startoftext|> #day8 completed!  1. learned more about matrices, fixed precision encoding, conversion, vectorization, and padding. 2. learned about data points, weights, and separable separables in a proper way. 3. read about a new library,   for assignment critics : \"mathagor‚Ä¶  <|endoftext|>\n",
            "<|startoftext|> day 9 of #60daysofudacity: 1. completed lesson 6 2. started notebook part 3 of \"intro to dl with pytorch\" 3. started with the\n",
            "====================\n",
            "> @arkachkrbrty@ash3ax@manishajhunjhunwala7 <|endoftext|>\n",
            "<|startoftext|> day 2: started lesson 2 - \"differential privacy\" link ‚Ä¢ made a small revision on the notes and revised lesson 2 for 5-7 days ‚Ä¢ started a new motivation to rebuild my baseline for differential privacy. <|endoftext|>\n",
            "<|startoftext|> day 2: 1. started lesson 6: differential privacy for deep learning 2. completed my notes of lesson 6: differential privacy for deep learning. 3. would like to encourage @abhinandalfio scheme for ml course, and @sabrina.palis  to start the challenge. <|endoftext|>\n",
            "<|startoftext|> day #2 #60daysofudacity completed lesson 1 completed lectures of lesson 2 completed 3. read more in differential privacy by cynthia dwork <|endoftext|>\n",
            "<|startoftext|> day1 - #60daysofudacity - finished the lesson 4! - took some notes on transfer learning <|endoftext|>\n",
            "<|startoftext|> great to have you @vivekspace94 :star-struck: keep up the excellent job in the wonderful initiative  <|endoftext|>\n",
            "<|startoftext|> *day 2:* 1. studied about data preparation 2. implemented project 3 of lesson 5 i would like to encourage @mahmoudsir@shari.ghoddadur@djnavin619@riday467366@muhammadsafitya07@sharmin.cse1.bu@mhmohona to do their best! <|endoftext|>\n",
            "<|startoftext|> day 2: 1. completed my first phase of coding: 1. gone through some of the kaggle kernels 2. read some articles on differential privacy 3. used local machine learning on the mnist dataset to learn more about ml <|endoftext|>\n",
            "<|startoftext|> #60daysofudacity *day 2* 1. worked on the secure and private ai scholarship challenge - applied a neural network to a toy dataset of rlstars2languagejupyter notebookusing cnns to predict medical and scientific diagnoses using deep learning. <|endoftext|\n",
            "====================\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rz2tjc8MfFPm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}