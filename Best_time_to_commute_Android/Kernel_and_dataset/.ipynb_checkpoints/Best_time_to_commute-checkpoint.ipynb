{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the libraries\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch as torch  \n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "#import syft as sy\n",
    "#from torch.utils import data\n",
    "#from syft.frameworks.torch.federated import utils\n",
    "#from syft.workers import WebsocketClientWorker\n",
    "#from syft.workers import VirtualWorker\n",
    " \n",
    "LOCATION_OF_DATA_FOR_TRAINING = 'time_taken_home_to_office - data.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.read_csv(LOCATION_OF_DATA_FOR_TRAINING, header=0, usecols=['day','start_time', 'is_any_road_blocked_in_between', 'mode_of_transport'])\n",
    " \n",
    "# transform all coloumns in X to numerical data and one-hot encode\n",
    "# We don't need feature scaling as the code below takes care of it as well\n",
    "X = pd.get_dummies(X, columns=['day','start_time', 'is_any_road_blocked_in_between', 'mode_of_transport']).values\n",
    " \n",
    "Y = pd.read_csv(LOCATION_OF_DATA_FOR_TRAINING, header=0, usecols=['time_taken']).values\n",
    " \n",
    "# Splitting the dataset into the Training set and Test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size = 0.25, random_state = 0)\n",
    " \n",
    "# get number of columns in training data\n",
    "# which means how many variables you have for training\n",
    "n_cols = X_train.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# Class for making the Neural Network\n",
    " \n",
    "# Make a regressor with three hidden layers\n",
    "class Regressor(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Regressor, self).__init__()\n",
    "        self.fc1 = nn.Linear(n_cols, 10)\n",
    "        self.fc2 = nn.Linear(10, 10)\n",
    "        self.fc3 = nn.Linear(10, 10)\n",
    "        self.fc4 = nn.Linear(10, 1)\n",
    " \n",
    "        #self.dropout = nn.Dropout(p=0.1)\n",
    " \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = F.relu(self.fc4(x))\n",
    " \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#some federated learning there in future\n",
    "#hook = sy.TorchHook(torch)\n",
    "#alice = sy.VirtualWorker(hook, id=\"alice\")\n",
    "#bob = sy.VirtualWorker(hook, id=\"bob\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.512.. \n",
      "Training Loss: 1.014.. \n",
      "Training Loss: 1.481.. \n",
      "Training Loss: 1.950.. \n",
      "Training Loss: 2.416.. \n",
      "Training Loss: 2.865.. \n",
      "Training Loss: 3.320.. \n",
      "Training Loss: 3.764.. \n",
      "Training Loss: 4.206.. \n",
      "Training Loss: 4.623.. \n",
      "Model Accuracy: -328.139.. \n",
      "Epoch: 1/300.. \n",
      "Training Loss: 0.424.. \n",
      "Training Loss: 0.855.. \n",
      "Training Loss: 1.262.. \n",
      "Training Loss: 1.678.. \n",
      "Training Loss: 2.096.. \n",
      "Training Loss: 2.502.. \n",
      "Training Loss: 2.917.. \n",
      "Training Loss: 3.324.. \n",
      "Training Loss: 3.733.. \n",
      "Training Loss: 4.117.. \n",
      "Model Accuracy: -297.970.. \n",
      "Epoch: 2/300.. \n",
      "Training Loss: 0.393.. \n",
      "Training Loss: 0.796.. \n",
      "Training Loss: 1.176.. \n",
      "Training Loss: 1.566.. \n",
      "Training Loss: 1.960.. \n",
      "Training Loss: 2.343.. \n",
      "Training Loss: 2.735.. \n",
      "Training Loss: 3.121.. \n",
      "Training Loss: 3.508.. \n",
      "Training Loss: 3.873.. \n",
      "Model Accuracy: -278.580.. \n",
      "Epoch: 3/300.. \n",
      "Training Loss: 0.374.. \n",
      "Training Loss: 0.758.. \n",
      "Training Loss: 1.120.. \n",
      "Training Loss: 1.492.. \n",
      "Training Loss: 1.869.. \n",
      "Training Loss: 2.235.. \n",
      "Training Loss: 2.611.. \n",
      "Training Loss: 2.981.. \n",
      "Training Loss: 3.353.. \n",
      "Training Loss: 3.703.. \n",
      "Model Accuracy: -263.972.. \n",
      "Epoch: 4/300.. \n",
      "Training Loss: 0.359.. \n",
      "Training Loss: 0.729.. \n",
      "Training Loss: 1.077.. \n",
      "Training Loss: 1.436.. \n",
      "Training Loss: 1.799.. \n",
      "Training Loss: 2.152.. \n",
      "Training Loss: 2.516.. \n",
      "Training Loss: 2.873.. \n",
      "Training Loss: 3.232.. \n",
      "Training Loss: 3.570.. \n",
      "Model Accuracy: -252.058.. \n",
      "Epoch: 5/300.. \n",
      "Training Loss: 0.347.. \n",
      "Training Loss: 0.705.. \n",
      "Training Loss: 1.042.. \n",
      "Training Loss: 1.390.. \n",
      "Training Loss: 1.741.. \n",
      "Training Loss: 2.084.. \n",
      "Training Loss: 2.436.. \n",
      "Training Loss: 2.783.. \n",
      "Training Loss: 3.132.. \n",
      "Training Loss: 3.459.. \n",
      "Model Accuracy: -241.875.. \n",
      "Epoch: 6/300.. \n",
      "Training Loss: 0.337.. \n",
      "Training Loss: 0.685.. \n",
      "Training Loss: 1.012.. \n",
      "Training Loss: 1.350.. \n",
      "Training Loss: 1.692.. \n",
      "Training Loss: 2.025.. \n",
      "Training Loss: 2.368.. \n",
      "Training Loss: 2.705.. \n",
      "Training Loss: 3.045.. \n",
      "Training Loss: 3.363.. \n",
      "Model Accuracy: -232.898.. \n",
      "Epoch: 7/300.. \n",
      "Training Loss: 0.328.. \n",
      "Training Loss: 0.667.. \n",
      "Training Loss: 0.985.. \n",
      "Training Loss: 1.314.. \n",
      "Training Loss: 1.648.. \n",
      "Training Loss: 1.972.. \n",
      "Training Loss: 2.307.. \n",
      "Training Loss: 2.636.. \n",
      "Training Loss: 2.967.. \n",
      "Training Loss: 3.277.. \n",
      "Model Accuracy: -224.802.. \n",
      "Epoch: 8/300.. \n",
      "Training Loss: 0.320.. \n",
      "Training Loss: 0.651.. \n",
      "Training Loss: 0.961.. \n",
      "Training Loss: 1.282.. \n",
      "Training Loss: 1.608.. \n",
      "Training Loss: 1.925.. \n",
      "Training Loss: 2.252.. \n",
      "Training Loss: 2.573.. \n",
      "Training Loss: 2.897.. \n",
      "Training Loss: 3.200.. \n",
      "Model Accuracy: -217.377.. \n",
      "Epoch: 9/300.. \n",
      "Training Loss: 0.312.. \n",
      "Training Loss: 0.636.. \n",
      "Training Loss: 0.939.. \n",
      "Training Loss: 1.253.. \n",
      "Training Loss: 1.571.. \n",
      "Training Loss: 1.881.. \n",
      "Training Loss: 2.201.. \n",
      "Training Loss: 2.515.. \n",
      "Training Loss: 2.832.. \n",
      "Training Loss: 3.128.. \n",
      "Model Accuracy: -210.492.. \n",
      "Epoch: 10/300.. \n",
      "Training Loss: 0.305.. \n",
      "Training Loss: 0.623.. \n",
      "Training Loss: 0.918.. \n",
      "Training Loss: 1.226.. \n",
      "Training Loss: 1.537.. \n",
      "Training Loss: 1.840.. \n",
      "Training Loss: 2.154.. \n",
      "Training Loss: 2.461.. \n",
      "Training Loss: 2.772.. \n",
      "Training Loss: 3.061.. \n",
      "Model Accuracy: -204.039.. \n",
      "Epoch: 11/300.. \n",
      "Training Loss: 0.299.. \n",
      "Training Loss: 0.610.. \n",
      "Training Loss: 0.899.. \n",
      "Training Loss: 1.200.. \n",
      "Training Loss: 1.506.. \n",
      "Training Loss: 1.802.. \n",
      "Training Loss: 2.109.. \n",
      "Training Loss: 2.411.. \n",
      "Training Loss: 2.715.. \n",
      "Training Loss: 2.998.. \n",
      "Model Accuracy: -197.943.. \n",
      "Epoch: 12/300.. \n",
      "Training Loss: 0.293.. \n",
      "Training Loss: 0.598.. \n",
      "Training Loss: 0.881.. \n",
      "Training Loss: 1.176.. \n",
      "Training Loss: 1.475.. \n",
      "Training Loss: 1.766.. \n",
      "Training Loss: 2.067.. \n",
      "Training Loss: 2.363.. \n",
      "Training Loss: 2.661.. \n",
      "Training Loss: 2.939.. \n",
      "Model Accuracy: -192.145.. \n",
      "Epoch: 13/300.. \n",
      "Training Loss: 0.287.. \n",
      "Training Loss: 0.586.. \n",
      "Training Loss: 0.864.. \n",
      "Training Loss: 1.153.. \n",
      "Training Loss: 1.447.. \n",
      "Training Loss: 1.732.. \n",
      "Training Loss: 2.027.. \n",
      "Training Loss: 2.317.. \n",
      "Training Loss: 2.610.. \n",
      "Training Loss: 2.882.. \n",
      "Model Accuracy: -186.601.. \n",
      "Epoch: 14/300.. \n",
      "Training Loss: 0.281.. \n",
      "Training Loss: 0.575.. \n",
      "Training Loss: 0.847.. \n",
      "Training Loss: 1.131.. \n",
      "Training Loss: 1.419.. \n",
      "Training Loss: 1.699.. \n",
      "Training Loss: 1.989.. \n",
      "Training Loss: 2.273.. \n",
      "Training Loss: 2.561.. \n",
      "Training Loss: 2.827.. \n",
      "Model Accuracy: -181.276.. \n",
      "Epoch: 15/300.. \n",
      "Training Loss: 0.276.. \n",
      "Training Loss: 0.564.. \n",
      "Training Loss: 0.831.. \n",
      "Training Loss: 1.110.. \n",
      "Training Loss: 1.393.. \n",
      "Training Loss: 1.667.. \n",
      "Training Loss: 1.952.. \n",
      "Training Loss: 2.231.. \n",
      "Training Loss: 2.514.. \n",
      "Training Loss: 2.775.. \n",
      "Model Accuracy: -176.141.. \n",
      "Epoch: 16/300.. \n",
      "Training Loss: 0.271.. \n",
      "Training Loss: 0.554.. \n",
      "Training Loss: 0.816.. \n",
      "Training Loss: 1.089.. \n",
      "Training Loss: 1.367.. \n",
      "Training Loss: 1.637.. \n",
      "Training Loss: 1.916.. \n",
      "Training Loss: 2.191.. \n",
      "Training Loss: 2.468.. \n",
      "Training Loss: 2.724.. \n",
      "Model Accuracy: -171.169.. \n",
      "Epoch: 17/300.. \n",
      "Training Loss: 0.266.. \n",
      "Training Loss: 0.544.. \n",
      "Training Loss: 0.801.. \n",
      "Training Loss: 1.069.. \n",
      "Training Loss: 1.342.. \n",
      "Training Loss: 1.607.. \n",
      "Training Loss: 1.882.. \n",
      "Training Loss: 2.151.. \n",
      "Training Loss: 2.424.. \n",
      "Training Loss: 2.675.. \n",
      "Model Accuracy: -166.339.. \n",
      "Epoch: 18/300.. \n",
      "Training Loss: 0.261.. \n",
      "Training Loss: 0.534.. \n",
      "Training Loss: 0.786.. \n",
      "Training Loss: 1.050.. \n",
      "Training Loss: 1.318.. \n",
      "Training Loss: 1.578.. \n",
      "Training Loss: 1.848.. \n",
      "Training Loss: 2.113.. \n",
      "Training Loss: 2.381.. \n",
      "Training Loss: 2.627.. \n",
      "Model Accuracy: -161.633.. \n",
      "Epoch: 19/300.. \n",
      "Training Loss: 0.256.. \n",
      "Training Loss: 0.525.. \n",
      "Training Loss: 0.772.. \n",
      "Training Loss: 1.031.. \n",
      "Training Loss: 1.295.. \n",
      "Training Loss: 1.550.. \n",
      "Training Loss: 1.816.. \n",
      "Training Loss: 2.076.. \n",
      "Training Loss: 2.339.. \n",
      "Training Loss: 2.581.. \n",
      "Model Accuracy: -157.037.. \n",
      "Epoch: 20/300.. \n",
      "Training Loss: 0.252.. \n",
      "Training Loss: 0.516.. \n",
      "Training Loss: 0.759.. \n",
      "Training Loss: 1.013.. \n",
      "Training Loss: 1.272.. \n",
      "Training Loss: 1.522.. \n",
      "Training Loss: 1.784.. \n",
      "Training Loss: 2.039.. \n",
      "Training Loss: 2.298.. \n",
      "Training Loss: 2.535.. \n",
      "Model Accuracy: -152.541.. \n",
      "Epoch: 21/300.. \n",
      "Training Loss: 0.247.. \n",
      "Training Loss: 0.506.. \n",
      "Training Loss: 0.745.. \n",
      "Training Loss: 0.995.. \n",
      "Training Loss: 1.249.. \n",
      "Training Loss: 1.496.. \n",
      "Training Loss: 1.752.. \n",
      "Training Loss: 2.003.. \n",
      "Training Loss: 2.257.. \n",
      "Training Loss: 2.490.. \n",
      "Model Accuracy: -148.129.. \n",
      "Epoch: 22/300.. \n",
      "Training Loss: 0.243.. \n",
      "Training Loss: 0.498.. \n",
      "Training Loss: 0.732.. \n",
      "Training Loss: 0.977.. \n",
      "Training Loss: 1.227.. \n",
      "Training Loss: 1.469.. \n",
      "Training Loss: 1.721.. \n",
      "Training Loss: 1.968.. \n",
      "Training Loss: 2.218.. \n",
      "Training Loss: 2.447.. \n",
      "Model Accuracy: -143.790.. \n",
      "Epoch: 23/300.. \n",
      "Training Loss: 0.238.. \n",
      "Training Loss: 0.489.. \n",
      "Training Loss: 0.719.. \n",
      "Training Loss: 0.960.. \n",
      "Training Loss: 1.206.. \n",
      "Training Loss: 1.443.. \n",
      "Training Loss: 1.691.. \n",
      "Training Loss: 1.933.. \n",
      "Training Loss: 2.179.. \n",
      "Training Loss: 2.403.. \n",
      "Model Accuracy: -139.510.. \n",
      "Epoch: 24/300.. \n",
      "Training Loss: 0.234.. \n",
      "Training Loss: 0.480.. \n",
      "Training Loss: 0.706.. \n",
      "Training Loss: 0.943.. \n",
      "Training Loss: 1.184.. \n",
      "Training Loss: 1.418.. \n",
      "Training Loss: 1.661.. \n",
      "Training Loss: 1.899.. \n",
      "Training Loss: 2.141.. \n",
      "Training Loss: 2.361.. \n",
      "Model Accuracy: -135.282.. \n",
      "Epoch: 25/300.. \n",
      "Training Loss: 0.230.. \n",
      "Training Loss: 0.472.. \n",
      "Training Loss: 0.693.. \n",
      "Training Loss: 0.926.. \n",
      "Training Loss: 1.163.. \n",
      "Training Loss: 1.392.. \n",
      "Training Loss: 1.632.. \n",
      "Training Loss: 1.865.. \n",
      "Training Loss: 2.103.. \n",
      "Training Loss: 2.319.. \n",
      "Model Accuracy: -131.102.. \n",
      "Epoch: 26/300.. \n",
      "Training Loss: 0.225.. \n",
      "Training Loss: 0.463.. \n",
      "Training Loss: 0.681.. \n",
      "Training Loss: 0.909.. \n",
      "Training Loss: 1.142.. \n",
      "Training Loss: 1.367.. \n",
      "Training Loss: 1.602.. \n",
      "Training Loss: 1.832.. \n",
      "Training Loss: 2.065.. \n",
      "Training Loss: 2.277.. \n",
      "Model Accuracy: -126.960.. \n",
      "Epoch: 27/300.. \n",
      "Training Loss: 0.221.. \n",
      "Training Loss: 0.455.. \n",
      "Training Loss: 0.668.. \n",
      "Training Loss: 0.892.. \n",
      "Training Loss: 1.121.. \n",
      "Training Loss: 1.342.. \n",
      "Training Loss: 1.573.. \n",
      "Training Loss: 1.799.. \n",
      "Training Loss: 2.028.. \n",
      "Training Loss: 2.235.. \n",
      "Model Accuracy: -122.843.. \n",
      "Epoch: 28/300.. \n",
      "Training Loss: 0.217.. \n",
      "Training Loss: 0.447.. \n",
      "Training Loss: 0.656.. \n",
      "Training Loss: 0.876.. \n",
      "Training Loss: 1.101.. \n",
      "Training Loss: 1.317.. \n",
      "Training Loss: 1.544.. \n",
      "Training Loss: 1.766.. \n",
      "Training Loss: 1.991.. \n",
      "Training Loss: 2.194.. \n",
      "Model Accuracy: -118.745.. \n",
      "Epoch: 29/300.. \n",
      "Training Loss: 0.213.. \n",
      "Training Loss: 0.438.. \n",
      "Training Loss: 0.643.. \n",
      "Training Loss: 0.859.. \n",
      "Training Loss: 1.080.. \n",
      "Training Loss: 1.293.. \n",
      "Training Loss: 1.516.. \n",
      "Training Loss: 1.733.. \n",
      "Training Loss: 1.954.. \n",
      "Training Loss: 2.153.. \n",
      "Model Accuracy: -114.658.. \n",
      "Epoch: 30/300.. \n",
      "Training Loss: 0.209.. \n",
      "Training Loss: 0.430.. \n",
      "Training Loss: 0.631.. \n",
      "Training Loss: 0.843.. \n",
      "Training Loss: 1.060.. \n",
      "Training Loss: 1.268.. \n",
      "Training Loss: 1.487.. \n",
      "Training Loss: 1.700.. \n",
      "Training Loss: 1.917.. \n",
      "Training Loss: 2.112.. \n",
      "Model Accuracy: -110.587.. \n",
      "Epoch: 31/300.. \n",
      "Training Loss: 0.205.. \n",
      "Training Loss: 0.422.. \n",
      "Training Loss: 0.619.. \n",
      "Training Loss: 0.826.. \n",
      "Training Loss: 1.039.. \n",
      "Training Loss: 1.244.. \n",
      "Training Loss: 1.459.. \n",
      "Training Loss: 1.668.. \n",
      "Training Loss: 1.880.. \n",
      "Training Loss: 2.072.. \n",
      "Model Accuracy: -106.513.. \n",
      "Epoch: 32/300.. \n",
      "Training Loss: 0.200.. \n",
      "Training Loss: 0.414.. \n",
      "Training Loss: 0.606.. \n",
      "Training Loss: 0.810.. \n",
      "Training Loss: 1.019.. \n",
      "Training Loss: 1.219.. \n",
      "Training Loss: 1.430.. \n",
      "Training Loss: 1.635.. \n",
      "Training Loss: 1.844.. \n",
      "Training Loss: 2.031.. \n",
      "Model Accuracy: -102.443.. \n",
      "Epoch: 33/300.. \n",
      "Training Loss: 0.196.. \n",
      "Training Loss: 0.405.. \n",
      "Training Loss: 0.594.. \n",
      "Training Loss: 0.794.. \n",
      "Training Loss: 0.998.. \n",
      "Training Loss: 1.195.. \n",
      "Training Loss: 1.402.. \n",
      "Training Loss: 1.602.. \n",
      "Training Loss: 1.807.. \n",
      "Training Loss: 1.990.. \n",
      "Model Accuracy: -98.364.. \n",
      "Epoch: 34/300.. \n",
      "Training Loss: 0.192.. \n",
      "Training Loss: 0.397.. \n",
      "Training Loss: 0.582.. \n",
      "Training Loss: 0.777.. \n",
      "Training Loss: 0.978.. \n",
      "Training Loss: 1.170.. \n",
      "Training Loss: 1.373.. \n",
      "Training Loss: 1.570.. \n",
      "Training Loss: 1.770.. \n",
      "Training Loss: 1.949.. \n",
      "Model Accuracy: -94.267.. \n",
      "Epoch: 35/300.. \n",
      "Training Loss: 0.188.. \n",
      "Training Loss: 0.389.. \n",
      "Training Loss: 0.569.. \n",
      "Training Loss: 0.761.. \n",
      "Training Loss: 0.957.. \n",
      "Training Loss: 1.146.. \n",
      "Training Loss: 1.344.. \n",
      "Training Loss: 1.537.. \n",
      "Training Loss: 1.733.. \n",
      "Training Loss: 1.908.. \n",
      "Model Accuracy: -90.159.. \n",
      "Epoch: 36/300.. \n",
      "Training Loss: 0.184.. \n",
      "Training Loss: 0.380.. \n",
      "Training Loss: 0.557.. \n",
      "Training Loss: 0.744.. \n",
      "Training Loss: 0.937.. \n",
      "Training Loss: 1.121.. \n",
      "Training Loss: 1.315.. \n",
      "Training Loss: 1.504.. \n",
      "Training Loss: 1.696.. \n",
      "Training Loss: 1.867.. \n",
      "Model Accuracy: -86.027.. \n",
      "Epoch: 37/300.. \n",
      "Training Loss: 0.179.. \n",
      "Training Loss: 0.372.. \n",
      "Training Loss: 0.545.. \n",
      "Training Loss: 0.728.. \n",
      "Training Loss: 0.916.. \n",
      "Training Loss: 1.096.. \n",
      "Training Loss: 1.286.. \n",
      "Training Loss: 1.470.. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 1.658.. \n",
      "Training Loss: 1.825.. \n",
      "Model Accuracy: -81.869.. \n",
      "Epoch: 38/300.. \n",
      "Training Loss: 0.175.. \n",
      "Training Loss: 0.364.. \n",
      "Training Loss: 0.532.. \n",
      "Training Loss: 0.711.. \n",
      "Training Loss: 0.895.. \n",
      "Training Loss: 1.071.. \n",
      "Training Loss: 1.257.. \n",
      "Training Loss: 1.437.. \n",
      "Training Loss: 1.621.. \n",
      "Training Loss: 1.783.. \n",
      "Model Accuracy: -77.682.. \n",
      "Epoch: 39/300.. \n",
      "Training Loss: 0.171.. \n",
      "Training Loss: 0.355.. \n",
      "Training Loss: 0.519.. \n",
      "Training Loss: 0.694.. \n",
      "Training Loss: 0.874.. \n",
      "Training Loss: 1.046.. \n",
      "Training Loss: 1.227.. \n",
      "Training Loss: 1.403.. \n",
      "Training Loss: 1.583.. \n",
      "Training Loss: 1.741.. \n",
      "Model Accuracy: -73.460.. \n",
      "Epoch: 40/300.. \n",
      "Training Loss: 0.167.. \n",
      "Training Loss: 0.347.. \n",
      "Training Loss: 0.507.. \n",
      "Training Loss: 0.677.. \n",
      "Training Loss: 0.852.. \n",
      "Training Loss: 1.020.. \n",
      "Training Loss: 1.198.. \n",
      "Training Loss: 1.369.. \n",
      "Training Loss: 1.544.. \n",
      "Training Loss: 1.698.. \n",
      "Model Accuracy: -69.200.. \n",
      "Epoch: 41/300.. \n",
      "Training Loss: 0.162.. \n",
      "Training Loss: 0.338.. \n",
      "Training Loss: 0.494.. \n",
      "Training Loss: 0.660.. \n",
      "Training Loss: 0.831.. \n",
      "Training Loss: 0.995.. \n",
      "Training Loss: 1.168.. \n",
      "Training Loss: 1.335.. \n",
      "Training Loss: 1.506.. \n",
      "Training Loss: 1.655.. \n",
      "Model Accuracy: -64.894.. \n",
      "Epoch: 42/300.. \n",
      "Training Loss: 0.158.. \n",
      "Training Loss: 0.329.. \n",
      "Training Loss: 0.481.. \n",
      "Training Loss: 0.642.. \n",
      "Training Loss: 0.809.. \n",
      "Training Loss: 0.968.. \n",
      "Training Loss: 1.137.. \n",
      "Training Loss: 1.300.. \n",
      "Training Loss: 1.467.. \n",
      "Training Loss: 1.612.. \n",
      "Model Accuracy: -60.539.. \n",
      "Epoch: 43/300.. \n",
      "Training Loss: 0.153.. \n",
      "Training Loss: 0.320.. \n",
      "Training Loss: 0.467.. \n",
      "Training Loss: 0.625.. \n",
      "Training Loss: 0.787.. \n",
      "Training Loss: 0.942.. \n",
      "Training Loss: 1.107.. \n",
      "Training Loss: 1.265.. \n",
      "Training Loss: 1.427.. \n",
      "Training Loss: 1.568.. \n",
      "Model Accuracy: -56.136.. \n",
      "Epoch: 44/300.. \n",
      "Training Loss: 0.149.. \n",
      "Training Loss: 0.311.. \n",
      "Training Loss: 0.454.. \n",
      "Training Loss: 0.607.. \n",
      "Training Loss: 0.765.. \n",
      "Training Loss: 0.916.. \n",
      "Training Loss: 1.076.. \n",
      "Training Loss: 1.229.. \n",
      "Training Loss: 1.387.. \n",
      "Training Loss: 1.524.. \n",
      "Model Accuracy: -51.680.. \n",
      "Epoch: 45/300.. \n",
      "Training Loss: 0.144.. \n",
      "Training Loss: 0.302.. \n",
      "Training Loss: 0.441.. \n",
      "Training Loss: 0.589.. \n",
      "Training Loss: 0.743.. \n",
      "Training Loss: 0.889.. \n",
      "Training Loss: 1.044.. \n",
      "Training Loss: 1.193.. \n",
      "Training Loss: 1.347.. \n",
      "Training Loss: 1.479.. \n",
      "Model Accuracy: -47.171.. \n",
      "Epoch: 46/300.. \n",
      "Training Loss: 0.140.. \n",
      "Training Loss: 0.293.. \n",
      "Training Loss: 0.427.. \n",
      "Training Loss: 0.571.. \n",
      "Training Loss: 0.720.. \n",
      "Training Loss: 0.861.. \n",
      "Training Loss: 1.012.. \n",
      "Training Loss: 1.157.. \n",
      "Training Loss: 1.306.. \n",
      "Training Loss: 1.433.. \n",
      "Model Accuracy: -42.605.. \n",
      "Epoch: 47/300.. \n",
      "Training Loss: 0.135.. \n",
      "Training Loss: 0.284.. \n",
      "Training Loss: 0.413.. \n",
      "Training Loss: 0.552.. \n",
      "Training Loss: 0.697.. \n",
      "Training Loss: 0.834.. \n",
      "Training Loss: 0.980.. \n",
      "Training Loss: 1.120.. \n",
      "Training Loss: 1.264.. \n",
      "Training Loss: 1.387.. \n",
      "Model Accuracy: -37.984.. \n",
      "Epoch: 48/300.. \n",
      "Training Loss: 0.130.. \n",
      "Training Loss: 0.274.. \n",
      "Training Loss: 0.399.. \n",
      "Training Loss: 0.534.. \n",
      "Training Loss: 0.673.. \n",
      "Training Loss: 0.806.. \n",
      "Training Loss: 0.947.. \n",
      "Training Loss: 1.083.. \n",
      "Training Loss: 1.222.. \n",
      "Training Loss: 1.340.. \n",
      "Model Accuracy: -33.309.. \n",
      "Epoch: 49/300.. \n",
      "Training Loss: 0.125.. \n",
      "Training Loss: 0.265.. \n",
      "Training Loss: 0.385.. \n",
      "Training Loss: 0.515.. \n",
      "Training Loss: 0.650.. \n",
      "Training Loss: 0.778.. \n",
      "Training Loss: 0.915.. \n",
      "Training Loss: 1.045.. \n",
      "Training Loss: 1.180.. \n",
      "Training Loss: 1.293.. \n",
      "Model Accuracy: -28.581.. \n",
      "Epoch: 50/300.. \n",
      "Training Loss: 0.120.. \n",
      "Training Loss: 0.255.. \n",
      "Training Loss: 0.371.. \n",
      "Training Loss: 0.496.. \n",
      "Training Loss: 0.626.. \n",
      "Training Loss: 0.749.. \n",
      "Training Loss: 0.881.. \n",
      "Training Loss: 1.007.. \n",
      "Training Loss: 1.137.. \n",
      "Training Loss: 1.245.. \n",
      "Model Accuracy: -23.804.. \n",
      "Epoch: 51/300.. \n",
      "Training Loss: 0.116.. \n",
      "Training Loss: 0.246.. \n",
      "Training Loss: 0.356.. \n",
      "Training Loss: 0.477.. \n",
      "Training Loss: 0.602.. \n",
      "Training Loss: 0.720.. \n",
      "Training Loss: 0.847.. \n",
      "Training Loss: 0.968.. \n",
      "Training Loss: 1.093.. \n",
      "Training Loss: 1.197.. \n",
      "Model Accuracy: -18.979.. \n",
      "Epoch: 52/300.. \n",
      "Training Loss: 0.111.. \n",
      "Training Loss: 0.236.. \n",
      "Training Loss: 0.342.. \n",
      "Training Loss: 0.457.. \n",
      "Training Loss: 0.577.. \n",
      "Training Loss: 0.691.. \n",
      "Training Loss: 0.813.. \n",
      "Training Loss: 0.929.. \n",
      "Training Loss: 1.050.. \n",
      "Training Loss: 1.149.. \n",
      "Model Accuracy: -14.110.. \n",
      "Epoch: 53/300.. \n",
      "Training Loss: 0.106.. \n",
      "Training Loss: 0.226.. \n",
      "Training Loss: 0.327.. \n",
      "Training Loss: 0.437.. \n",
      "Training Loss: 0.553.. \n",
      "Training Loss: 0.662.. \n",
      "Training Loss: 0.779.. \n",
      "Training Loss: 0.890.. \n",
      "Training Loss: 1.005.. \n",
      "Training Loss: 1.100.. \n",
      "Model Accuracy: -9.203.. \n",
      "Epoch: 54/300.. \n",
      "Training Loss: 0.100.. \n",
      "Training Loss: 0.216.. \n",
      "Training Loss: 0.312.. \n",
      "Training Loss: 0.418.. \n",
      "Training Loss: 0.528.. \n",
      "Training Loss: 0.632.. \n",
      "Training Loss: 0.745.. \n",
      "Training Loss: 0.850.. \n",
      "Training Loss: 0.961.. \n",
      "Training Loss: 1.050.. \n",
      "Model Accuracy: -4.269.. \n",
      "Epoch: 55/300.. \n",
      "Training Loss: 0.095.. \n",
      "Training Loss: 0.206.. \n",
      "Training Loss: 0.297.. \n",
      "Training Loss: 0.398.. \n",
      "Training Loss: 0.503.. \n",
      "Training Loss: 0.602.. \n",
      "Training Loss: 0.710.. \n",
      "Training Loss: 0.811.. \n",
      "Training Loss: 0.916.. \n",
      "Training Loss: 1.001.. \n",
      "Model Accuracy: 0.686.. \n",
      "Epoch: 56/300.. \n",
      "Training Loss: 0.090.. \n",
      "Training Loss: 0.196.. \n",
      "Training Loss: 0.282.. \n",
      "Training Loss: 0.378.. \n",
      "Training Loss: 0.478.. \n",
      "Training Loss: 0.572.. \n",
      "Training Loss: 0.675.. \n",
      "Training Loss: 0.771.. \n",
      "Training Loss: 0.871.. \n",
      "Training Loss: 0.951.. \n",
      "Model Accuracy: 5.652.. \n",
      "Epoch: 57/300.. \n",
      "Training Loss: 0.085.. \n",
      "Training Loss: 0.186.. \n",
      "Training Loss: 0.267.. \n",
      "Training Loss: 0.358.. \n",
      "Training Loss: 0.453.. \n",
      "Training Loss: 0.542.. \n",
      "Training Loss: 0.640.. \n",
      "Training Loss: 0.731.. \n",
      "Training Loss: 0.826.. \n",
      "Training Loss: 0.901.. \n",
      "Model Accuracy: 10.620.. \n",
      "Epoch: 58/300.. \n",
      "Training Loss: 0.080.. \n",
      "Training Loss: 0.176.. \n",
      "Training Loss: 0.252.. \n",
      "Training Loss: 0.338.. \n",
      "Training Loss: 0.428.. \n",
      "Training Loss: 0.513.. \n",
      "Training Loss: 0.605.. \n",
      "Training Loss: 0.691.. \n",
      "Training Loss: 0.781.. \n",
      "Training Loss: 0.852.. \n",
      "Model Accuracy: 15.575.. \n",
      "Epoch: 59/300.. \n",
      "Training Loss: 0.075.. \n",
      "Training Loss: 0.166.. \n",
      "Training Loss: 0.237.. \n",
      "Training Loss: 0.318.. \n",
      "Training Loss: 0.403.. \n",
      "Training Loss: 0.483.. \n",
      "Training Loss: 0.570.. \n",
      "Training Loss: 0.651.. \n",
      "Training Loss: 0.737.. \n",
      "Training Loss: 0.802.. \n",
      "Model Accuracy: 20.504.. \n",
      "Epoch: 60/300.. \n",
      "Training Loss: 0.070.. \n",
      "Training Loss: 0.156.. \n",
      "Training Loss: 0.223.. \n",
      "Training Loss: 0.298.. \n",
      "Training Loss: 0.378.. \n",
      "Training Loss: 0.453.. \n",
      "Training Loss: 0.535.. \n",
      "Training Loss: 0.611.. \n",
      "Training Loss: 0.692.. \n",
      "Training Loss: 0.753.. \n",
      "Model Accuracy: 25.390.. \n",
      "Epoch: 61/300.. \n",
      "Training Loss: 0.065.. \n",
      "Training Loss: 0.146.. \n",
      "Training Loss: 0.208.. \n",
      "Training Loss: 0.278.. \n",
      "Training Loss: 0.353.. \n",
      "Training Loss: 0.424.. \n",
      "Training Loss: 0.501.. \n",
      "Training Loss: 0.572.. \n",
      "Training Loss: 0.648.. \n",
      "Training Loss: 0.704.. \n",
      "Model Accuracy: 30.214.. \n",
      "Epoch: 62/300.. \n",
      "Training Loss: 0.060.. \n",
      "Training Loss: 0.136.. \n",
      "Training Loss: 0.193.. \n",
      "Training Loss: 0.259.. \n",
      "Training Loss: 0.329.. \n",
      "Training Loss: 0.395.. \n",
      "Training Loss: 0.467.. \n",
      "Training Loss: 0.533.. \n",
      "Training Loss: 0.604.. \n",
      "Training Loss: 0.657.. \n",
      "Model Accuracy: 34.953.. \n",
      "Epoch: 63/300.. \n",
      "Training Loss: 0.055.. \n",
      "Training Loss: 0.126.. \n",
      "Training Loss: 0.179.. \n",
      "Training Loss: 0.239.. \n",
      "Training Loss: 0.305.. \n",
      "Training Loss: 0.366.. \n",
      "Training Loss: 0.433.. \n",
      "Training Loss: 0.495.. \n",
      "Training Loss: 0.562.. \n",
      "Training Loss: 0.610.. \n",
      "Model Accuracy: 39.577.. \n",
      "Epoch: 64/300.. \n",
      "Training Loss: 0.050.. \n",
      "Training Loss: 0.117.. \n",
      "Training Loss: 0.165.. \n",
      "Training Loss: 0.221.. \n",
      "Training Loss: 0.281.. \n",
      "Training Loss: 0.338.. \n",
      "Training Loss: 0.401.. \n",
      "Training Loss: 0.457.. \n",
      "Training Loss: 0.520.. \n",
      "Training Loss: 0.564.. \n",
      "Model Accuracy: 44.055.. \n",
      "Epoch: 65/300.. \n",
      "Training Loss: 0.045.. \n",
      "Training Loss: 0.108.. \n",
      "Training Loss: 0.152.. \n",
      "Training Loss: 0.203.. \n",
      "Training Loss: 0.259.. \n",
      "Training Loss: 0.312.. \n",
      "Training Loss: 0.369.. \n",
      "Training Loss: 0.421.. \n",
      "Training Loss: 0.480.. \n",
      "Training Loss: 0.520.. \n",
      "Model Accuracy: 48.349.. \n",
      "Epoch: 66/300.. \n",
      "Training Loss: 0.041.. \n",
      "Training Loss: 0.099.. \n",
      "Training Loss: 0.139.. \n",
      "Training Loss: 0.185.. \n",
      "Training Loss: 0.237.. \n",
      "Training Loss: 0.286.. \n",
      "Training Loss: 0.339.. \n",
      "Training Loss: 0.387.. \n",
      "Training Loss: 0.441.. \n",
      "Training Loss: 0.478.. \n",
      "Model Accuracy: 52.420.. \n",
      "Epoch: 67/300.. \n",
      "Training Loss: 0.036.. \n",
      "Training Loss: 0.091.. \n",
      "Training Loss: 0.127.. \n",
      "Training Loss: 0.169.. \n",
      "Training Loss: 0.216.. \n",
      "Training Loss: 0.262.. \n",
      "Training Loss: 0.311.. \n",
      "Training Loss: 0.354.. \n",
      "Training Loss: 0.405.. \n",
      "Training Loss: 0.439.. \n",
      "Model Accuracy: 56.223.. \n",
      "Epoch: 68/300.. \n",
      "Training Loss: 0.032.. \n",
      "Training Loss: 0.083.. \n",
      "Training Loss: 0.116.. \n",
      "Training Loss: 0.154.. \n",
      "Training Loss: 0.197.. \n",
      "Training Loss: 0.239.. \n",
      "Training Loss: 0.284.. \n",
      "Training Loss: 0.323.. \n",
      "Training Loss: 0.371.. \n",
      "Training Loss: 0.403.. \n",
      "Model Accuracy: 59.712.. \n",
      "Epoch: 69/300.. \n",
      "Training Loss: 0.029.. \n",
      "Training Loss: 0.076.. \n",
      "Training Loss: 0.106.. \n",
      "Training Loss: 0.140.. \n",
      "Training Loss: 0.179.. \n",
      "Training Loss: 0.219.. \n",
      "Training Loss: 0.260.. \n",
      "Training Loss: 0.296.. \n",
      "Training Loss: 0.340.. \n",
      "Training Loss: 0.370.. \n",
      "Model Accuracy: 62.842.. \n",
      "Epoch: 70/300.. \n",
      "Training Loss: 0.025.. \n",
      "Training Loss: 0.069.. \n",
      "Training Loss: 0.097.. \n",
      "Training Loss: 0.128.. \n",
      "Training Loss: 0.163.. \n",
      "Training Loss: 0.200.. \n",
      "Training Loss: 0.238.. \n",
      "Training Loss: 0.271.. \n",
      "Training Loss: 0.312.. \n",
      "Training Loss: 0.341.. \n",
      "Model Accuracy: 65.577.. \n",
      "Epoch: 71/300.. \n",
      "Training Loss: 0.023.. \n",
      "Training Loss: 0.064.. \n",
      "Training Loss: 0.089.. \n",
      "Training Loss: 0.117.. \n",
      "Training Loss: 0.150.. \n",
      "Training Loss: 0.185.. \n",
      "Training Loss: 0.219.. \n",
      "Training Loss: 0.249.. \n",
      "Training Loss: 0.288.. \n",
      "Training Loss: 0.316.. \n",
      "Model Accuracy: 67.894.. \n",
      "Epoch: 72/300.. \n",
      "Training Loss: 0.020.. \n",
      "Training Loss: 0.059.. \n",
      "Training Loss: 0.083.. \n",
      "Training Loss: 0.108.. \n",
      "Training Loss: 0.138.. \n",
      "Training Loss: 0.172.. \n",
      "Training Loss: 0.203.. \n",
      "Training Loss: 0.231.. \n",
      "Training Loss: 0.268.. \n",
      "Training Loss: 0.296.. \n",
      "Model Accuracy: 69.795.. \n",
      "Epoch: 73/300.. \n",
      "Training Loss: 0.018.. \n",
      "Training Loss: 0.055.. \n",
      "Training Loss: 0.079.. \n",
      "Training Loss: 0.102.. \n",
      "Training Loss: 0.129.. \n",
      "Training Loss: 0.161.. \n",
      "Training Loss: 0.191.. \n",
      "Training Loss: 0.216.. \n",
      "Training Loss: 0.252.. \n",
      "Training Loss: 0.280.. \n",
      "Model Accuracy: 71.305.. \n",
      "Epoch: 74/300.. \n",
      "Training Loss: 0.017.. \n",
      "Training Loss: 0.052.. \n",
      "Training Loss: 0.075.. \n",
      "Training Loss: 0.096.. \n",
      "Training Loss: 0.122.. \n",
      "Training Loss: 0.153.. \n",
      "Training Loss: 0.181.. \n",
      "Training Loss: 0.205.. \n",
      "Training Loss: 0.239.. \n",
      "Training Loss: 0.267.. \n",
      "Model Accuracy: 72.473.. \n",
      "Epoch: 75/300.. \n",
      "Training Loss: 0.016.. \n",
      "Training Loss: 0.050.. \n",
      "Training Loss: 0.073.. \n",
      "Training Loss: 0.093.. \n",
      "Training Loss: 0.117.. \n",
      "Training Loss: 0.148.. \n",
      "Training Loss: 0.173.. \n",
      "Training Loss: 0.196.. \n",
      "Training Loss: 0.229.. \n",
      "Training Loss: 0.258.. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Accuracy: 73.356.. \n",
      "Epoch: 76/300.. \n",
      "Training Loss: 0.016.. \n",
      "Training Loss: 0.049.. \n",
      "Training Loss: 0.071.. \n",
      "Training Loss: 0.090.. \n",
      "Training Loss: 0.113.. \n",
      "Training Loss: 0.143.. \n",
      "Training Loss: 0.168.. \n",
      "Training Loss: 0.190.. \n",
      "Training Loss: 0.222.. \n",
      "Training Loss: 0.251.. \n",
      "Model Accuracy: 74.018.. \n",
      "Epoch: 77/300.. \n",
      "Training Loss: 0.016.. \n",
      "Training Loss: 0.048.. \n",
      "Training Loss: 0.070.. \n",
      "Training Loss: 0.088.. \n",
      "Training Loss: 0.110.. \n",
      "Training Loss: 0.140.. \n",
      "Training Loss: 0.164.. \n",
      "Training Loss: 0.185.. \n",
      "Training Loss: 0.217.. \n",
      "Training Loss: 0.245.. \n",
      "Model Accuracy: 74.513.. \n",
      "Epoch: 78/300.. \n",
      "Training Loss: 0.015.. \n",
      "Training Loss: 0.047.. \n",
      "Training Loss: 0.069.. \n",
      "Training Loss: 0.087.. \n",
      "Training Loss: 0.108.. \n",
      "Training Loss: 0.138.. \n",
      "Training Loss: 0.161.. \n",
      "Training Loss: 0.181.. \n",
      "Training Loss: 0.213.. \n",
      "Training Loss: 0.242.. \n",
      "Model Accuracy: 74.886.. \n",
      "Epoch: 79/300.. \n",
      "Training Loss: 0.015.. \n",
      "Training Loss: 0.046.. \n",
      "Training Loss: 0.069.. \n",
      "Training Loss: 0.086.. \n",
      "Training Loss: 0.107.. \n",
      "Training Loss: 0.136.. \n",
      "Training Loss: 0.158.. \n",
      "Training Loss: 0.179.. \n",
      "Training Loss: 0.209.. \n",
      "Training Loss: 0.239.. \n",
      "Model Accuracy: 75.174.. \n",
      "Epoch: 80/300.. \n",
      "Training Loss: 0.015.. \n",
      "Training Loss: 0.046.. \n",
      "Training Loss: 0.069.. \n",
      "Training Loss: 0.085.. \n",
      "Training Loss: 0.106.. \n",
      "Training Loss: 0.135.. \n",
      "Training Loss: 0.157.. \n",
      "Training Loss: 0.177.. \n",
      "Training Loss: 0.207.. \n",
      "Training Loss: 0.236.. \n",
      "Model Accuracy: 75.401.. \n",
      "Epoch: 81/300.. \n",
      "Training Loss: 0.015.. \n",
      "Training Loss: 0.046.. \n",
      "Training Loss: 0.069.. \n",
      "Training Loss: 0.085.. \n",
      "Training Loss: 0.105.. \n",
      "Training Loss: 0.134.. \n",
      "Training Loss: 0.155.. \n",
      "Training Loss: 0.175.. \n",
      "Training Loss: 0.205.. \n",
      "Training Loss: 0.235.. \n",
      "Model Accuracy: 75.585.. \n",
      "Epoch: 82/300.. \n",
      "Training Loss: 0.016.. \n",
      "Training Loss: 0.045.. \n",
      "Training Loss: 0.068.. \n",
      "Training Loss: 0.085.. \n",
      "Training Loss: 0.105.. \n",
      "Training Loss: 0.134.. \n",
      "Training Loss: 0.154.. \n",
      "Training Loss: 0.174.. \n",
      "Training Loss: 0.204.. \n",
      "Training Loss: 0.233.. \n",
      "Model Accuracy: 75.740.. \n",
      "Epoch: 83/300.. \n",
      "Training Loss: 0.016.. \n",
      "Training Loss: 0.045.. \n",
      "Training Loss: 0.068.. \n",
      "Training Loss: 0.084.. \n",
      "Training Loss: 0.104.. \n",
      "Training Loss: 0.133.. \n",
      "Training Loss: 0.153.. \n",
      "Training Loss: 0.173.. \n",
      "Training Loss: 0.202.. \n",
      "Training Loss: 0.232.. \n",
      "Model Accuracy: 75.874.. \n",
      "Epoch: 84/300.. \n",
      "Training Loss: 0.016.. \n",
      "Training Loss: 0.045.. \n",
      "Training Loss: 0.068.. \n",
      "Training Loss: 0.084.. \n",
      "Training Loss: 0.103.. \n",
      "Training Loss: 0.132.. \n",
      "Training Loss: 0.152.. \n",
      "Training Loss: 0.172.. \n",
      "Training Loss: 0.201.. \n",
      "Training Loss: 0.231.. \n",
      "Model Accuracy: 75.994.. \n",
      "Epoch: 85/300.. \n",
      "Training Loss: 0.016.. \n",
      "Training Loss: 0.045.. \n",
      "Training Loss: 0.068.. \n",
      "Training Loss: 0.084.. \n",
      "Training Loss: 0.103.. \n",
      "Training Loss: 0.132.. \n",
      "Training Loss: 0.152.. \n",
      "Training Loss: 0.171.. \n",
      "Training Loss: 0.200.. \n",
      "Training Loss: 0.230.. \n",
      "Model Accuracy: 76.103.. \n",
      "Epoch: 86/300.. \n",
      "Training Loss: 0.016.. \n",
      "Training Loss: 0.045.. \n",
      "Training Loss: 0.068.. \n",
      "Training Loss: 0.083.. \n",
      "Training Loss: 0.103.. \n",
      "Training Loss: 0.131.. \n",
      "Training Loss: 0.151.. \n",
      "Training Loss: 0.170.. \n",
      "Training Loss: 0.199.. \n",
      "Training Loss: 0.229.. \n",
      "Model Accuracy: 76.205.. \n",
      "Epoch: 87/300.. \n",
      "Training Loss: 0.016.. \n",
      "Training Loss: 0.044.. \n",
      "Training Loss: 0.068.. \n",
      "Training Loss: 0.083.. \n",
      "Training Loss: 0.102.. \n",
      "Training Loss: 0.131.. \n",
      "Training Loss: 0.150.. \n",
      "Training Loss: 0.169.. \n",
      "Training Loss: 0.198.. \n",
      "Training Loss: 0.228.. \n",
      "Model Accuracy: 76.301.. \n",
      "Epoch: 88/300.. \n",
      "Training Loss: 0.016.. \n",
      "Training Loss: 0.044.. \n",
      "Training Loss: 0.067.. \n",
      "Training Loss: 0.083.. \n",
      "Training Loss: 0.102.. \n",
      "Training Loss: 0.130.. \n",
      "Training Loss: 0.150.. \n",
      "Training Loss: 0.168.. \n",
      "Training Loss: 0.197.. \n",
      "Training Loss: 0.227.. \n",
      "Model Accuracy: 76.392.. \n",
      "Epoch: 89/300.. \n",
      "Training Loss: 0.015.. \n",
      "Training Loss: 0.044.. \n",
      "Training Loss: 0.067.. \n",
      "Training Loss: 0.083.. \n",
      "Training Loss: 0.101.. \n",
      "Training Loss: 0.130.. \n",
      "Training Loss: 0.149.. \n",
      "Training Loss: 0.168.. \n",
      "Training Loss: 0.196.. \n",
      "Training Loss: 0.226.. \n",
      "Model Accuracy: 76.480.. \n",
      "Epoch: 90/300.. \n",
      "Training Loss: 0.015.. \n",
      "Training Loss: 0.044.. \n",
      "Training Loss: 0.067.. \n",
      "Training Loss: 0.082.. \n",
      "Training Loss: 0.101.. \n",
      "Training Loss: 0.129.. \n",
      "Training Loss: 0.149.. \n",
      "Training Loss: 0.167.. \n",
      "Training Loss: 0.195.. \n",
      "Training Loss: 0.225.. \n",
      "Model Accuracy: 76.566.. \n",
      "Epoch: 91/300.. \n",
      "Training Loss: 0.015.. \n",
      "Training Loss: 0.044.. \n",
      "Training Loss: 0.067.. \n",
      "Training Loss: 0.082.. \n",
      "Training Loss: 0.101.. \n",
      "Training Loss: 0.129.. \n",
      "Training Loss: 0.148.. \n",
      "Training Loss: 0.166.. \n",
      "Training Loss: 0.195.. \n",
      "Training Loss: 0.224.. \n",
      "Model Accuracy: 76.649.. \n",
      "Epoch: 92/300.. \n",
      "Training Loss: 0.015.. \n",
      "Training Loss: 0.044.. \n",
      "Training Loss: 0.066.. \n",
      "Training Loss: 0.082.. \n",
      "Training Loss: 0.100.. \n",
      "Training Loss: 0.128.. \n",
      "Training Loss: 0.147.. \n",
      "Training Loss: 0.165.. \n",
      "Training Loss: 0.194.. \n",
      "Training Loss: 0.223.. \n",
      "Model Accuracy: 76.731.. \n",
      "Epoch: 93/300.. \n",
      "Training Loss: 0.015.. \n",
      "Training Loss: 0.043.. \n",
      "Training Loss: 0.066.. \n",
      "Training Loss: 0.081.. \n",
      "Training Loss: 0.100.. \n",
      "Training Loss: 0.128.. \n",
      "Training Loss: 0.147.. \n",
      "Training Loss: 0.165.. \n",
      "Training Loss: 0.193.. \n",
      "Training Loss: 0.222.. \n",
      "Model Accuracy: 76.811.. \n",
      "Epoch: 94/300.. \n",
      "Training Loss: 0.015.. \n",
      "Training Loss: 0.043.. \n",
      "Training Loss: 0.066.. \n",
      "Training Loss: 0.081.. \n",
      "Training Loss: 0.100.. \n",
      "Training Loss: 0.128.. \n",
      "Training Loss: 0.146.. \n",
      "Training Loss: 0.164.. \n",
      "Training Loss: 0.192.. \n",
      "Training Loss: 0.221.. \n",
      "Model Accuracy: 76.890.. \n",
      "Epoch: 95/300.. \n",
      "Training Loss: 0.015.. \n",
      "Training Loss: 0.043.. \n",
      "Training Loss: 0.066.. \n",
      "Training Loss: 0.081.. \n",
      "Training Loss: 0.099.. \n",
      "Training Loss: 0.127.. \n",
      "Training Loss: 0.146.. \n",
      "Training Loss: 0.164.. \n",
      "Training Loss: 0.191.. \n",
      "Training Loss: 0.220.. \n",
      "Model Accuracy: 76.968.. \n",
      "Epoch: 96/300.. \n",
      "Training Loss: 0.015.. \n",
      "Training Loss: 0.043.. \n",
      "Training Loss: 0.066.. \n",
      "Training Loss: 0.080.. \n",
      "Training Loss: 0.099.. \n",
      "Training Loss: 0.127.. \n",
      "Training Loss: 0.145.. \n",
      "Training Loss: 0.163.. \n",
      "Training Loss: 0.191.. \n",
      "Training Loss: 0.219.. \n",
      "Model Accuracy: 77.045.. \n",
      "Epoch: 97/300.. \n",
      "Training Loss: 0.015.. \n",
      "Training Loss: 0.043.. \n",
      "Training Loss: 0.065.. \n",
      "Training Loss: 0.080.. \n",
      "Training Loss: 0.099.. \n",
      "Training Loss: 0.126.. \n",
      "Training Loss: 0.145.. \n",
      "Training Loss: 0.162.. \n",
      "Training Loss: 0.190.. \n",
      "Training Loss: 0.218.. \n",
      "Model Accuracy: 77.121.. \n",
      "Epoch: 98/300.. \n",
      "Training Loss: 0.015.. \n",
      "Training Loss: 0.042.. \n",
      "Training Loss: 0.065.. \n",
      "Training Loss: 0.080.. \n",
      "Training Loss: 0.098.. \n",
      "Training Loss: 0.126.. \n",
      "Training Loss: 0.144.. \n",
      "Training Loss: 0.162.. \n",
      "Training Loss: 0.189.. \n",
      "Training Loss: 0.218.. \n",
      "Model Accuracy: 77.196.. \n",
      "Epoch: 99/300.. \n",
      "Training Loss: 0.015.. \n",
      "Training Loss: 0.042.. \n",
      "Training Loss: 0.065.. \n",
      "Training Loss: 0.079.. \n",
      "Training Loss: 0.098.. \n",
      "Training Loss: 0.125.. \n",
      "Training Loss: 0.144.. \n",
      "Training Loss: 0.161.. \n",
      "Training Loss: 0.188.. \n",
      "Training Loss: 0.217.. \n",
      "Model Accuracy: 77.271.. \n",
      "Epoch: 100/300.. \n",
      "Training Loss: 0.015.. \n",
      "Training Loss: 0.042.. \n",
      "Training Loss: 0.065.. \n",
      "Training Loss: 0.079.. \n",
      "Training Loss: 0.097.. \n",
      "Training Loss: 0.125.. \n",
      "Training Loss: 0.143.. \n",
      "Training Loss: 0.160.. \n",
      "Training Loss: 0.188.. \n",
      "Training Loss: 0.216.. \n",
      "Model Accuracy: 77.345.. \n",
      "Epoch: 101/300.. \n",
      "Training Loss: 0.015.. \n",
      "Training Loss: 0.042.. \n",
      "Training Loss: 0.064.. \n",
      "Training Loss: 0.079.. \n",
      "Training Loss: 0.097.. \n",
      "Training Loss: 0.124.. \n",
      "Training Loss: 0.143.. \n",
      "Training Loss: 0.160.. \n",
      "Training Loss: 0.187.. \n",
      "Training Loss: 0.215.. \n",
      "Model Accuracy: 77.418.. \n",
      "Epoch: 102/300.. \n",
      "Training Loss: 0.015.. \n",
      "Training Loss: 0.042.. \n",
      "Training Loss: 0.064.. \n",
      "Training Loss: 0.079.. \n",
      "Training Loss: 0.097.. \n",
      "Training Loss: 0.124.. \n",
      "Training Loss: 0.142.. \n",
      "Training Loss: 0.159.. \n",
      "Training Loss: 0.186.. \n",
      "Training Loss: 0.214.. \n",
      "Model Accuracy: 77.490.. \n",
      "Epoch: 103/300.. \n",
      "Training Loss: 0.014.. \n",
      "Training Loss: 0.042.. \n",
      "Training Loss: 0.064.. \n",
      "Training Loss: 0.078.. \n",
      "Training Loss: 0.096.. \n",
      "Training Loss: 0.124.. \n",
      "Training Loss: 0.142.. \n",
      "Training Loss: 0.159.. \n",
      "Training Loss: 0.186.. \n",
      "Training Loss: 0.213.. \n",
      "Model Accuracy: 77.562.. \n",
      "Epoch: 104/300.. \n",
      "Training Loss: 0.014.. \n",
      "Training Loss: 0.041.. \n",
      "Training Loss: 0.064.. \n",
      "Training Loss: 0.078.. \n",
      "Training Loss: 0.096.. \n",
      "Training Loss: 0.123.. \n",
      "Training Loss: 0.141.. \n",
      "Training Loss: 0.158.. \n",
      "Training Loss: 0.185.. \n",
      "Training Loss: 0.213.. \n",
      "Model Accuracy: 77.633.. \n",
      "Epoch: 105/300.. \n",
      "Training Loss: 0.014.. \n",
      "Training Loss: 0.041.. \n",
      "Training Loss: 0.063.. \n",
      "Training Loss: 0.078.. \n",
      "Training Loss: 0.096.. \n",
      "Training Loss: 0.123.. \n",
      "Training Loss: 0.141.. \n",
      "Training Loss: 0.158.. \n",
      "Training Loss: 0.184.. \n",
      "Training Loss: 0.212.. \n",
      "Model Accuracy: 77.704.. \n",
      "Epoch: 106/300.. \n",
      "Training Loss: 0.014.. \n",
      "Training Loss: 0.041.. \n",
      "Training Loss: 0.063.. \n",
      "Training Loss: 0.077.. \n",
      "Training Loss: 0.095.. \n",
      "Training Loss: 0.122.. \n",
      "Training Loss: 0.140.. \n",
      "Training Loss: 0.157.. \n",
      "Training Loss: 0.184.. \n",
      "Training Loss: 0.211.. \n",
      "Model Accuracy: 77.774.. \n",
      "Epoch: 107/300.. \n",
      "Training Loss: 0.014.. \n",
      "Training Loss: 0.041.. \n",
      "Training Loss: 0.063.. \n",
      "Training Loss: 0.077.. \n",
      "Training Loss: 0.095.. \n",
      "Training Loss: 0.122.. \n",
      "Training Loss: 0.140.. \n",
      "Training Loss: 0.156.. \n",
      "Training Loss: 0.183.. \n",
      "Training Loss: 0.210.. \n",
      "Model Accuracy: 77.844.. \n",
      "Epoch: 108/300.. \n",
      "Training Loss: 0.014.. \n",
      "Training Loss: 0.041.. \n",
      "Training Loss: 0.063.. \n",
      "Training Loss: 0.077.. \n",
      "Training Loss: 0.095.. \n",
      "Training Loss: 0.122.. \n",
      "Training Loss: 0.140.. \n",
      "Training Loss: 0.156.. \n",
      "Training Loss: 0.182.. \n",
      "Training Loss: 0.209.. \n",
      "Model Accuracy: 77.913.. \n",
      "Epoch: 109/300.. \n",
      "Training Loss: 0.014.. \n",
      "Training Loss: 0.041.. \n",
      "Training Loss: 0.062.. \n",
      "Training Loss: 0.077.. \n",
      "Training Loss: 0.095.. \n",
      "Training Loss: 0.121.. \n",
      "Training Loss: 0.139.. \n",
      "Training Loss: 0.155.. \n",
      "Training Loss: 0.182.. \n",
      "Training Loss: 0.209.. \n",
      "Model Accuracy: 77.981.. \n",
      "Epoch: 110/300.. \n",
      "Training Loss: 0.014.. \n",
      "Training Loss: 0.040.. \n",
      "Training Loss: 0.062.. \n",
      "Training Loss: 0.076.. \n",
      "Training Loss: 0.094.. \n",
      "Training Loss: 0.121.. \n",
      "Training Loss: 0.139.. \n",
      "Training Loss: 0.155.. \n",
      "Training Loss: 0.181.. \n",
      "Training Loss: 0.208.. \n",
      "Model Accuracy: 78.049.. \n",
      "Epoch: 111/300.. \n",
      "Training Loss: 0.014.. \n",
      "Training Loss: 0.040.. \n",
      "Training Loss: 0.062.. \n",
      "Training Loss: 0.076.. \n",
      "Training Loss: 0.094.. \n",
      "Training Loss: 0.120.. \n",
      "Training Loss: 0.138.. \n",
      "Training Loss: 0.154.. \n",
      "Training Loss: 0.180.. \n",
      "Training Loss: 0.207.. \n",
      "Model Accuracy: 78.117.. \n",
      "Epoch: 112/300.. \n",
      "Training Loss: 0.014.. \n",
      "Training Loss: 0.040.. \n",
      "Training Loss: 0.062.. \n",
      "Training Loss: 0.076.. \n",
      "Training Loss: 0.094.. \n",
      "Training Loss: 0.120.. \n",
      "Training Loss: 0.138.. \n",
      "Training Loss: 0.154.. \n",
      "Training Loss: 0.180.. \n",
      "Training Loss: 0.206.. \n",
      "Model Accuracy: 78.184.. \n",
      "Epoch: 113/300.. \n",
      "Training Loss: 0.014.. \n",
      "Training Loss: 0.040.. \n",
      "Training Loss: 0.062.. \n",
      "Training Loss: 0.075.. \n",
      "Training Loss: 0.093.. \n",
      "Training Loss: 0.120.. \n",
      "Training Loss: 0.137.. \n",
      "Training Loss: 0.153.. \n",
      "Training Loss: 0.179.. \n",
      "Training Loss: 0.206.. \n",
      "Model Accuracy: 78.250.. \n",
      "Epoch: 114/300.. \n",
      "Training Loss: 0.014.. \n",
      "Training Loss: 0.040.. \n",
      "Training Loss: 0.061.. \n",
      "Training Loss: 0.075.. \n",
      "Training Loss: 0.093.. \n",
      "Training Loss: 0.119.. \n",
      "Training Loss: 0.137.. \n",
      "Training Loss: 0.153.. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.178.. \n",
      "Training Loss: 0.205.. \n",
      "Model Accuracy: 78.316.. \n",
      "Epoch: 115/300.. \n",
      "Training Loss: 0.014.. \n",
      "Training Loss: 0.040.. \n",
      "Training Loss: 0.061.. \n",
      "Training Loss: 0.075.. \n",
      "Training Loss: 0.093.. \n",
      "Training Loss: 0.119.. \n",
      "Training Loss: 0.136.. \n",
      "Training Loss: 0.152.. \n",
      "Training Loss: 0.178.. \n",
      "Training Loss: 0.204.. \n",
      "Model Accuracy: 78.382.. \n",
      "Epoch: 116/300.. \n",
      "Training Loss: 0.013.. \n",
      "Training Loss: 0.039.. \n",
      "Training Loss: 0.061.. \n",
      "Training Loss: 0.075.. \n",
      "Training Loss: 0.092.. \n",
      "Training Loss: 0.119.. \n",
      "Training Loss: 0.136.. \n",
      "Training Loss: 0.152.. \n",
      "Training Loss: 0.177.. \n",
      "Training Loss: 0.203.. \n",
      "Model Accuracy: 78.447.. \n",
      "Epoch: 117/300.. \n",
      "Training Loss: 0.013.. \n",
      "Training Loss: 0.039.. \n",
      "Training Loss: 0.061.. \n",
      "Training Loss: 0.074.. \n",
      "Training Loss: 0.092.. \n",
      "Training Loss: 0.118.. \n",
      "Training Loss: 0.136.. \n",
      "Training Loss: 0.151.. \n",
      "Training Loss: 0.176.. \n",
      "Training Loss: 0.203.. \n",
      "Model Accuracy: 78.511.. \n",
      "Epoch: 118/300.. \n",
      "Training Loss: 0.013.. \n",
      "Training Loss: 0.039.. \n",
      "Training Loss: 0.061.. \n",
      "Training Loss: 0.074.. \n",
      "Training Loss: 0.092.. \n",
      "Training Loss: 0.118.. \n",
      "Training Loss: 0.135.. \n",
      "Training Loss: 0.151.. \n",
      "Training Loss: 0.176.. \n",
      "Training Loss: 0.202.. \n",
      "Model Accuracy: 78.576.. \n",
      "Epoch: 119/300.. \n",
      "Training Loss: 0.013.. \n",
      "Training Loss: 0.039.. \n",
      "Training Loss: 0.060.. \n",
      "Training Loss: 0.074.. \n",
      "Training Loss: 0.092.. \n",
      "Training Loss: 0.117.. \n",
      "Training Loss: 0.135.. \n",
      "Training Loss: 0.150.. \n",
      "Training Loss: 0.175.. \n",
      "Training Loss: 0.201.. \n",
      "Model Accuracy: 78.639.. \n",
      "Epoch: 120/300.. \n",
      "Training Loss: 0.013.. \n",
      "Training Loss: 0.039.. \n",
      "Training Loss: 0.060.. \n",
      "Training Loss: 0.074.. \n",
      "Training Loss: 0.091.. \n",
      "Training Loss: 0.117.. \n",
      "Training Loss: 0.134.. \n",
      "Training Loss: 0.150.. \n",
      "Training Loss: 0.175.. \n",
      "Training Loss: 0.200.. \n",
      "Model Accuracy: 78.702.. \n",
      "Epoch: 121/300.. \n",
      "Training Loss: 0.013.. \n",
      "Training Loss: 0.039.. \n",
      "Training Loss: 0.060.. \n",
      "Training Loss: 0.073.. \n",
      "Training Loss: 0.091.. \n",
      "Training Loss: 0.117.. \n",
      "Training Loss: 0.134.. \n",
      "Training Loss: 0.149.. \n",
      "Training Loss: 0.174.. \n",
      "Training Loss: 0.200.. \n",
      "Model Accuracy: 78.765.. \n",
      "Epoch: 122/300.. \n",
      "Training Loss: 0.013.. \n",
      "Training Loss: 0.039.. \n",
      "Training Loss: 0.060.. \n",
      "Training Loss: 0.073.. \n",
      "Training Loss: 0.091.. \n",
      "Training Loss: 0.116.. \n",
      "Training Loss: 0.134.. \n",
      "Training Loss: 0.149.. \n",
      "Training Loss: 0.174.. \n",
      "Training Loss: 0.199.. \n",
      "Model Accuracy: 78.827.. \n",
      "Epoch: 123/300.. \n",
      "Training Loss: 0.013.. \n",
      "Training Loss: 0.038.. \n",
      "Training Loss: 0.060.. \n",
      "Training Loss: 0.073.. \n",
      "Training Loss: 0.091.. \n",
      "Training Loss: 0.116.. \n",
      "Training Loss: 0.133.. \n",
      "Training Loss: 0.148.. \n",
      "Training Loss: 0.173.. \n",
      "Training Loss: 0.198.. \n",
      "Model Accuracy: 78.889.. \n",
      "Epoch: 124/300.. \n",
      "Training Loss: 0.013.. \n",
      "Training Loss: 0.038.. \n",
      "Training Loss: 0.059.. \n",
      "Training Loss: 0.073.. \n",
      "Training Loss: 0.090.. \n",
      "Training Loss: 0.116.. \n",
      "Training Loss: 0.133.. \n",
      "Training Loss: 0.148.. \n",
      "Training Loss: 0.172.. \n",
      "Training Loss: 0.198.. \n",
      "Model Accuracy: 78.950.. \n",
      "Epoch: 125/300.. \n",
      "Training Loss: 0.013.. \n",
      "Training Loss: 0.038.. \n",
      "Training Loss: 0.059.. \n",
      "Training Loss: 0.072.. \n",
      "Training Loss: 0.090.. \n",
      "Training Loss: 0.115.. \n",
      "Training Loss: 0.133.. \n",
      "Training Loss: 0.147.. \n",
      "Training Loss: 0.172.. \n",
      "Training Loss: 0.197.. \n",
      "Model Accuracy: 79.011.. \n",
      "Epoch: 126/300.. \n",
      "Training Loss: 0.013.. \n",
      "Training Loss: 0.038.. \n",
      "Training Loss: 0.059.. \n",
      "Training Loss: 0.072.. \n",
      "Training Loss: 0.090.. \n",
      "Training Loss: 0.115.. \n",
      "Training Loss: 0.132.. \n",
      "Training Loss: 0.147.. \n",
      "Training Loss: 0.171.. \n",
      "Training Loss: 0.196.. \n",
      "Model Accuracy: 79.071.. \n",
      "Epoch: 127/300.. \n",
      "Training Loss: 0.013.. \n",
      "Training Loss: 0.038.. \n",
      "Training Loss: 0.059.. \n",
      "Training Loss: 0.072.. \n",
      "Training Loss: 0.089.. \n",
      "Training Loss: 0.115.. \n",
      "Training Loss: 0.132.. \n",
      "Training Loss: 0.146.. \n",
      "Training Loss: 0.171.. \n",
      "Training Loss: 0.196.. \n",
      "Model Accuracy: 79.131.. \n",
      "Epoch: 128/300.. \n",
      "Training Loss: 0.013.. \n",
      "Training Loss: 0.038.. \n",
      "Training Loss: 0.059.. \n",
      "Training Loss: 0.072.. \n",
      "Training Loss: 0.089.. \n",
      "Training Loss: 0.114.. \n",
      "Training Loss: 0.131.. \n",
      "Training Loss: 0.146.. \n",
      "Training Loss: 0.170.. \n",
      "Training Loss: 0.195.. \n",
      "Model Accuracy: 79.191.. \n",
      "Epoch: 129/300.. \n",
      "Training Loss: 0.013.. \n",
      "Training Loss: 0.038.. \n",
      "Training Loss: 0.058.. \n",
      "Training Loss: 0.072.. \n",
      "Training Loss: 0.089.. \n",
      "Training Loss: 0.114.. \n",
      "Training Loss: 0.131.. \n",
      "Training Loss: 0.146.. \n",
      "Training Loss: 0.170.. \n",
      "Training Loss: 0.194.. \n",
      "Model Accuracy: 79.250.. \n",
      "Epoch: 130/300.. \n",
      "Training Loss: 0.013.. \n",
      "Training Loss: 0.037.. \n",
      "Training Loss: 0.058.. \n",
      "Training Loss: 0.071.. \n",
      "Training Loss: 0.089.. \n",
      "Training Loss: 0.114.. \n",
      "Training Loss: 0.131.. \n",
      "Training Loss: 0.145.. \n",
      "Training Loss: 0.169.. \n",
      "Training Loss: 0.194.. \n",
      "Model Accuracy: 79.309.. \n",
      "Epoch: 131/300.. \n",
      "Training Loss: 0.012.. \n",
      "Training Loss: 0.037.. \n",
      "Training Loss: 0.058.. \n",
      "Training Loss: 0.071.. \n",
      "Training Loss: 0.089.. \n",
      "Training Loss: 0.113.. \n",
      "Training Loss: 0.130.. \n",
      "Training Loss: 0.145.. \n",
      "Training Loss: 0.169.. \n",
      "Training Loss: 0.193.. \n",
      "Model Accuracy: 79.367.. \n",
      "Epoch: 132/300.. \n",
      "Training Loss: 0.012.. \n",
      "Training Loss: 0.037.. \n",
      "Training Loss: 0.058.. \n",
      "Training Loss: 0.071.. \n",
      "Training Loss: 0.088.. \n",
      "Training Loss: 0.113.. \n",
      "Training Loss: 0.130.. \n",
      "Training Loss: 0.144.. \n",
      "Training Loss: 0.168.. \n",
      "Training Loss: 0.192.. \n",
      "Model Accuracy: 79.425.. \n",
      "Epoch: 133/300.. \n",
      "Training Loss: 0.012.. \n",
      "Training Loss: 0.037.. \n",
      "Training Loss: 0.058.. \n",
      "Training Loss: 0.071.. \n",
      "Training Loss: 0.088.. \n",
      "Training Loss: 0.113.. \n",
      "Training Loss: 0.130.. \n",
      "Training Loss: 0.144.. \n",
      "Training Loss: 0.168.. \n",
      "Training Loss: 0.192.. \n",
      "Model Accuracy: 79.482.. \n",
      "Epoch: 134/300.. \n",
      "Training Loss: 0.012.. \n",
      "Training Loss: 0.037.. \n",
      "Training Loss: 0.057.. \n",
      "Training Loss: 0.071.. \n",
      "Training Loss: 0.088.. \n",
      "Training Loss: 0.113.. \n",
      "Training Loss: 0.129.. \n",
      "Training Loss: 0.143.. \n",
      "Training Loss: 0.167.. \n",
      "Training Loss: 0.191.. \n",
      "Model Accuracy: 79.539.. \n",
      "Epoch: 135/300.. \n",
      "Training Loss: 0.012.. \n",
      "Training Loss: 0.037.. \n",
      "Training Loss: 0.057.. \n",
      "Training Loss: 0.070.. \n",
      "Training Loss: 0.088.. \n",
      "Training Loss: 0.112.. \n",
      "Training Loss: 0.129.. \n",
      "Training Loss: 0.143.. \n",
      "Training Loss: 0.167.. \n",
      "Training Loss: 0.191.. \n",
      "Model Accuracy: 79.595.. \n",
      "Epoch: 136/300.. \n",
      "Training Loss: 0.012.. \n",
      "Training Loss: 0.037.. \n",
      "Training Loss: 0.057.. \n",
      "Training Loss: 0.070.. \n",
      "Training Loss: 0.087.. \n",
      "Training Loss: 0.112.. \n",
      "Training Loss: 0.129.. \n",
      "Training Loss: 0.143.. \n",
      "Training Loss: 0.166.. \n",
      "Training Loss: 0.190.. \n",
      "Model Accuracy: 79.651.. \n",
      "Epoch: 137/300.. \n",
      "Training Loss: 0.012.. \n",
      "Training Loss: 0.036.. \n",
      "Training Loss: 0.057.. \n",
      "Training Loss: 0.070.. \n",
      "Training Loss: 0.087.. \n",
      "Training Loss: 0.112.. \n",
      "Training Loss: 0.128.. \n",
      "Training Loss: 0.142.. \n",
      "Training Loss: 0.166.. \n",
      "Training Loss: 0.189.. \n",
      "Model Accuracy: 79.707.. \n",
      "Epoch: 138/300.. \n",
      "Training Loss: 0.012.. \n",
      "Training Loss: 0.036.. \n",
      "Training Loss: 0.057.. \n",
      "Training Loss: 0.070.. \n",
      "Training Loss: 0.087.. \n",
      "Training Loss: 0.111.. \n",
      "Training Loss: 0.128.. \n",
      "Training Loss: 0.142.. \n",
      "Training Loss: 0.165.. \n",
      "Training Loss: 0.189.. \n",
      "Model Accuracy: 79.762.. \n",
      "Epoch: 139/300.. \n",
      "Training Loss: 0.012.. \n",
      "Training Loss: 0.036.. \n",
      "Training Loss: 0.057.. \n",
      "Training Loss: 0.070.. \n",
      "Training Loss: 0.087.. \n",
      "Training Loss: 0.111.. \n",
      "Training Loss: 0.128.. \n",
      "Training Loss: 0.141.. \n",
      "Training Loss: 0.165.. \n",
      "Training Loss: 0.188.. \n",
      "Model Accuracy: 79.816.. \n",
      "Epoch: 140/300.. \n",
      "Training Loss: 0.012.. \n",
      "Training Loss: 0.036.. \n",
      "Training Loss: 0.056.. \n",
      "Training Loss: 0.069.. \n",
      "Training Loss: 0.087.. \n",
      "Training Loss: 0.111.. \n",
      "Training Loss: 0.127.. \n",
      "Training Loss: 0.141.. \n",
      "Training Loss: 0.164.. \n",
      "Training Loss: 0.188.. \n",
      "Model Accuracy: 79.870.. \n",
      "Epoch: 141/300.. \n",
      "Training Loss: 0.012.. \n",
      "Training Loss: 0.036.. \n",
      "Training Loss: 0.056.. \n",
      "Training Loss: 0.069.. \n",
      "Training Loss: 0.086.. \n",
      "Training Loss: 0.111.. \n",
      "Training Loss: 0.127.. \n",
      "Training Loss: 0.141.. \n",
      "Training Loss: 0.164.. \n",
      "Training Loss: 0.187.. \n",
      "Model Accuracy: 79.924.. \n",
      "Epoch: 142/300.. \n",
      "Training Loss: 0.012.. \n",
      "Training Loss: 0.036.. \n",
      "Training Loss: 0.056.. \n",
      "Training Loss: 0.069.. \n",
      "Training Loss: 0.086.. \n",
      "Training Loss: 0.110.. \n",
      "Training Loss: 0.127.. \n",
      "Training Loss: 0.140.. \n",
      "Training Loss: 0.163.. \n",
      "Training Loss: 0.186.. \n",
      "Model Accuracy: 79.977.. \n",
      "Epoch: 143/300.. \n",
      "Training Loss: 0.012.. \n",
      "Training Loss: 0.036.. \n",
      "Training Loss: 0.056.. \n",
      "Training Loss: 0.069.. \n",
      "Training Loss: 0.086.. \n",
      "Training Loss: 0.110.. \n",
      "Training Loss: 0.127.. \n",
      "Training Loss: 0.140.. \n",
      "Training Loss: 0.163.. \n",
      "Training Loss: 0.186.. \n",
      "Model Accuracy: 80.030.. \n",
      "Epoch: 144/300.. \n",
      "Training Loss: 0.012.. \n",
      "Training Loss: 0.036.. \n",
      "Training Loss: 0.056.. \n",
      "Training Loss: 0.069.. \n",
      "Training Loss: 0.086.. \n",
      "Training Loss: 0.110.. \n",
      "Training Loss: 0.126.. \n",
      "Training Loss: 0.140.. \n",
      "Training Loss: 0.162.. \n",
      "Training Loss: 0.185.. \n",
      "Model Accuracy: 80.083.. \n",
      "Epoch: 145/300.. \n",
      "Training Loss: 0.012.. \n",
      "Training Loss: 0.035.. \n",
      "Training Loss: 0.056.. \n",
      "Training Loss: 0.069.. \n",
      "Training Loss: 0.086.. \n",
      "Training Loss: 0.109.. \n",
      "Training Loss: 0.126.. \n",
      "Training Loss: 0.139.. \n",
      "Training Loss: 0.162.. \n",
      "Training Loss: 0.185.. \n",
      "Model Accuracy: 80.134.. \n",
      "Epoch: 146/300.. \n",
      "Training Loss: 0.012.. \n",
      "Training Loss: 0.035.. \n",
      "Training Loss: 0.055.. \n",
      "Training Loss: 0.068.. \n",
      "Training Loss: 0.085.. \n",
      "Training Loss: 0.109.. \n",
      "Training Loss: 0.126.. \n",
      "Training Loss: 0.139.. \n",
      "Training Loss: 0.161.. \n",
      "Training Loss: 0.184.. \n",
      "Model Accuracy: 80.186.. \n",
      "Epoch: 147/300.. \n",
      "Training Loss: 0.012.. \n",
      "Training Loss: 0.035.. \n",
      "Training Loss: 0.055.. \n",
      "Training Loss: 0.068.. \n",
      "Training Loss: 0.085.. \n",
      "Training Loss: 0.109.. \n",
      "Training Loss: 0.125.. \n",
      "Training Loss: 0.139.. \n",
      "Training Loss: 0.161.. \n",
      "Training Loss: 0.184.. \n",
      "Model Accuracy: 80.237.. \n",
      "Epoch: 148/300.. \n",
      "Training Loss: 0.011.. \n",
      "Training Loss: 0.035.. \n",
      "Training Loss: 0.055.. \n",
      "Training Loss: 0.068.. \n",
      "Training Loss: 0.085.. \n",
      "Training Loss: 0.109.. \n",
      "Training Loss: 0.125.. \n",
      "Training Loss: 0.138.. \n",
      "Training Loss: 0.160.. \n",
      "Training Loss: 0.183.. \n",
      "Model Accuracy: 80.287.. \n",
      "Epoch: 149/300.. \n",
      "Training Loss: 0.011.. \n",
      "Training Loss: 0.035.. \n",
      "Training Loss: 0.055.. \n",
      "Training Loss: 0.068.. \n",
      "Training Loss: 0.085.. \n",
      "Training Loss: 0.108.. \n",
      "Training Loss: 0.125.. \n",
      "Training Loss: 0.138.. \n",
      "Training Loss: 0.160.. \n",
      "Training Loss: 0.183.. \n",
      "Model Accuracy: 80.338.. \n",
      "Epoch: 150/300.. \n",
      "Training Loss: 0.011.. \n",
      "Training Loss: 0.035.. \n",
      "Training Loss: 0.055.. \n",
      "Training Loss: 0.068.. \n",
      "Training Loss: 0.085.. \n",
      "Training Loss: 0.108.. \n",
      "Training Loss: 0.125.. \n",
      "Training Loss: 0.138.. \n",
      "Training Loss: 0.160.. \n",
      "Training Loss: 0.182.. \n",
      "Model Accuracy: 80.387.. \n",
      "Epoch: 151/300.. \n",
      "Training Loss: 0.011.. \n",
      "Training Loss: 0.035.. \n",
      "Training Loss: 0.055.. \n",
      "Training Loss: 0.068.. \n",
      "Training Loss: 0.084.. \n",
      "Training Loss: 0.108.. \n",
      "Training Loss: 0.124.. \n",
      "Training Loss: 0.137.. \n",
      "Training Loss: 0.159.. \n",
      "Training Loss: 0.182.. \n",
      "Model Accuracy: 80.436.. \n",
      "Epoch: 152/300.. \n",
      "Training Loss: 0.011.. \n",
      "Training Loss: 0.035.. \n",
      "Training Loss: 0.054.. \n",
      "Training Loss: 0.067.. \n",
      "Training Loss: 0.084.. \n",
      "Training Loss: 0.108.. \n",
      "Training Loss: 0.124.. \n",
      "Training Loss: 0.137.. \n",
      "Training Loss: 0.159.. \n",
      "Training Loss: 0.181.. \n",
      "Model Accuracy: 80.485.. \n",
      "Epoch: 153/300.. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.011.. \n",
      "Training Loss: 0.035.. \n",
      "Training Loss: 0.054.. \n",
      "Training Loss: 0.067.. \n",
      "Training Loss: 0.084.. \n",
      "Training Loss: 0.107.. \n",
      "Training Loss: 0.124.. \n",
      "Training Loss: 0.137.. \n",
      "Training Loss: 0.158.. \n",
      "Training Loss: 0.180.. \n",
      "Model Accuracy: 80.534.. \n",
      "Epoch: 154/300.. \n",
      "Training Loss: 0.011.. \n",
      "Training Loss: 0.034.. \n",
      "Training Loss: 0.054.. \n",
      "Training Loss: 0.067.. \n",
      "Training Loss: 0.084.. \n",
      "Training Loss: 0.107.. \n",
      "Training Loss: 0.124.. \n",
      "Training Loss: 0.136.. \n",
      "Training Loss: 0.158.. \n",
      "Training Loss: 0.180.. \n",
      "Model Accuracy: 80.581.. \n",
      "Epoch: 155/300.. \n",
      "Training Loss: 0.011.. \n",
      "Training Loss: 0.034.. \n",
      "Training Loss: 0.054.. \n",
      "Training Loss: 0.067.. \n",
      "Training Loss: 0.084.. \n",
      "Training Loss: 0.107.. \n",
      "Training Loss: 0.123.. \n",
      "Training Loss: 0.136.. \n",
      "Training Loss: 0.158.. \n",
      "Training Loss: 0.179.. \n",
      "Model Accuracy: 80.629.. \n",
      "Epoch: 156/300.. \n",
      "Training Loss: 0.011.. \n",
      "Training Loss: 0.034.. \n",
      "Training Loss: 0.054.. \n",
      "Training Loss: 0.067.. \n",
      "Training Loss: 0.084.. \n",
      "Training Loss: 0.107.. \n",
      "Training Loss: 0.123.. \n",
      "Training Loss: 0.136.. \n",
      "Training Loss: 0.157.. \n",
      "Training Loss: 0.179.. \n",
      "Model Accuracy: 80.676.. \n",
      "Epoch: 157/300.. \n",
      "Training Loss: 0.011.. \n",
      "Training Loss: 0.034.. \n",
      "Training Loss: 0.054.. \n",
      "Training Loss: 0.067.. \n",
      "Training Loss: 0.083.. \n",
      "Training Loss: 0.107.. \n",
      "Training Loss: 0.123.. \n",
      "Training Loss: 0.135.. \n",
      "Training Loss: 0.157.. \n",
      "Training Loss: 0.179.. \n",
      "Model Accuracy: 80.722.. \n",
      "Epoch: 158/300.. \n",
      "Training Loss: 0.011.. \n",
      "Training Loss: 0.034.. \n",
      "Training Loss: 0.054.. \n",
      "Training Loss: 0.067.. \n",
      "Training Loss: 0.083.. \n",
      "Training Loss: 0.106.. \n",
      "Training Loss: 0.123.. \n",
      "Training Loss: 0.135.. \n",
      "Training Loss: 0.157.. \n",
      "Training Loss: 0.178.. \n",
      "Model Accuracy: 80.768.. \n",
      "Epoch: 159/300.. \n",
      "Training Loss: 0.011.. \n",
      "Training Loss: 0.034.. \n",
      "Training Loss: 0.054.. \n",
      "Training Loss: 0.067.. \n",
      "Training Loss: 0.083.. \n",
      "Training Loss: 0.106.. \n",
      "Training Loss: 0.122.. \n",
      "Training Loss: 0.135.. \n",
      "Training Loss: 0.156.. \n",
      "Training Loss: 0.178.. \n",
      "Model Accuracy: 80.814.. \n",
      "Epoch: 160/300.. \n",
      "Training Loss: 0.011.. \n",
      "Training Loss: 0.034.. \n",
      "Training Loss: 0.053.. \n",
      "Training Loss: 0.066.. \n",
      "Training Loss: 0.083.. \n",
      "Training Loss: 0.106.. \n",
      "Training Loss: 0.122.. \n",
      "Training Loss: 0.135.. \n",
      "Training Loss: 0.156.. \n",
      "Training Loss: 0.177.. \n",
      "Model Accuracy: 80.859.. \n",
      "Epoch: 161/300.. \n",
      "Training Loss: 0.011.. \n",
      "Training Loss: 0.034.. \n",
      "Training Loss: 0.053.. \n",
      "Training Loss: 0.066.. \n",
      "Training Loss: 0.083.. \n",
      "Training Loss: 0.106.. \n",
      "Training Loss: 0.122.. \n",
      "Training Loss: 0.134.. \n",
      "Training Loss: 0.155.. \n",
      "Training Loss: 0.177.. \n",
      "Model Accuracy: 80.904.. \n",
      "Epoch: 162/300.. \n",
      "Training Loss: 0.011.. \n",
      "Training Loss: 0.034.. \n",
      "Training Loss: 0.053.. \n",
      "Training Loss: 0.066.. \n",
      "Training Loss: 0.083.. \n",
      "Training Loss: 0.106.. \n",
      "Training Loss: 0.122.. \n",
      "Training Loss: 0.134.. \n",
      "Training Loss: 0.155.. \n",
      "Training Loss: 0.176.. \n",
      "Model Accuracy: 80.948.. \n",
      "Epoch: 163/300.. \n",
      "Training Loss: 0.011.. \n",
      "Training Loss: 0.034.. \n",
      "Training Loss: 0.053.. \n",
      "Training Loss: 0.066.. \n",
      "Training Loss: 0.083.. \n",
      "Training Loss: 0.105.. \n",
      "Training Loss: 0.121.. \n",
      "Training Loss: 0.134.. \n",
      "Training Loss: 0.155.. \n",
      "Training Loss: 0.176.. \n",
      "Model Accuracy: 80.992.. \n",
      "Epoch: 164/300.. \n",
      "Training Loss: 0.011.. \n",
      "Training Loss: 0.033.. \n",
      "Training Loss: 0.053.. \n",
      "Training Loss: 0.066.. \n",
      "Training Loss: 0.082.. \n",
      "Training Loss: 0.105.. \n",
      "Training Loss: 0.121.. \n",
      "Training Loss: 0.133.. \n",
      "Training Loss: 0.154.. \n",
      "Training Loss: 0.175.. \n",
      "Model Accuracy: 81.035.. \n",
      "Epoch: 165/300.. \n",
      "Training Loss: 0.011.. \n",
      "Training Loss: 0.033.. \n",
      "Training Loss: 0.053.. \n",
      "Training Loss: 0.066.. \n",
      "Training Loss: 0.082.. \n",
      "Training Loss: 0.105.. \n",
      "Training Loss: 0.121.. \n",
      "Training Loss: 0.133.. \n",
      "Training Loss: 0.154.. \n",
      "Training Loss: 0.175.. \n",
      "Model Accuracy: 81.078.. \n",
      "Epoch: 166/300.. \n",
      "Training Loss: 0.011.. \n",
      "Training Loss: 0.033.. \n",
      "Training Loss: 0.053.. \n",
      "Training Loss: 0.066.. \n",
      "Training Loss: 0.082.. \n",
      "Training Loss: 0.105.. \n",
      "Training Loss: 0.121.. \n",
      "Training Loss: 0.133.. \n",
      "Training Loss: 0.154.. \n",
      "Training Loss: 0.174.. \n",
      "Model Accuracy: 81.120.. \n",
      "Epoch: 167/300.. \n",
      "Training Loss: 0.011.. \n",
      "Training Loss: 0.033.. \n",
      "Training Loss: 0.052.. \n",
      "Training Loss: 0.066.. \n",
      "Training Loss: 0.082.. \n",
      "Training Loss: 0.105.. \n",
      "Training Loss: 0.121.. \n",
      "Training Loss: 0.133.. \n",
      "Training Loss: 0.153.. \n",
      "Training Loss: 0.174.. \n",
      "Model Accuracy: 81.162.. \n",
      "Epoch: 168/300.. \n",
      "Training Loss: 0.011.. \n",
      "Training Loss: 0.033.. \n",
      "Training Loss: 0.052.. \n",
      "Training Loss: 0.066.. \n",
      "Training Loss: 0.082.. \n",
      "Training Loss: 0.104.. \n",
      "Training Loss: 0.120.. \n",
      "Training Loss: 0.132.. \n",
      "Training Loss: 0.153.. \n",
      "Training Loss: 0.174.. \n",
      "Model Accuracy: 81.204.. \n",
      "Epoch: 169/300.. \n",
      "Training Loss: 0.010.. \n",
      "Training Loss: 0.033.. \n",
      "Training Loss: 0.052.. \n",
      "Training Loss: 0.065.. \n",
      "Training Loss: 0.082.. \n",
      "Training Loss: 0.104.. \n",
      "Training Loss: 0.120.. \n",
      "Training Loss: 0.132.. \n",
      "Training Loss: 0.153.. \n",
      "Training Loss: 0.173.. \n",
      "Model Accuracy: 81.245.. \n",
      "Epoch: 170/300.. \n",
      "Training Loss: 0.010.. \n",
      "Training Loss: 0.033.. \n",
      "Training Loss: 0.052.. \n",
      "Training Loss: 0.065.. \n",
      "Training Loss: 0.082.. \n",
      "Training Loss: 0.104.. \n",
      "Training Loss: 0.120.. \n",
      "Training Loss: 0.132.. \n",
      "Training Loss: 0.152.. \n",
      "Training Loss: 0.173.. \n",
      "Model Accuracy: 81.286.. \n",
      "Epoch: 171/300.. \n",
      "Training Loss: 0.010.. \n",
      "Training Loss: 0.033.. \n",
      "Training Loss: 0.052.. \n",
      "Training Loss: 0.065.. \n",
      "Training Loss: 0.082.. \n",
      "Training Loss: 0.104.. \n",
      "Training Loss: 0.120.. \n",
      "Training Loss: 0.132.. \n",
      "Training Loss: 0.152.. \n",
      "Training Loss: 0.172.. \n",
      "Model Accuracy: 81.326.. \n",
      "Epoch: 172/300.. \n",
      "Training Loss: 0.010.. \n",
      "Training Loss: 0.033.. \n",
      "Training Loss: 0.052.. \n",
      "Training Loss: 0.065.. \n",
      "Training Loss: 0.081.. \n",
      "Training Loss: 0.104.. \n",
      "Training Loss: 0.120.. \n",
      "Training Loss: 0.132.. \n",
      "Training Loss: 0.152.. \n",
      "Training Loss: 0.172.. \n",
      "Model Accuracy: 81.366.. \n",
      "Epoch: 173/300.. \n",
      "Training Loss: 0.010.. \n",
      "Training Loss: 0.033.. \n",
      "Training Loss: 0.052.. \n",
      "Training Loss: 0.065.. \n",
      "Training Loss: 0.081.. \n",
      "Training Loss: 0.104.. \n",
      "Training Loss: 0.120.. \n",
      "Training Loss: 0.131.. \n",
      "Training Loss: 0.152.. \n",
      "Training Loss: 0.172.. \n",
      "Model Accuracy: 81.405.. \n",
      "Epoch: 174/300.. \n",
      "Training Loss: 0.010.. \n",
      "Training Loss: 0.032.. \n",
      "Training Loss: 0.052.. \n",
      "Training Loss: 0.065.. \n",
      "Training Loss: 0.081.. \n",
      "Training Loss: 0.103.. \n",
      "Training Loss: 0.119.. \n",
      "Training Loss: 0.131.. \n",
      "Training Loss: 0.151.. \n",
      "Training Loss: 0.171.. \n",
      "Model Accuracy: 81.444.. \n",
      "Epoch: 175/300.. \n",
      "Training Loss: 0.010.. \n",
      "Training Loss: 0.032.. \n",
      "Training Loss: 0.052.. \n",
      "Training Loss: 0.065.. \n",
      "Training Loss: 0.081.. \n",
      "Training Loss: 0.103.. \n",
      "Training Loss: 0.119.. \n",
      "Training Loss: 0.131.. \n",
      "Training Loss: 0.151.. \n",
      "Training Loss: 0.171.. \n",
      "Model Accuracy: 81.483.. \n",
      "Epoch: 176/300.. \n",
      "Training Loss: 0.010.. \n",
      "Training Loss: 0.032.. \n",
      "Training Loss: 0.051.. \n",
      "Training Loss: 0.065.. \n",
      "Training Loss: 0.081.. \n",
      "Training Loss: 0.103.. \n",
      "Training Loss: 0.119.. \n",
      "Training Loss: 0.131.. \n",
      "Training Loss: 0.151.. \n",
      "Training Loss: 0.170.. \n",
      "Model Accuracy: 81.521.. \n",
      "Epoch: 177/300.. \n",
      "Training Loss: 0.010.. \n",
      "Training Loss: 0.032.. \n",
      "Training Loss: 0.051.. \n",
      "Training Loss: 0.065.. \n",
      "Training Loss: 0.081.. \n",
      "Training Loss: 0.103.. \n",
      "Training Loss: 0.119.. \n",
      "Training Loss: 0.130.. \n",
      "Training Loss: 0.150.. \n",
      "Training Loss: 0.170.. \n",
      "Model Accuracy: 81.558.. \n",
      "Epoch: 178/300.. \n",
      "Training Loss: 0.010.. \n",
      "Training Loss: 0.032.. \n",
      "Training Loss: 0.051.. \n",
      "Training Loss: 0.065.. \n",
      "Training Loss: 0.081.. \n",
      "Training Loss: 0.103.. \n",
      "Training Loss: 0.119.. \n",
      "Training Loss: 0.130.. \n",
      "Training Loss: 0.150.. \n",
      "Training Loss: 0.170.. \n",
      "Model Accuracy: 81.595.. \n",
      "Epoch: 179/300.. \n",
      "Training Loss: 0.010.. \n",
      "Training Loss: 0.032.. \n",
      "Training Loss: 0.051.. \n",
      "Training Loss: 0.065.. \n",
      "Training Loss: 0.081.. \n",
      "Training Loss: 0.103.. \n",
      "Training Loss: 0.119.. \n",
      "Training Loss: 0.130.. \n",
      "Training Loss: 0.150.. \n",
      "Training Loss: 0.169.. \n",
      "Model Accuracy: 81.632.. \n",
      "Epoch: 180/300.. \n",
      "Training Loss: 0.010.. \n",
      "Training Loss: 0.032.. \n",
      "Training Loss: 0.051.. \n",
      "Training Loss: 0.064.. \n",
      "Training Loss: 0.081.. \n",
      "Training Loss: 0.102.. \n",
      "Training Loss: 0.118.. \n",
      "Training Loss: 0.130.. \n",
      "Training Loss: 0.150.. \n",
      "Training Loss: 0.169.. \n",
      "Model Accuracy: 81.668.. \n",
      "Epoch: 181/300.. \n",
      "Training Loss: 0.010.. \n",
      "Training Loss: 0.032.. \n",
      "Training Loss: 0.051.. \n",
      "Training Loss: 0.064.. \n",
      "Training Loss: 0.081.. \n",
      "Training Loss: 0.102.. \n",
      "Training Loss: 0.118.. \n",
      "Training Loss: 0.130.. \n",
      "Training Loss: 0.149.. \n",
      "Training Loss: 0.169.. \n",
      "Model Accuracy: 81.704.. \n",
      "Epoch: 182/300.. \n",
      "Training Loss: 0.010.. \n",
      "Training Loss: 0.032.. \n",
      "Training Loss: 0.051.. \n",
      "Training Loss: 0.064.. \n",
      "Training Loss: 0.080.. \n",
      "Training Loss: 0.102.. \n",
      "Training Loss: 0.118.. \n",
      "Training Loss: 0.130.. \n",
      "Training Loss: 0.149.. \n",
      "Training Loss: 0.168.. \n",
      "Model Accuracy: 81.740.. \n",
      "Epoch: 183/300.. \n",
      "Training Loss: 0.010.. \n",
      "Training Loss: 0.032.. \n",
      "Training Loss: 0.051.. \n",
      "Training Loss: 0.064.. \n",
      "Training Loss: 0.080.. \n",
      "Training Loss: 0.102.. \n",
      "Training Loss: 0.118.. \n",
      "Training Loss: 0.129.. \n",
      "Training Loss: 0.149.. \n",
      "Training Loss: 0.168.. \n",
      "Model Accuracy: 81.775.. \n",
      "Epoch: 184/300.. \n",
      "Training Loss: 0.010.. \n",
      "Training Loss: 0.032.. \n",
      "Training Loss: 0.051.. \n",
      "Training Loss: 0.064.. \n",
      "Training Loss: 0.080.. \n",
      "Training Loss: 0.102.. \n",
      "Training Loss: 0.118.. \n",
      "Training Loss: 0.129.. \n",
      "Training Loss: 0.149.. \n",
      "Training Loss: 0.168.. \n",
      "Model Accuracy: 81.809.. \n",
      "Epoch: 185/300.. \n",
      "Training Loss: 0.010.. \n",
      "Training Loss: 0.032.. \n",
      "Training Loss: 0.051.. \n",
      "Training Loss: 0.064.. \n",
      "Training Loss: 0.080.. \n",
      "Training Loss: 0.102.. \n",
      "Training Loss: 0.118.. \n",
      "Training Loss: 0.129.. \n",
      "Training Loss: 0.148.. \n",
      "Training Loss: 0.167.. \n",
      "Model Accuracy: 81.843.. \n",
      "Epoch: 186/300.. \n",
      "Training Loss: 0.010.. \n",
      "Training Loss: 0.032.. \n",
      "Training Loss: 0.051.. \n",
      "Training Loss: 0.064.. \n",
      "Training Loss: 0.080.. \n",
      "Training Loss: 0.102.. \n",
      "Training Loss: 0.118.. \n",
      "Training Loss: 0.129.. \n",
      "Training Loss: 0.148.. \n",
      "Training Loss: 0.167.. \n",
      "Model Accuracy: 81.877.. \n",
      "Epoch: 187/300.. \n",
      "Training Loss: 0.010.. \n",
      "Training Loss: 0.032.. \n",
      "Training Loss: 0.050.. \n",
      "Training Loss: 0.064.. \n",
      "Training Loss: 0.080.. \n",
      "Training Loss: 0.102.. \n",
      "Training Loss: 0.117.. \n",
      "Training Loss: 0.129.. \n",
      "Training Loss: 0.148.. \n",
      "Training Loss: 0.167.. \n",
      "Model Accuracy: 81.910.. \n",
      "Epoch: 188/300.. \n",
      "Training Loss: 0.010.. \n",
      "Training Loss: 0.031.. \n",
      "Training Loss: 0.050.. \n",
      "Training Loss: 0.064.. \n",
      "Training Loss: 0.080.. \n",
      "Training Loss: 0.101.. \n",
      "Training Loss: 0.117.. \n",
      "Training Loss: 0.128.. \n",
      "Training Loss: 0.148.. \n",
      "Training Loss: 0.166.. \n",
      "Model Accuracy: 81.943.. \n",
      "Epoch: 189/300.. \n",
      "Training Loss: 0.010.. \n",
      "Training Loss: 0.031.. \n",
      "Training Loss: 0.050.. \n",
      "Training Loss: 0.064.. \n",
      "Training Loss: 0.080.. \n",
      "Training Loss: 0.101.. \n",
      "Training Loss: 0.117.. \n",
      "Training Loss: 0.128.. \n",
      "Training Loss: 0.148.. \n",
      "Training Loss: 0.166.. \n",
      "Model Accuracy: 81.975.. \n",
      "Epoch: 190/300.. \n",
      "Training Loss: 0.010.. \n",
      "Training Loss: 0.031.. \n",
      "Training Loss: 0.050.. \n",
      "Training Loss: 0.064.. \n",
      "Training Loss: 0.080.. \n",
      "Training Loss: 0.101.. \n",
      "Training Loss: 0.117.. \n",
      "Training Loss: 0.128.. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.147.. \n",
      "Training Loss: 0.166.. \n",
      "Model Accuracy: 82.007.. \n",
      "Epoch: 191/300.. \n",
      "Training Loss: 0.010.. \n",
      "Training Loss: 0.031.. \n",
      "Training Loss: 0.050.. \n",
      "Training Loss: 0.064.. \n",
      "Training Loss: 0.080.. \n",
      "Training Loss: 0.101.. \n",
      "Training Loss: 0.117.. \n",
      "Training Loss: 0.128.. \n",
      "Training Loss: 0.147.. \n",
      "Training Loss: 0.166.. \n",
      "Model Accuracy: 82.039.. \n",
      "Epoch: 192/300.. \n",
      "Training Loss: 0.010.. \n",
      "Training Loss: 0.031.. \n",
      "Training Loss: 0.050.. \n",
      "Training Loss: 0.064.. \n",
      "Training Loss: 0.080.. \n",
      "Training Loss: 0.101.. \n",
      "Training Loss: 0.117.. \n",
      "Training Loss: 0.128.. \n",
      "Training Loss: 0.147.. \n",
      "Training Loss: 0.165.. \n",
      "Model Accuracy: 82.070.. \n",
      "Epoch: 193/300.. \n",
      "Training Loss: 0.010.. \n",
      "Training Loss: 0.031.. \n",
      "Training Loss: 0.050.. \n",
      "Training Loss: 0.064.. \n",
      "Training Loss: 0.080.. \n",
      "Training Loss: 0.101.. \n",
      "Training Loss: 0.117.. \n",
      "Training Loss: 0.128.. \n",
      "Training Loss: 0.147.. \n",
      "Training Loss: 0.165.. \n",
      "Model Accuracy: 82.101.. \n",
      "Epoch: 194/300.. \n",
      "Training Loss: 0.010.. \n",
      "Training Loss: 0.031.. \n",
      "Training Loss: 0.050.. \n",
      "Training Loss: 0.064.. \n",
      "Training Loss: 0.080.. \n",
      "Training Loss: 0.101.. \n",
      "Training Loss: 0.117.. \n",
      "Training Loss: 0.128.. \n",
      "Training Loss: 0.146.. \n",
      "Training Loss: 0.165.. \n",
      "Model Accuracy: 82.131.. \n",
      "Epoch: 195/300.. \n",
      "Training Loss: 0.010.. \n",
      "Training Loss: 0.031.. \n",
      "Training Loss: 0.050.. \n",
      "Training Loss: 0.064.. \n",
      "Training Loss: 0.079.. \n",
      "Training Loss: 0.101.. \n",
      "Training Loss: 0.116.. \n",
      "Training Loss: 0.127.. \n",
      "Training Loss: 0.146.. \n",
      "Training Loss: 0.164.. \n",
      "Model Accuracy: 82.161.. \n",
      "Epoch: 196/300.. \n",
      "Training Loss: 0.010.. \n",
      "Training Loss: 0.031.. \n",
      "Training Loss: 0.050.. \n",
      "Training Loss: 0.064.. \n",
      "Training Loss: 0.079.. \n",
      "Training Loss: 0.101.. \n",
      "Training Loss: 0.116.. \n",
      "Training Loss: 0.127.. \n",
      "Training Loss: 0.146.. \n",
      "Training Loss: 0.164.. \n",
      "Model Accuracy: 82.190.. \n",
      "Epoch: 197/300.. \n",
      "Training Loss: 0.010.. \n",
      "Training Loss: 0.031.. \n",
      "Training Loss: 0.050.. \n",
      "Training Loss: 0.064.. \n",
      "Training Loss: 0.079.. \n",
      "Training Loss: 0.100.. \n",
      "Training Loss: 0.116.. \n",
      "Training Loss: 0.127.. \n",
      "Training Loss: 0.146.. \n",
      "Training Loss: 0.164.. \n",
      "Model Accuracy: 82.219.. \n",
      "Epoch: 198/300.. \n",
      "Training Loss: 0.010.. \n",
      "Training Loss: 0.031.. \n",
      "Training Loss: 0.050.. \n",
      "Training Loss: 0.064.. \n",
      "Training Loss: 0.079.. \n",
      "Training Loss: 0.100.. \n",
      "Training Loss: 0.116.. \n",
      "Training Loss: 0.127.. \n",
      "Training Loss: 0.146.. \n",
      "Training Loss: 0.164.. \n",
      "Model Accuracy: 82.248.. \n",
      "Epoch: 199/300.. \n",
      "Training Loss: 0.010.. \n",
      "Training Loss: 0.031.. \n",
      "Training Loss: 0.050.. \n",
      "Training Loss: 0.064.. \n",
      "Training Loss: 0.079.. \n",
      "Training Loss: 0.100.. \n",
      "Training Loss: 0.116.. \n",
      "Training Loss: 0.127.. \n",
      "Training Loss: 0.146.. \n",
      "Training Loss: 0.163.. \n",
      "Model Accuracy: 82.277.. \n",
      "Epoch: 200/300.. \n",
      "Training Loss: 0.010.. \n",
      "Training Loss: 0.031.. \n",
      "Training Loss: 0.049.. \n",
      "Training Loss: 0.063.. \n",
      "Training Loss: 0.079.. \n",
      "Training Loss: 0.100.. \n",
      "Training Loss: 0.116.. \n",
      "Training Loss: 0.127.. \n",
      "Training Loss: 0.145.. \n",
      "Training Loss: 0.163.. \n",
      "Model Accuracy: 82.304.. \n",
      "Epoch: 201/300.. \n",
      "Training Loss: 0.010.. \n",
      "Training Loss: 0.031.. \n",
      "Training Loss: 0.049.. \n",
      "Training Loss: 0.063.. \n",
      "Training Loss: 0.079.. \n",
      "Training Loss: 0.100.. \n",
      "Training Loss: 0.116.. \n",
      "Training Loss: 0.127.. \n",
      "Training Loss: 0.145.. \n",
      "Training Loss: 0.163.. \n",
      "Model Accuracy: 82.332.. \n",
      "Epoch: 202/300.. \n",
      "Training Loss: 0.010.. \n",
      "Training Loss: 0.031.. \n",
      "Training Loss: 0.049.. \n",
      "Training Loss: 0.063.. \n",
      "Training Loss: 0.079.. \n",
      "Training Loss: 0.100.. \n",
      "Training Loss: 0.116.. \n",
      "Training Loss: 0.127.. \n",
      "Training Loss: 0.145.. \n",
      "Training Loss: 0.163.. \n",
      "Model Accuracy: 82.359.. \n",
      "Epoch: 203/300.. \n",
      "Training Loss: 0.010.. \n",
      "Training Loss: 0.031.. \n",
      "Training Loss: 0.049.. \n",
      "Training Loss: 0.063.. \n",
      "Training Loss: 0.079.. \n",
      "Training Loss: 0.100.. \n",
      "Training Loss: 0.116.. \n",
      "Training Loss: 0.127.. \n",
      "Training Loss: 0.145.. \n",
      "Training Loss: 0.162.. \n",
      "Model Accuracy: 82.386.. \n",
      "Epoch: 204/300.. \n",
      "Training Loss: 0.010.. \n",
      "Training Loss: 0.031.. \n",
      "Training Loss: 0.049.. \n",
      "Training Loss: 0.063.. \n",
      "Training Loss: 0.079.. \n",
      "Training Loss: 0.100.. \n",
      "Training Loss: 0.116.. \n",
      "Training Loss: 0.126.. \n",
      "Training Loss: 0.145.. \n",
      "Training Loss: 0.162.. \n",
      "Model Accuracy: 82.412.. \n",
      "Epoch: 205/300.. \n",
      "Training Loss: 0.010.. \n",
      "Training Loss: 0.030.. \n",
      "Training Loss: 0.049.. \n",
      "Training Loss: 0.063.. \n",
      "Training Loss: 0.079.. \n",
      "Training Loss: 0.100.. \n",
      "Training Loss: 0.116.. \n",
      "Training Loss: 0.126.. \n",
      "Training Loss: 0.145.. \n",
      "Training Loss: 0.162.. \n",
      "Model Accuracy: 82.438.. \n",
      "Epoch: 206/300.. \n",
      "Training Loss: 0.009.. \n",
      "Training Loss: 0.030.. \n",
      "Training Loss: 0.049.. \n",
      "Training Loss: 0.063.. \n",
      "Training Loss: 0.079.. \n",
      "Training Loss: 0.100.. \n",
      "Training Loss: 0.115.. \n",
      "Training Loss: 0.126.. \n",
      "Training Loss: 0.144.. \n",
      "Training Loss: 0.162.. \n",
      "Model Accuracy: 82.464.. \n",
      "Epoch: 207/300.. \n",
      "Training Loss: 0.009.. \n",
      "Training Loss: 0.030.. \n",
      "Training Loss: 0.049.. \n",
      "Training Loss: 0.063.. \n",
      "Training Loss: 0.079.. \n",
      "Training Loss: 0.100.. \n",
      "Training Loss: 0.115.. \n",
      "Training Loss: 0.126.. \n",
      "Training Loss: 0.144.. \n",
      "Training Loss: 0.162.. \n",
      "Model Accuracy: 82.489.. \n",
      "Epoch: 208/300.. \n",
      "Training Loss: 0.009.. \n",
      "Training Loss: 0.030.. \n",
      "Training Loss: 0.049.. \n",
      "Training Loss: 0.063.. \n",
      "Training Loss: 0.079.. \n",
      "Training Loss: 0.099.. \n",
      "Training Loss: 0.115.. \n",
      "Training Loss: 0.126.. \n",
      "Training Loss: 0.144.. \n",
      "Training Loss: 0.161.. \n",
      "Model Accuracy: 82.514.. \n",
      "Epoch: 209/300.. \n",
      "Training Loss: 0.009.. \n",
      "Training Loss: 0.030.. \n",
      "Training Loss: 0.049.. \n",
      "Training Loss: 0.063.. \n",
      "Training Loss: 0.079.. \n",
      "Training Loss: 0.099.. \n",
      "Training Loss: 0.115.. \n",
      "Training Loss: 0.126.. \n",
      "Training Loss: 0.144.. \n",
      "Training Loss: 0.161.. \n",
      "Model Accuracy: 82.539.. \n",
      "Epoch: 210/300.. \n",
      "Training Loss: 0.009.. \n",
      "Training Loss: 0.030.. \n",
      "Training Loss: 0.049.. \n",
      "Training Loss: 0.063.. \n",
      "Training Loss: 0.079.. \n",
      "Training Loss: 0.099.. \n",
      "Training Loss: 0.115.. \n",
      "Training Loss: 0.126.. \n",
      "Training Loss: 0.144.. \n",
      "Training Loss: 0.161.. \n",
      "Model Accuracy: 82.563.. \n",
      "Epoch: 211/300.. \n",
      "Training Loss: 0.009.. \n",
      "Training Loss: 0.030.. \n",
      "Training Loss: 0.049.. \n",
      "Training Loss: 0.063.. \n",
      "Training Loss: 0.079.. \n",
      "Training Loss: 0.099.. \n",
      "Training Loss: 0.115.. \n",
      "Training Loss: 0.126.. \n",
      "Training Loss: 0.144.. \n",
      "Training Loss: 0.161.. \n",
      "Model Accuracy: 82.587.. \n",
      "Epoch: 212/300.. \n",
      "Training Loss: 0.009.. \n",
      "Training Loss: 0.030.. \n",
      "Training Loss: 0.049.. \n",
      "Training Loss: 0.063.. \n",
      "Training Loss: 0.079.. \n",
      "Training Loss: 0.099.. \n",
      "Training Loss: 0.115.. \n",
      "Training Loss: 0.126.. \n",
      "Training Loss: 0.144.. \n",
      "Training Loss: 0.161.. \n",
      "Model Accuracy: 82.610.. \n",
      "Epoch: 213/300.. \n",
      "Training Loss: 0.009.. \n",
      "Training Loss: 0.030.. \n",
      "Training Loss: 0.049.. \n",
      "Training Loss: 0.063.. \n",
      "Training Loss: 0.079.. \n",
      "Training Loss: 0.099.. \n",
      "Training Loss: 0.115.. \n",
      "Training Loss: 0.126.. \n",
      "Training Loss: 0.144.. \n",
      "Training Loss: 0.160.. \n",
      "Model Accuracy: 82.633.. \n",
      "Epoch: 214/300.. \n",
      "Training Loss: 0.009.. \n",
      "Training Loss: 0.030.. \n",
      "Training Loss: 0.049.. \n",
      "Training Loss: 0.063.. \n",
      "Training Loss: 0.079.. \n",
      "Training Loss: 0.099.. \n",
      "Training Loss: 0.115.. \n",
      "Training Loss: 0.126.. \n",
      "Training Loss: 0.143.. \n",
      "Training Loss: 0.160.. \n",
      "Model Accuracy: 82.656.. \n",
      "Epoch: 215/300.. \n",
      "Training Loss: 0.009.. \n",
      "Training Loss: 0.030.. \n",
      "Training Loss: 0.049.. \n",
      "Training Loss: 0.063.. \n",
      "Training Loss: 0.079.. \n",
      "Training Loss: 0.099.. \n",
      "Training Loss: 0.115.. \n",
      "Training Loss: 0.125.. \n",
      "Training Loss: 0.143.. \n",
      "Training Loss: 0.160.. \n",
      "Model Accuracy: 82.678.. \n",
      "Epoch: 216/300.. \n",
      "Training Loss: 0.009.. \n",
      "Training Loss: 0.030.. \n",
      "Training Loss: 0.049.. \n",
      "Training Loss: 0.063.. \n",
      "Training Loss: 0.079.. \n",
      "Training Loss: 0.099.. \n",
      "Training Loss: 0.115.. \n",
      "Training Loss: 0.125.. \n",
      "Training Loss: 0.143.. \n",
      "Training Loss: 0.160.. \n",
      "Model Accuracy: 82.701.. \n",
      "Epoch: 217/300.. \n",
      "Training Loss: 0.009.. \n",
      "Training Loss: 0.030.. \n",
      "Training Loss: 0.049.. \n",
      "Training Loss: 0.063.. \n",
      "Training Loss: 0.079.. \n",
      "Training Loss: 0.099.. \n",
      "Training Loss: 0.115.. \n",
      "Training Loss: 0.125.. \n",
      "Training Loss: 0.143.. \n",
      "Training Loss: 0.160.. \n",
      "Model Accuracy: 82.722.. \n",
      "Epoch: 218/300.. \n",
      "Training Loss: 0.009.. \n",
      "Training Loss: 0.030.. \n",
      "Training Loss: 0.049.. \n",
      "Training Loss: 0.063.. \n",
      "Training Loss: 0.078.. \n",
      "Training Loss: 0.099.. \n",
      "Training Loss: 0.115.. \n",
      "Training Loss: 0.125.. \n",
      "Training Loss: 0.143.. \n",
      "Training Loss: 0.159.. \n",
      "Model Accuracy: 82.744.. \n",
      "Epoch: 219/300.. \n",
      "Training Loss: 0.009.. \n",
      "Training Loss: 0.030.. \n",
      "Training Loss: 0.048.. \n",
      "Training Loss: 0.063.. \n",
      "Training Loss: 0.078.. \n",
      "Training Loss: 0.099.. \n",
      "Training Loss: 0.115.. \n",
      "Training Loss: 0.125.. \n",
      "Training Loss: 0.143.. \n",
      "Training Loss: 0.159.. \n",
      "Model Accuracy: 82.765.. \n",
      "Epoch: 220/300.. \n",
      "Training Loss: 0.009.. \n",
      "Training Loss: 0.030.. \n",
      "Training Loss: 0.048.. \n",
      "Training Loss: 0.063.. \n",
      "Training Loss: 0.078.. \n",
      "Training Loss: 0.099.. \n",
      "Training Loss: 0.115.. \n",
      "Training Loss: 0.125.. \n",
      "Training Loss: 0.143.. \n",
      "Training Loss: 0.159.. \n",
      "Model Accuracy: 82.785.. \n",
      "Epoch: 221/300.. \n",
      "Training Loss: 0.009.. \n",
      "Training Loss: 0.030.. \n",
      "Training Loss: 0.048.. \n",
      "Training Loss: 0.063.. \n",
      "Training Loss: 0.078.. \n",
      "Training Loss: 0.099.. \n",
      "Training Loss: 0.114.. \n",
      "Training Loss: 0.125.. \n",
      "Training Loss: 0.143.. \n",
      "Training Loss: 0.159.. \n",
      "Model Accuracy: 82.806.. \n",
      "Epoch: 222/300.. \n",
      "Training Loss: 0.009.. \n",
      "Training Loss: 0.030.. \n",
      "Training Loss: 0.048.. \n",
      "Training Loss: 0.063.. \n",
      "Training Loss: 0.078.. \n",
      "Training Loss: 0.099.. \n",
      "Training Loss: 0.114.. \n",
      "Training Loss: 0.125.. \n",
      "Training Loss: 0.143.. \n",
      "Training Loss: 0.159.. \n",
      "Model Accuracy: 82.826.. \n",
      "Epoch: 223/300.. \n",
      "Training Loss: 0.009.. \n",
      "Training Loss: 0.030.. \n",
      "Training Loss: 0.048.. \n",
      "Training Loss: 0.063.. \n",
      "Training Loss: 0.078.. \n",
      "Training Loss: 0.099.. \n",
      "Training Loss: 0.114.. \n",
      "Training Loss: 0.125.. \n",
      "Training Loss: 0.142.. \n",
      "Training Loss: 0.159.. \n",
      "Model Accuracy: 82.845.. \n",
      "Epoch: 224/300.. \n",
      "Training Loss: 0.009.. \n",
      "Training Loss: 0.030.. \n",
      "Training Loss: 0.048.. \n",
      "Training Loss: 0.063.. \n",
      "Training Loss: 0.078.. \n",
      "Training Loss: 0.099.. \n",
      "Training Loss: 0.114.. \n",
      "Training Loss: 0.125.. \n",
      "Training Loss: 0.142.. \n",
      "Training Loss: 0.159.. \n",
      "Model Accuracy: 82.865.. \n",
      "Epoch: 225/300.. \n",
      "Training Loss: 0.009.. \n",
      "Training Loss: 0.030.. \n",
      "Training Loss: 0.048.. \n",
      "Training Loss: 0.063.. \n",
      "Training Loss: 0.078.. \n",
      "Training Loss: 0.098.. \n",
      "Training Loss: 0.114.. \n",
      "Training Loss: 0.125.. \n",
      "Training Loss: 0.142.. \n",
      "Training Loss: 0.158.. \n",
      "Model Accuracy: 82.884.. \n",
      "Epoch: 226/300.. \n",
      "Training Loss: 0.009.. \n",
      "Training Loss: 0.030.. \n",
      "Training Loss: 0.048.. \n",
      "Training Loss: 0.063.. \n",
      "Training Loss: 0.078.. \n",
      "Training Loss: 0.098.. \n",
      "Training Loss: 0.114.. \n",
      "Training Loss: 0.125.. \n",
      "Training Loss: 0.142.. \n",
      "Training Loss: 0.158.. \n",
      "Model Accuracy: 82.903.. \n",
      "Epoch: 227/300.. \n",
      "Training Loss: 0.009.. \n",
      "Training Loss: 0.030.. \n",
      "Training Loss: 0.048.. \n",
      "Training Loss: 0.063.. \n",
      "Training Loss: 0.078.. \n",
      "Training Loss: 0.098.. \n",
      "Training Loss: 0.114.. \n",
      "Training Loss: 0.125.. \n",
      "Training Loss: 0.142.. \n",
      "Training Loss: 0.158.. \n",
      "Model Accuracy: 82.921.. \n",
      "Epoch: 228/300.. \n",
      "Training Loss: 0.009.. \n",
      "Training Loss: 0.030.. \n",
      "Training Loss: 0.048.. \n",
      "Training Loss: 0.063.. \n",
      "Training Loss: 0.078.. \n",
      "Training Loss: 0.098.. \n",
      "Training Loss: 0.114.. \n",
      "Training Loss: 0.125.. \n",
      "Training Loss: 0.142.. \n",
      "Training Loss: 0.158.. \n",
      "Model Accuracy: 82.939.. \n",
      "Epoch: 229/300.. \n",
      "Training Loss: 0.009.. \n",
      "Training Loss: 0.030.. \n",
      "Training Loss: 0.048.. \n",
      "Training Loss: 0.063.. \n",
      "Training Loss: 0.078.. \n",
      "Training Loss: 0.098.. \n",
      "Training Loss: 0.114.. \n",
      "Training Loss: 0.125.. \n",
      "Training Loss: 0.142.. \n",
      "Training Loss: 0.158.. \n",
      "Model Accuracy: 82.957.. \n",
      "Epoch: 230/300.. \n",
      "Training Loss: 0.009.. \n",
      "Training Loss: 0.030.. \n",
      "Training Loss: 0.048.. \n",
      "Training Loss: 0.063.. \n",
      "Training Loss: 0.078.. \n",
      "Training Loss: 0.098.. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.114.. \n",
      "Training Loss: 0.125.. \n",
      "Training Loss: 0.142.. \n",
      "Training Loss: 0.158.. \n",
      "Model Accuracy: 82.975.. \n",
      "Epoch: 231/300.. \n",
      "Training Loss: 0.009.. \n",
      "Training Loss: 0.030.. \n",
      "Training Loss: 0.048.. \n",
      "Training Loss: 0.063.. \n",
      "Training Loss: 0.078.. \n",
      "Training Loss: 0.098.. \n",
      "Training Loss: 0.114.. \n",
      "Training Loss: 0.125.. \n",
      "Training Loss: 0.142.. \n",
      "Training Loss: 0.158.. \n",
      "Model Accuracy: 82.992.. \n",
      "Epoch: 232/300.. \n",
      "Training Loss: 0.009.. \n",
      "Training Loss: 0.030.. \n",
      "Training Loss: 0.048.. \n",
      "Training Loss: 0.063.. \n",
      "Training Loss: 0.078.. \n",
      "Training Loss: 0.098.. \n",
      "Training Loss: 0.114.. \n",
      "Training Loss: 0.124.. \n",
      "Training Loss: 0.142.. \n",
      "Training Loss: 0.157.. \n",
      "Model Accuracy: 83.009.. \n",
      "Epoch: 233/300.. \n",
      "Training Loss: 0.009.. \n",
      "Training Loss: 0.030.. \n",
      "Training Loss: 0.048.. \n",
      "Training Loss: 0.063.. \n",
      "Training Loss: 0.078.. \n",
      "Training Loss: 0.098.. \n",
      "Training Loss: 0.114.. \n",
      "Training Loss: 0.124.. \n",
      "Training Loss: 0.142.. \n",
      "Training Loss: 0.157.. \n",
      "Model Accuracy: 83.025.. \n",
      "Epoch: 234/300.. \n",
      "Training Loss: 0.009.. \n",
      "Training Loss: 0.029.. \n",
      "Training Loss: 0.048.. \n",
      "Training Loss: 0.063.. \n",
      "Training Loss: 0.078.. \n",
      "Training Loss: 0.098.. \n",
      "Training Loss: 0.114.. \n",
      "Training Loss: 0.124.. \n",
      "Training Loss: 0.142.. \n",
      "Training Loss: 0.157.. \n",
      "Model Accuracy: 83.041.. \n",
      "Epoch: 235/300.. \n",
      "Training Loss: 0.009.. \n",
      "Training Loss: 0.029.. \n",
      "Training Loss: 0.048.. \n",
      "Training Loss: 0.063.. \n",
      "Training Loss: 0.078.. \n",
      "Training Loss: 0.098.. \n",
      "Training Loss: 0.114.. \n",
      "Training Loss: 0.124.. \n",
      "Training Loss: 0.142.. \n",
      "Training Loss: 0.157.. \n",
      "Model Accuracy: 83.057.. \n",
      "Epoch: 236/300.. \n",
      "Training Loss: 0.009.. \n",
      "Training Loss: 0.029.. \n",
      "Training Loss: 0.048.. \n",
      "Training Loss: 0.063.. \n",
      "Training Loss: 0.078.. \n",
      "Training Loss: 0.098.. \n",
      "Training Loss: 0.114.. \n",
      "Training Loss: 0.124.. \n",
      "Training Loss: 0.141.. \n",
      "Training Loss: 0.157.. \n",
      "Model Accuracy: 83.073.. \n",
      "Epoch: 237/300.. \n",
      "Training Loss: 0.009.. \n",
      "Training Loss: 0.029.. \n",
      "Training Loss: 0.048.. \n",
      "Training Loss: 0.063.. \n",
      "Training Loss: 0.078.. \n",
      "Training Loss: 0.098.. \n",
      "Training Loss: 0.114.. \n",
      "Training Loss: 0.124.. \n",
      "Training Loss: 0.141.. \n",
      "Training Loss: 0.157.. \n",
      "Model Accuracy: 83.088.. \n",
      "Epoch: 238/300.. \n",
      "Training Loss: 0.009.. \n",
      "Training Loss: 0.029.. \n",
      "Training Loss: 0.048.. \n",
      "Training Loss: 0.063.. \n",
      "Training Loss: 0.078.. \n",
      "Training Loss: 0.098.. \n",
      "Training Loss: 0.114.. \n",
      "Training Loss: 0.124.. \n",
      "Training Loss: 0.141.. \n",
      "Training Loss: 0.157.. \n",
      "Model Accuracy: 83.104.. \n",
      "Epoch: 239/300.. \n",
      "Training Loss: 0.009.. \n",
      "Training Loss: 0.029.. \n",
      "Training Loss: 0.048.. \n",
      "Training Loss: 0.063.. \n",
      "Training Loss: 0.078.. \n",
      "Training Loss: 0.098.. \n",
      "Training Loss: 0.114.. \n",
      "Training Loss: 0.124.. \n",
      "Training Loss: 0.141.. \n",
      "Training Loss: 0.157.. \n",
      "Model Accuracy: 83.118.. \n",
      "Epoch: 240/300.. \n",
      "Training Loss: 0.009.. \n",
      "Training Loss: 0.029.. \n",
      "Training Loss: 0.048.. \n",
      "Training Loss: 0.063.. \n",
      "Training Loss: 0.078.. \n",
      "Training Loss: 0.098.. \n",
      "Training Loss: 0.114.. \n",
      "Training Loss: 0.124.. \n",
      "Training Loss: 0.141.. \n",
      "Training Loss: 0.157.. \n",
      "Model Accuracy: 83.133.. \n",
      "Epoch: 241/300.. \n",
      "Training Loss: 0.009.. \n",
      "Training Loss: 0.029.. \n",
      "Training Loss: 0.048.. \n",
      "Training Loss: 0.063.. \n",
      "Training Loss: 0.078.. \n",
      "Training Loss: 0.098.. \n",
      "Training Loss: 0.114.. \n",
      "Training Loss: 0.124.. \n",
      "Training Loss: 0.141.. \n",
      "Training Loss: 0.156.. \n",
      "Model Accuracy: 83.147.. \n",
      "Epoch: 242/300.. \n",
      "Training Loss: 0.009.. \n",
      "Training Loss: 0.029.. \n",
      "Training Loss: 0.048.. \n",
      "Training Loss: 0.063.. \n",
      "Training Loss: 0.078.. \n",
      "Training Loss: 0.098.. \n",
      "Training Loss: 0.114.. \n",
      "Training Loss: 0.124.. \n",
      "Training Loss: 0.141.. \n",
      "Training Loss: 0.156.. \n",
      "Model Accuracy: 83.161.. \n",
      "Epoch: 243/300.. \n",
      "Training Loss: 0.009.. \n",
      "Training Loss: 0.029.. \n",
      "Training Loss: 0.048.. \n",
      "Training Loss: 0.063.. \n",
      "Training Loss: 0.078.. \n",
      "Training Loss: 0.098.. \n",
      "Training Loss: 0.114.. \n",
      "Training Loss: 0.124.. \n",
      "Training Loss: 0.141.. \n",
      "Training Loss: 0.156.. \n",
      "Model Accuracy: 83.175.. \n",
      "Epoch: 244/300.. \n",
      "Training Loss: 0.009.. \n",
      "Training Loss: 0.029.. \n",
      "Training Loss: 0.048.. \n",
      "Training Loss: 0.063.. \n",
      "Training Loss: 0.078.. \n",
      "Training Loss: 0.098.. \n",
      "Training Loss: 0.114.. \n",
      "Training Loss: 0.124.. \n",
      "Training Loss: 0.141.. \n",
      "Training Loss: 0.156.. \n",
      "Model Accuracy: 83.189.. \n",
      "Epoch: 245/300.. \n",
      "Training Loss: 0.009.. \n",
      "Training Loss: 0.029.. \n",
      "Training Loss: 0.048.. \n",
      "Training Loss: 0.063.. \n",
      "Training Loss: 0.078.. \n",
      "Training Loss: 0.098.. \n",
      "Training Loss: 0.114.. \n",
      "Training Loss: 0.124.. \n",
      "Training Loss: 0.141.. \n",
      "Training Loss: 0.156.. \n",
      "Model Accuracy: 83.202.. \n",
      "Epoch: 246/300.. \n",
      "Training Loss: 0.009.. \n",
      "Training Loss: 0.029.. \n",
      "Training Loss: 0.048.. \n",
      "Training Loss: 0.063.. \n",
      "Training Loss: 0.078.. \n",
      "Training Loss: 0.098.. \n",
      "Training Loss: 0.114.. \n",
      "Training Loss: 0.124.. \n",
      "Training Loss: 0.141.. \n",
      "Training Loss: 0.156.. \n",
      "Model Accuracy: 83.215.. \n",
      "Epoch: 247/300.. \n",
      "Training Loss: 0.009.. \n",
      "Training Loss: 0.029.. \n",
      "Training Loss: 0.048.. \n",
      "Training Loss: 0.063.. \n",
      "Training Loss: 0.078.. \n",
      "Training Loss: 0.098.. \n",
      "Training Loss: 0.114.. \n",
      "Training Loss: 0.124.. \n",
      "Training Loss: 0.141.. \n",
      "Training Loss: 0.156.. \n",
      "Model Accuracy: 83.228.. \n",
      "Epoch: 248/300.. \n",
      "Training Loss: 0.009.. \n",
      "Training Loss: 0.029.. \n",
      "Training Loss: 0.048.. \n",
      "Training Loss: 0.063.. \n",
      "Training Loss: 0.078.. \n",
      "Training Loss: 0.098.. \n",
      "Training Loss: 0.114.. \n",
      "Training Loss: 0.124.. \n",
      "Training Loss: 0.141.. \n",
      "Training Loss: 0.156.. \n",
      "Model Accuracy: 83.240.. \n",
      "Epoch: 249/300.. \n",
      "Training Loss: 0.009.. \n",
      "Training Loss: 0.029.. \n",
      "Training Loss: 0.048.. \n",
      "Training Loss: 0.063.. \n",
      "Training Loss: 0.078.. \n",
      "Training Loss: 0.098.. \n",
      "Training Loss: 0.114.. \n",
      "Training Loss: 0.124.. \n",
      "Training Loss: 0.141.. \n",
      "Training Loss: 0.156.. \n",
      "Model Accuracy: 83.253.. \n",
      "Epoch: 250/300.. \n",
      "Training Loss: 0.009.. \n",
      "Training Loss: 0.029.. \n",
      "Training Loss: 0.048.. \n",
      "Training Loss: 0.063.. \n",
      "Training Loss: 0.078.. \n",
      "Training Loss: 0.098.. \n",
      "Training Loss: 0.114.. \n",
      "Training Loss: 0.124.. \n",
      "Training Loss: 0.141.. \n",
      "Training Loss: 0.156.. \n",
      "Model Accuracy: 83.265.. \n",
      "Epoch: 251/300.. \n",
      "Training Loss: 0.009.. \n",
      "Training Loss: 0.029.. \n",
      "Training Loss: 0.048.. \n",
      "Training Loss: 0.063.. \n",
      "Training Loss: 0.078.. \n",
      "Training Loss: 0.098.. \n",
      "Training Loss: 0.114.. \n",
      "Training Loss: 0.124.. \n",
      "Training Loss: 0.141.. \n",
      "Training Loss: 0.156.. \n",
      "Model Accuracy: 83.277.. \n",
      "Epoch: 252/300.. \n",
      "Training Loss: 0.009.. \n",
      "Training Loss: 0.029.. \n",
      "Training Loss: 0.048.. \n",
      "Training Loss: 0.063.. \n",
      "Training Loss: 0.078.. \n",
      "Training Loss: 0.098.. \n",
      "Training Loss: 0.114.. \n",
      "Training Loss: 0.124.. \n",
      "Training Loss: 0.141.. \n",
      "Training Loss: 0.156.. \n",
      "Model Accuracy: 83.288.. \n",
      "Epoch: 253/300.. \n",
      "Training Loss: 0.009.. \n",
      "Training Loss: 0.029.. \n",
      "Training Loss: 0.048.. \n",
      "Training Loss: 0.063.. \n",
      "Training Loss: 0.078.. \n",
      "Training Loss: 0.098.. \n",
      "Training Loss: 0.114.. \n",
      "Training Loss: 0.124.. \n",
      "Training Loss: 0.141.. \n",
      "Training Loss: 0.155.. \n",
      "Model Accuracy: 83.300.. \n",
      "Epoch: 254/300.. \n",
      "Training Loss: 0.009.. \n",
      "Training Loss: 0.029.. \n",
      "Training Loss: 0.048.. \n",
      "Training Loss: 0.063.. \n",
      "Training Loss: 0.078.. \n",
      "Training Loss: 0.098.. \n",
      "Training Loss: 0.114.. \n",
      "Training Loss: 0.124.. \n",
      "Training Loss: 0.141.. \n",
      "Training Loss: 0.155.. \n",
      "Model Accuracy: 83.311.. \n",
      "Epoch: 255/300.. \n",
      "Training Loss: 0.009.. \n",
      "Training Loss: 0.029.. \n",
      "Training Loss: 0.048.. \n",
      "Training Loss: 0.063.. \n",
      "Training Loss: 0.078.. \n",
      "Training Loss: 0.098.. \n",
      "Training Loss: 0.114.. \n",
      "Training Loss: 0.124.. \n",
      "Training Loss: 0.141.. \n",
      "Training Loss: 0.155.. \n",
      "Model Accuracy: 83.322.. \n",
      "Epoch: 256/300.. \n",
      "Training Loss: 0.009.. \n",
      "Training Loss: 0.029.. \n",
      "Training Loss: 0.048.. \n",
      "Training Loss: 0.063.. \n",
      "Training Loss: 0.078.. \n",
      "Training Loss: 0.098.. \n",
      "Training Loss: 0.114.. \n",
      "Training Loss: 0.124.. \n",
      "Training Loss: 0.141.. \n",
      "Training Loss: 0.155.. \n",
      "Model Accuracy: 83.333.. \n",
      "Epoch: 257/300.. \n",
      "Training Loss: 0.009.. \n",
      "Training Loss: 0.029.. \n",
      "Training Loss: 0.048.. \n",
      "Training Loss: 0.063.. \n",
      "Training Loss: 0.078.. \n",
      "Training Loss: 0.098.. \n",
      "Training Loss: 0.114.. \n",
      "Training Loss: 0.124.. \n",
      "Training Loss: 0.141.. \n",
      "Training Loss: 0.155.. \n",
      "Model Accuracy: 83.343.. \n",
      "Epoch: 258/300.. \n",
      "Training Loss: 0.009.. \n",
      "Training Loss: 0.029.. \n",
      "Training Loss: 0.048.. \n",
      "Training Loss: 0.063.. \n",
      "Training Loss: 0.078.. \n",
      "Training Loss: 0.098.. \n",
      "Training Loss: 0.113.. \n",
      "Training Loss: 0.124.. \n",
      "Training Loss: 0.141.. \n",
      "Training Loss: 0.155.. \n",
      "Model Accuracy: 83.354.. \n",
      "Epoch: 259/300.. \n",
      "Training Loss: 0.009.. \n",
      "Training Loss: 0.029.. \n",
      "Training Loss: 0.048.. \n",
      "Training Loss: 0.063.. \n",
      "Training Loss: 0.078.. \n",
      "Training Loss: 0.098.. \n",
      "Training Loss: 0.113.. \n",
      "Training Loss: 0.124.. \n",
      "Training Loss: 0.141.. \n",
      "Training Loss: 0.155.. \n",
      "Model Accuracy: 83.364.. \n",
      "Epoch: 260/300.. \n",
      "Training Loss: 0.009.. \n",
      "Training Loss: 0.029.. \n",
      "Training Loss: 0.048.. \n",
      "Training Loss: 0.063.. \n",
      "Training Loss: 0.078.. \n",
      "Training Loss: 0.098.. \n",
      "Training Loss: 0.113.. \n",
      "Training Loss: 0.124.. \n",
      "Training Loss: 0.140.. \n",
      "Training Loss: 0.155.. \n",
      "Model Accuracy: 83.374.. \n",
      "Epoch: 261/300.. \n",
      "Training Loss: 0.009.. \n",
      "Training Loss: 0.029.. \n",
      "Training Loss: 0.048.. \n",
      "Training Loss: 0.063.. \n",
      "Training Loss: 0.078.. \n",
      "Training Loss: 0.098.. \n",
      "Training Loss: 0.113.. \n",
      "Training Loss: 0.124.. \n",
      "Training Loss: 0.140.. \n",
      "Training Loss: 0.155.. \n",
      "Model Accuracy: 83.384.. \n",
      "Epoch: 262/300.. \n",
      "Training Loss: 0.009.. \n",
      "Training Loss: 0.029.. \n",
      "Training Loss: 0.048.. \n",
      "Training Loss: 0.063.. \n",
      "Training Loss: 0.078.. \n",
      "Training Loss: 0.098.. \n",
      "Training Loss: 0.113.. \n",
      "Training Loss: 0.124.. \n",
      "Training Loss: 0.140.. \n",
      "Training Loss: 0.155.. \n",
      "Model Accuracy: 83.393.. \n",
      "Epoch: 263/300.. \n",
      "Training Loss: 0.009.. \n",
      "Training Loss: 0.029.. \n",
      "Training Loss: 0.048.. \n",
      "Training Loss: 0.063.. \n",
      "Training Loss: 0.078.. \n",
      "Training Loss: 0.098.. \n",
      "Training Loss: 0.113.. \n",
      "Training Loss: 0.124.. \n",
      "Training Loss: 0.140.. \n",
      "Training Loss: 0.155.. \n",
      "Model Accuracy: 83.403.. \n",
      "Epoch: 264/300.. \n",
      "Training Loss: 0.009.. \n",
      "Training Loss: 0.029.. \n",
      "Training Loss: 0.048.. \n",
      "Training Loss: 0.063.. \n",
      "Training Loss: 0.078.. \n",
      "Training Loss: 0.098.. \n",
      "Training Loss: 0.113.. \n",
      "Training Loss: 0.124.. \n",
      "Training Loss: 0.140.. \n",
      "Training Loss: 0.155.. \n",
      "Model Accuracy: 83.412.. \n",
      "Epoch: 265/300.. \n",
      "Training Loss: 0.009.. \n",
      "Training Loss: 0.029.. \n",
      "Training Loss: 0.048.. \n",
      "Training Loss: 0.063.. \n",
      "Training Loss: 0.078.. \n",
      "Training Loss: 0.097.. \n",
      "Training Loss: 0.113.. \n",
      "Training Loss: 0.124.. \n",
      "Training Loss: 0.140.. \n",
      "Training Loss: 0.155.. \n",
      "Model Accuracy: 83.421.. \n",
      "Epoch: 266/300.. \n",
      "Training Loss: 0.009.. \n",
      "Training Loss: 0.029.. \n",
      "Training Loss: 0.048.. \n",
      "Training Loss: 0.063.. \n",
      "Training Loss: 0.078.. \n",
      "Training Loss: 0.097.. \n",
      "Training Loss: 0.113.. \n",
      "Training Loss: 0.124.. \n",
      "Training Loss: 0.140.. \n",
      "Training Loss: 0.155.. \n",
      "Model Accuracy: 83.430.. \n",
      "Epoch: 267/300.. \n",
      "Training Loss: 0.009.. \n",
      "Training Loss: 0.029.. \n",
      "Training Loss: 0.048.. \n",
      "Training Loss: 0.063.. \n",
      "Training Loss: 0.078.. \n",
      "Training Loss: 0.097.. \n",
      "Training Loss: 0.113.. \n",
      "Training Loss: 0.124.. \n",
      "Training Loss: 0.140.. \n",
      "Training Loss: 0.155.. \n",
      "Model Accuracy: 83.438.. \n",
      "Epoch: 268/300.. \n",
      "Training Loss: 0.009.. \n",
      "Training Loss: 0.029.. \n",
      "Training Loss: 0.048.. \n",
      "Training Loss: 0.064.. \n",
      "Training Loss: 0.078.. \n",
      "Training Loss: 0.097.. \n",
      "Training Loss: 0.113.. \n",
      "Training Loss: 0.124.. \n",
      "Training Loss: 0.140.. \n",
      "Training Loss: 0.155.. \n",
      "Model Accuracy: 83.447.. \n",
      "Epoch: 269/300.. \n",
      "Training Loss: 0.009.. \n",
      "Training Loss: 0.029.. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.048.. \n",
      "Training Loss: 0.064.. \n",
      "Training Loss: 0.078.. \n",
      "Training Loss: 0.097.. \n",
      "Training Loss: 0.113.. \n",
      "Training Loss: 0.124.. \n",
      "Training Loss: 0.140.. \n",
      "Training Loss: 0.155.. \n",
      "Model Accuracy: 83.455.. \n",
      "Epoch: 270/300.. \n",
      "Training Loss: 0.009.. \n",
      "Training Loss: 0.029.. \n",
      "Training Loss: 0.047.. \n",
      "Training Loss: 0.064.. \n",
      "Training Loss: 0.078.. \n",
      "Training Loss: 0.097.. \n",
      "Training Loss: 0.113.. \n",
      "Training Loss: 0.124.. \n",
      "Training Loss: 0.140.. \n",
      "Training Loss: 0.155.. \n",
      "Model Accuracy: 83.463.. \n",
      "Epoch: 271/300.. \n",
      "Training Loss: 0.009.. \n",
      "Training Loss: 0.029.. \n",
      "Training Loss: 0.047.. \n",
      "Training Loss: 0.064.. \n",
      "Training Loss: 0.078.. \n",
      "Training Loss: 0.097.. \n",
      "Training Loss: 0.113.. \n",
      "Training Loss: 0.124.. \n",
      "Training Loss: 0.140.. \n",
      "Training Loss: 0.154.. \n",
      "Model Accuracy: 83.471.. \n",
      "Epoch: 272/300.. \n",
      "Training Loss: 0.009.. \n",
      "Training Loss: 0.029.. \n",
      "Training Loss: 0.047.. \n",
      "Training Loss: 0.064.. \n",
      "Training Loss: 0.078.. \n",
      "Training Loss: 0.097.. \n",
      "Training Loss: 0.113.. \n",
      "Training Loss: 0.124.. \n",
      "Training Loss: 0.140.. \n",
      "Training Loss: 0.154.. \n",
      "Model Accuracy: 83.479.. \n",
      "Epoch: 273/300.. \n",
      "Training Loss: 0.009.. \n",
      "Training Loss: 0.029.. \n",
      "Training Loss: 0.047.. \n",
      "Training Loss: 0.064.. \n",
      "Training Loss: 0.078.. \n",
      "Training Loss: 0.097.. \n",
      "Training Loss: 0.113.. \n",
      "Training Loss: 0.124.. \n",
      "Training Loss: 0.140.. \n",
      "Training Loss: 0.154.. \n",
      "Model Accuracy: 83.487.. \n",
      "Epoch: 274/300.. \n",
      "Training Loss: 0.009.. \n",
      "Training Loss: 0.029.. \n",
      "Training Loss: 0.047.. \n",
      "Training Loss: 0.064.. \n",
      "Training Loss: 0.078.. \n",
      "Training Loss: 0.097.. \n",
      "Training Loss: 0.113.. \n",
      "Training Loss: 0.124.. \n",
      "Training Loss: 0.140.. \n",
      "Training Loss: 0.154.. \n",
      "Model Accuracy: 83.494.. \n",
      "Epoch: 275/300.. \n",
      "Training Loss: 0.009.. \n",
      "Training Loss: 0.029.. \n",
      "Training Loss: 0.047.. \n",
      "Training Loss: 0.064.. \n",
      "Training Loss: 0.078.. \n",
      "Training Loss: 0.097.. \n",
      "Training Loss: 0.113.. \n",
      "Training Loss: 0.124.. \n",
      "Training Loss: 0.140.. \n",
      "Training Loss: 0.154.. \n",
      "Model Accuracy: 83.502.. \n",
      "Epoch: 276/300.. \n",
      "Training Loss: 0.009.. \n",
      "Training Loss: 0.029.. \n",
      "Training Loss: 0.047.. \n",
      "Training Loss: 0.064.. \n",
      "Training Loss: 0.078.. \n",
      "Training Loss: 0.097.. \n",
      "Training Loss: 0.113.. \n",
      "Training Loss: 0.124.. \n",
      "Training Loss: 0.140.. \n",
      "Training Loss: 0.154.. \n",
      "Model Accuracy: 83.509.. \n",
      "Epoch: 277/300.. \n",
      "Training Loss: 0.009.. \n",
      "Training Loss: 0.029.. \n",
      "Training Loss: 0.047.. \n",
      "Training Loss: 0.064.. \n",
      "Training Loss: 0.078.. \n",
      "Training Loss: 0.097.. \n",
      "Training Loss: 0.113.. \n",
      "Training Loss: 0.124.. \n",
      "Training Loss: 0.140.. \n",
      "Training Loss: 0.154.. \n",
      "Model Accuracy: 83.516.. \n",
      "Epoch: 278/300.. \n",
      "Training Loss: 0.009.. \n",
      "Training Loss: 0.029.. \n",
      "Training Loss: 0.047.. \n",
      "Training Loss: 0.064.. \n",
      "Training Loss: 0.078.. \n",
      "Training Loss: 0.097.. \n",
      "Training Loss: 0.113.. \n",
      "Training Loss: 0.124.. \n",
      "Training Loss: 0.140.. \n",
      "Training Loss: 0.154.. \n",
      "Model Accuracy: 83.523.. \n",
      "Epoch: 279/300.. \n",
      "Training Loss: 0.009.. \n",
      "Training Loss: 0.029.. \n",
      "Training Loss: 0.047.. \n",
      "Training Loss: 0.064.. \n",
      "Training Loss: 0.078.. \n",
      "Training Loss: 0.097.. \n",
      "Training Loss: 0.113.. \n",
      "Training Loss: 0.124.. \n",
      "Training Loss: 0.140.. \n",
      "Training Loss: 0.154.. \n",
      "Model Accuracy: 83.530.. \n",
      "Epoch: 280/300.. \n",
      "Training Loss: 0.009.. \n",
      "Training Loss: 0.029.. \n",
      "Training Loss: 0.047.. \n",
      "Training Loss: 0.064.. \n",
      "Training Loss: 0.078.. \n",
      "Training Loss: 0.097.. \n",
      "Training Loss: 0.113.. \n",
      "Training Loss: 0.124.. \n",
      "Training Loss: 0.140.. \n",
      "Training Loss: 0.154.. \n",
      "Model Accuracy: 83.536.. \n",
      "Epoch: 281/300.. \n",
      "Training Loss: 0.009.. \n",
      "Training Loss: 0.029.. \n",
      "Training Loss: 0.047.. \n",
      "Training Loss: 0.064.. \n",
      "Training Loss: 0.078.. \n",
      "Training Loss: 0.097.. \n",
      "Training Loss: 0.113.. \n",
      "Training Loss: 0.124.. \n",
      "Training Loss: 0.140.. \n",
      "Training Loss: 0.154.. \n",
      "Model Accuracy: 83.543.. \n",
      "Epoch: 282/300.. \n",
      "Training Loss: 0.009.. \n",
      "Training Loss: 0.029.. \n",
      "Training Loss: 0.047.. \n",
      "Training Loss: 0.064.. \n",
      "Training Loss: 0.078.. \n",
      "Training Loss: 0.097.. \n",
      "Training Loss: 0.113.. \n",
      "Training Loss: 0.124.. \n",
      "Training Loss: 0.140.. \n",
      "Training Loss: 0.154.. \n",
      "Model Accuracy: 83.549.. \n",
      "Epoch: 283/300.. \n",
      "Training Loss: 0.009.. \n",
      "Training Loss: 0.029.. \n",
      "Training Loss: 0.047.. \n",
      "Training Loss: 0.064.. \n",
      "Training Loss: 0.078.. \n",
      "Training Loss: 0.097.. \n",
      "Training Loss: 0.113.. \n",
      "Training Loss: 0.124.. \n",
      "Training Loss: 0.140.. \n",
      "Training Loss: 0.154.. \n",
      "Model Accuracy: 83.555.. \n",
      "Epoch: 284/300.. \n",
      "Training Loss: 0.009.. \n",
      "Training Loss: 0.029.. \n",
      "Training Loss: 0.047.. \n",
      "Training Loss: 0.064.. \n",
      "Training Loss: 0.078.. \n",
      "Training Loss: 0.097.. \n",
      "Training Loss: 0.113.. \n",
      "Training Loss: 0.124.. \n",
      "Training Loss: 0.140.. \n",
      "Training Loss: 0.154.. \n",
      "Model Accuracy: 83.561.. \n",
      "Epoch: 285/300.. \n",
      "Training Loss: 0.009.. \n",
      "Training Loss: 0.029.. \n",
      "Training Loss: 0.047.. \n",
      "Training Loss: 0.064.. \n",
      "Training Loss: 0.078.. \n",
      "Training Loss: 0.097.. \n",
      "Training Loss: 0.113.. \n",
      "Training Loss: 0.124.. \n",
      "Training Loss: 0.140.. \n",
      "Training Loss: 0.154.. \n",
      "Model Accuracy: 83.567.. \n",
      "Epoch: 286/300.. \n",
      "Training Loss: 0.009.. \n",
      "Training Loss: 0.029.. \n",
      "Training Loss: 0.047.. \n",
      "Training Loss: 0.064.. \n",
      "Training Loss: 0.078.. \n",
      "Training Loss: 0.097.. \n",
      "Training Loss: 0.113.. \n",
      "Training Loss: 0.124.. \n",
      "Training Loss: 0.140.. \n",
      "Training Loss: 0.154.. \n",
      "Model Accuracy: 83.573.. \n",
      "Epoch: 287/300.. \n",
      "Training Loss: 0.009.. \n",
      "Training Loss: 0.029.. \n",
      "Training Loss: 0.047.. \n",
      "Training Loss: 0.064.. \n",
      "Training Loss: 0.078.. \n",
      "Training Loss: 0.097.. \n",
      "Training Loss: 0.113.. \n",
      "Training Loss: 0.124.. \n",
      "Training Loss: 0.140.. \n",
      "Training Loss: 0.154.. \n",
      "Model Accuracy: 83.579.. \n",
      "Epoch: 288/300.. \n",
      "Training Loss: 0.009.. \n",
      "Training Loss: 0.029.. \n",
      "Training Loss: 0.047.. \n",
      "Training Loss: 0.064.. \n",
      "Training Loss: 0.078.. \n",
      "Training Loss: 0.097.. \n",
      "Training Loss: 0.113.. \n",
      "Training Loss: 0.124.. \n",
      "Training Loss: 0.140.. \n",
      "Training Loss: 0.154.. \n",
      "Model Accuracy: 83.584.. \n",
      "Epoch: 289/300.. \n",
      "Training Loss: 0.009.. \n",
      "Training Loss: 0.029.. \n",
      "Training Loss: 0.047.. \n",
      "Training Loss: 0.064.. \n",
      "Training Loss: 0.078.. \n",
      "Training Loss: 0.097.. \n",
      "Training Loss: 0.113.. \n",
      "Training Loss: 0.124.. \n",
      "Training Loss: 0.140.. \n",
      "Training Loss: 0.154.. \n",
      "Model Accuracy: 83.590.. \n",
      "Epoch: 290/300.. \n",
      "Training Loss: 0.009.. \n",
      "Training Loss: 0.029.. \n",
      "Training Loss: 0.047.. \n",
      "Training Loss: 0.064.. \n",
      "Training Loss: 0.078.. \n",
      "Training Loss: 0.097.. \n",
      "Training Loss: 0.113.. \n",
      "Training Loss: 0.124.. \n",
      "Training Loss: 0.140.. \n",
      "Training Loss: 0.154.. \n",
      "Model Accuracy: 83.595.. \n",
      "Epoch: 291/300.. \n",
      "Training Loss: 0.009.. \n",
      "Training Loss: 0.029.. \n",
      "Training Loss: 0.047.. \n",
      "Training Loss: 0.064.. \n",
      "Training Loss: 0.078.. \n",
      "Training Loss: 0.097.. \n",
      "Training Loss: 0.113.. \n",
      "Training Loss: 0.124.. \n",
      "Training Loss: 0.140.. \n",
      "Training Loss: 0.154.. \n",
      "Model Accuracy: 83.600.. \n",
      "Epoch: 292/300.. \n",
      "Training Loss: 0.009.. \n",
      "Training Loss: 0.029.. \n",
      "Training Loss: 0.047.. \n",
      "Training Loss: 0.064.. \n",
      "Training Loss: 0.078.. \n",
      "Training Loss: 0.097.. \n",
      "Training Loss: 0.113.. \n",
      "Training Loss: 0.124.. \n",
      "Training Loss: 0.140.. \n",
      "Training Loss: 0.154.. \n",
      "Model Accuracy: 83.605.. \n",
      "Epoch: 293/300.. \n",
      "Training Loss: 0.009.. \n",
      "Training Loss: 0.029.. \n",
      "Training Loss: 0.047.. \n",
      "Training Loss: 0.064.. \n",
      "Training Loss: 0.078.. \n",
      "Training Loss: 0.097.. \n",
      "Training Loss: 0.113.. \n",
      "Training Loss: 0.124.. \n",
      "Training Loss: 0.140.. \n",
      "Training Loss: 0.154.. \n",
      "Model Accuracy: 83.610.. \n",
      "Epoch: 294/300.. \n",
      "Training Loss: 0.009.. \n",
      "Training Loss: 0.029.. \n",
      "Training Loss: 0.047.. \n",
      "Training Loss: 0.064.. \n",
      "Training Loss: 0.078.. \n",
      "Training Loss: 0.097.. \n",
      "Training Loss: 0.113.. \n",
      "Training Loss: 0.124.. \n",
      "Training Loss: 0.140.. \n",
      "Training Loss: 0.154.. \n",
      "Model Accuracy: 83.615.. \n",
      "Epoch: 295/300.. \n",
      "Training Loss: 0.009.. \n",
      "Training Loss: 0.029.. \n",
      "Training Loss: 0.047.. \n",
      "Training Loss: 0.064.. \n",
      "Training Loss: 0.078.. \n",
      "Training Loss: 0.097.. \n",
      "Training Loss: 0.113.. \n",
      "Training Loss: 0.124.. \n",
      "Training Loss: 0.140.. \n",
      "Training Loss: 0.154.. \n",
      "Model Accuracy: 83.620.. \n",
      "Epoch: 296/300.. \n",
      "Training Loss: 0.009.. \n",
      "Training Loss: 0.029.. \n",
      "Training Loss: 0.047.. \n",
      "Training Loss: 0.064.. \n",
      "Training Loss: 0.078.. \n",
      "Training Loss: 0.097.. \n",
      "Training Loss: 0.113.. \n",
      "Training Loss: 0.124.. \n",
      "Training Loss: 0.140.. \n",
      "Training Loss: 0.154.. \n",
      "Model Accuracy: 83.625.. \n",
      "Epoch: 297/300.. \n",
      "Training Loss: 0.009.. \n",
      "Training Loss: 0.029.. \n",
      "Training Loss: 0.047.. \n",
      "Training Loss: 0.064.. \n",
      "Training Loss: 0.078.. \n",
      "Training Loss: 0.097.. \n",
      "Training Loss: 0.113.. \n",
      "Training Loss: 0.124.. \n",
      "Training Loss: 0.140.. \n",
      "Training Loss: 0.154.. \n",
      "Model Accuracy: 83.629.. \n",
      "Epoch: 298/300.. \n",
      "Training Loss: 0.009.. \n",
      "Training Loss: 0.029.. \n",
      "Training Loss: 0.047.. \n",
      "Training Loss: 0.064.. \n",
      "Training Loss: 0.078.. \n",
      "Training Loss: 0.097.. \n",
      "Training Loss: 0.113.. \n",
      "Training Loss: 0.124.. \n",
      "Training Loss: 0.140.. \n",
      "Training Loss: 0.154.. \n",
      "Model Accuracy: 83.634.. \n",
      "Epoch: 299/300.. \n",
      "Training Loss: 0.009.. \n",
      "Training Loss: 0.029.. \n",
      "Training Loss: 0.047.. \n",
      "Training Loss: 0.064.. \n",
      "Training Loss: 0.078.. \n",
      "Training Loss: 0.097.. \n",
      "Training Loss: 0.113.. \n",
      "Training Loss: 0.124.. \n",
      "Training Loss: 0.140.. \n",
      "Training Loss: 0.154.. \n",
      "Model Accuracy: 83.638.. \n",
      "Epoch: 300/300.. \n"
     ]
    }
   ],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# Let's train the model\n",
    "    \n",
    "train_batch = np.array_split(X_train, 10)\n",
    "label_batch = np.array_split(y_train, 10)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    " \n",
    "for i in range(len(train_batch)):\n",
    "    train_batch[i] = torch.from_numpy(train_batch[i]).float()\n",
    "for i in range(len(label_batch)):\n",
    "    label_batch[i] = torch.from_numpy(label_batch[i]).float().view(-1, 1)\n",
    " \n",
    "X_test = torch.from_numpy(X_test).float()\n",
    "y_test = torch.from_numpy(y_test).float().view(-1, 1)\n",
    " \n",
    "model = Regressor()\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=.01)\n",
    "epochs = 300\n",
    " \n",
    "train_losses, test_losses = [], []\n",
    "\n",
    "\n",
    "def train(model, device, optimizer, epoch):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for i in range(len(train_batch)):\n",
    "        optimizer.zero_grad()\n",
    "        output = model(train_batch[i])\n",
    "        loss = torch.sqrt(criterion(torch.log(output), torch.log(label_batch[i])))\n",
    "        loss.backward()\n",
    "        optimizer.step()        \n",
    "        train_loss += loss.item()\n",
    "        train_losses.append(train_loss/len(train_batch))\n",
    "        print(\"Training Loss: {:.3f}.. \".format(train_loss/len(train_batch)))\n",
    "          \n",
    "\n",
    "\n",
    "def test(model, device):\n",
    "    test_loss = 0\n",
    "    accuracy = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        predictions = model(X_test)\n",
    "        test_loss += torch.sqrt(criterion(torch.log(predictions), torch.log(y_test)))       \n",
    "        \n",
    "        test_losses.append(test_loss)\n",
    "    print(\"Model Accuracy: {:.3f}.. \".format((1-test_loss)*100))\n",
    "        \n",
    "\n",
    "for epoch in range(epochs):\n",
    "    train(model, device, optimizer, epoch)\n",
    "    test(model, device)\n",
    "    print(\"Epoch: {}/{}.. \".format(epoch+1, epochs))\n",
    "        \n",
    "\n",
    "\n",
    "        \n",
    " \n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f6dcbf7f550>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the loss on graph       \n",
    "plt.plot(train_losses, label='Training loss')\n",
    "plt.plot(test_losses, label='Validation loss')\n",
    "plt.legend(frameon=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best time to start for office on Monday is 10.30 AM, it takes  28.391273498535156  minutes\n"
     ]
    }
   ],
   "source": [
    "#------------------------------------------------------------------------------    \n",
    "# let's do the prediction\n",
    " \n",
    "# input for if time is 8.30 AM on Monday\n",
    "inpt1 = np.array([[1,0,0,0,0,1,0,0,0,0,0,0,1,0]])\n",
    "# input for if time is 9.00 AM on Monday\n",
    "inpt2 = np.array([[1,0,0,0,0,0,1,0,0,0,0,0,1,0]])\n",
    "# input for if time is 9.30 AM on Monday\n",
    "inpt3 = np.array([[1,0,0,0,0,0,0,1,0,0,0,0,1,0]])\n",
    "# input for if time is 10.00 AM on Monday\n",
    "inpt4 = np.array([[1,0,0,0,0,0,0,0,1,0,0,0,1,0]])\n",
    "# input for if time is 10.30 AM on Monday\n",
    "inpt5 = np.array([[1,0,0,0,0,0,0,0,0,1,0,0,1,0]])\n",
    " \n",
    "#Convert numpy array to tensor because PyTorch works on Tensors\n",
    "tensor1 = torch.Tensor(inpt1)\n",
    "y_pred1 = model(tensor1)\n",
    " \n",
    "tensor2 = torch.Tensor(inpt2)\n",
    "y_pred2 = model(tensor2)\n",
    " \n",
    "tensor3 = torch.Tensor(inpt3)\n",
    "y_pred3 = model(tensor3)\n",
    " \n",
    "tensor4 = torch.Tensor(inpt4)\n",
    "y_pred4 = model(tensor4)\n",
    " \n",
    "tensor5 = torch.Tensor(inpt5)\n",
    "y_pred5 = model(tensor5)\n",
    " \n",
    "best_time = min(y_pred1,y_pred2,y_pred3,y_pred4, y_pred5)\n",
    " \n",
    "if y_pred1 is best_time:\n",
    "    print(\"Best time to start for office on Monday is 8.30 AM, it takes \", best_time[0].item() , ' minutes')\n",
    " \n",
    "elif y_pred2 is best_time:\n",
    "    print('Best time to start for office on Monday is 9.00 AM, it takes ', best_time[0].item() , ' minutes')\n",
    "    \n",
    "elif y_pred3 is best_time:\n",
    "    print('Best time to start for office on Monday is 9.30 AM, it takes ', best_time[0].item() , ' minutes')\n",
    " \n",
    "elif y_pred4 is best_time:\n",
    "    print('Best time to start for office on Monday is 10.00 AM, it takes ', best_time[0].item() , ' minutes')\n",
    "    \n",
    "elif y_pred5 is best_time:\n",
    "    print('Best time to start for office on Monday is 10.30 AM, it takes ' , best_time[0].item() , ' minutes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
