{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "SaFASiLe: Secure and Federated learning with American Sign Language\n",
    "coded by Richárd Ádám Vécsey Dr. (Richard)\n",
    "Udacity Secure and Private AI Final Keystone Challange\n",
    "\n",
    "\n",
    "SUMMARY:\n",
    "This program learns gestures from american sign language (ASL). The dataset comes from Kaggle. With this program you are\n",
    "able to train a neural network to identify hand gestures from ASL. Use it as a module your code or a standalone program.\n",
    "My goal was bulding a small and fast neural network. You are very welcome to rebuild it. See variables part to refine the\n",
    "code.\n",
    "\n",
    "This program contains 3 big parts:\n",
    "    - neural network\n",
    "    - federated learning\n",
    "    - secure learning\n",
    "\n",
    "The core concepts meet with the requirements from SPAIC Keystone Challange: ND185, Lesson 9, Chapter 8\n",
    "\n",
    "\n",
    "REQUIREMENTS:\n",
    "PySyft 0.1.16a1\n",
    "PyTorch 1.0.1\n",
    "TorchVision 0.2.2\n",
    "\n",
    "\n",
    "INSTRUCTIONS:\n",
    "You have to save the trained model if you want to use it next time.\n",
    "For saving you have to use the 'torch.save(model.state_dict(), PATH)' process.\n",
    "For loading you have to use the 'torch.save(model.state_dict(), PATH)' process.\n",
    "PATH is the path of your model.\n",
    "\n",
    "    \n",
    "VARIABLES:\n",
    "***************\n",
    "batch size:    Batch size during training. It's better if the size is higher than the number of result categories.\n",
    "epochs:        Count of training epoch Higher numbers aren't always better since overfitting. Watch out, this model could be\n",
    "               overfitted over 35-45 epoch. If you want to prevent this, use higher dropout value, or make a bigger model. \n",
    "               Default value: 1\n",
    "learning_rate: Learning rate of optimizer. I use ADAM. Default value: 0.0001\n",
    "size:          One dimension from the size of input pictures (28x28). You have to change this if you use other source with\n",
    "               different sizes. Be careful, you have to transform your data before load it. Default value: 28\n",
    "num_workers:   Number of workres during training. Default value: 1\n",
    "***************\n",
    "hidden_1:      Number of hidden layers. Be careful, this model contains fully connected layers. Default value: 256\n",
    "hidden_2:      Number of hidden layers. Be careful, this model contains fully connected layers. Default value: 256\n",
    "output:        Number of result categories. If you want to predict more gestures, you have to change it a higher value.\n",
    "               Default value: 25\n",
    "dropout:       Value of dropout rate, where 0=0% (no dropout) and 1=100%. The value must be between 0 and 1. Higher value\n",
    "               means higher dropping to prevent overfitting. However higher value causes longen the training process.\n",
    "               Default value: 0.2\n",
    "***************\n",
    "\n",
    "\n",
    "LINKS:\n",
    "dataset: https://www.kaggle.com/datamunge/sign-language-mnist\n",
    "PyTorch documentation: https://pytorch.org/docs/stable/index.html\n",
    "PySyft: https://github.com/OpenMined/PySyft\n",
    "\n",
    "\n",
    "SPECIAL THANKS:\n",
    "My best friend, Axel helps me a lot about this code. He supervised the coding process and made me motivated. He is a deep\n",
    "learning coder and participant of another Udacity course names Deep Learning with PyyTorch Nanodegree.\n",
    "\n",
    "\n",
    "OTHER:\n",
    "@author: Richárd Ádám Vécsey Dr. (Richard)\n",
    "@email: richard@hyperrixel.com\n",
    "@github: https://github.com/richardvecsey\n",
    "\n",
    "@contributed partner: Axel Ország-Krisz Dr. (Axel)\n",
    "@email: axel@hyperrixel.com\n",
    "@github: https://github.com/okaxel\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "__author__ = 'Richárd Ádám Vécsey Dr.'\n",
    "\n",
    "__version__ = '1.0'\n",
    "\n",
    "\n",
    "import syft as sy\n",
    "\n",
    "import torch\n",
    "\n",
    "from torch import nn, optim\n",
    "\n",
    "from torchvision import transforms, models\n",
    "\n",
    "\n",
    "batch_size = 75\n",
    "epochs = 1\n",
    "learning_rate = 0.0001\n",
    "size = 28\n",
    "num_workers = 0\n",
    "\n",
    "\n",
    "# Device sets on automaticly. If you want to use cpu only, use the next line:\n",
    "# device = torch.device('cpu')\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Defining Neural Network architecture\n",
    "class SLCatcher(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "\n",
    "        super(SLCatcher, self).__init__()\n",
    "\n",
    "        hidden_1 = 256\n",
    "        hidden_2 = 256\n",
    "        output = 25\n",
    "        dropout = 0.2\n",
    "\n",
    "        self.fc1 = nn.Linear(size * size, hidden_1)\n",
    "        self.fc2 = nn.Linear(hidden_1, hidden_2)\n",
    "        self.fc3 = nn.Linear(hidden_2, output)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = x.view(-1, size * size)\n",
    "        x = torch.relu(self.dropout(self.fc1(x)))\n",
    "        x = torch.relu(self.dropout(self.fc2(x)))\n",
    "        x = self.fc3(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "    \n",
    "def loadmnist():\n",
    "\n",
    "    target_test = list()\n",
    "    target_train = list()\n",
    "    data_test = list()\n",
    "    data_train = list()\n",
    "\n",
    "    # Loading MNIST data\n",
    "    with open('./dataset/sign_mnist_test.csv', 'r') as _data_test_loaded:\n",
    "        _data_test_temp = _data_test_loaded.readlines()\n",
    "\n",
    "    with open('./dataset/sign_mnist_train.csv', 'r') as _data_train_loaded:\n",
    "        _data_train_temp = _data_train_loaded.readlines()\n",
    "\n",
    "    is_first = True\n",
    "\n",
    "    for i in _data_test_temp:\n",
    "\n",
    "        # Dropping first label-line\n",
    "        if is_first:\n",
    "            is_first = False\n",
    "        else:\n",
    "            i = i.split(',')\n",
    "            _target_test = int(i[0])\n",
    "            _data_test = [float(i[x]) / 255.0 for x in range(1, len(i))]\n",
    "\n",
    "            target_test.append(_target_test)\n",
    "            data_test.append(_data_test)\n",
    "\n",
    "    is_first = True\n",
    "\n",
    "    for i in _data_train_temp:\n",
    "\n",
    "        # Dropping first label-line\n",
    "        if is_first:\n",
    "            is_first = False\n",
    "        else:\n",
    "            i = i.split(',')\n",
    "            _target_train = int(i[0])\n",
    "            _data_train = [float(i[x]) / 255.0 for x in range(1, len(i))]\n",
    "\n",
    "            target_train.append(_target_train)\n",
    "            data_train.append(_data_train)\n",
    "\n",
    "    return data_test, target_test, data_train, target_train\n",
    "\n",
    "\n",
    "\n",
    "def main():\n",
    "\n",
    "    global batch_size\n",
    "    global device\n",
    "    global epochs\n",
    "    global learning_rate\n",
    "    global num_workers\n",
    "\n",
    "    model = SLCatcher()\n",
    "\n",
    "    # Waiting for Godot... to load data\n",
    "    data_test, target_test, raw_data_train, raw_target_train = loadmnist()\n",
    "\n",
    "    len_train = len(raw_data_train)\n",
    "    len_test = len(data_test)\n",
    "\n",
    "    # Autobots and Decepticons help to Transform source data to tensor\n",
    "    data_test = torch.tensor(data_test)\n",
    "    target_test = torch.tensor(target_test)\n",
    "    raw_data_train = torch.tensor(raw_data_train)\n",
    "    raw_target_train = torch.tensor(raw_target_train)\n",
    "    \n",
    "    data_test = torch.utils.data.DataLoader(data_test, batch_size=batch_size, num_workers=num_workers)\n",
    "    target_test = torch.utils.data.DataLoader(target_test, batch_size=batch_size, num_workers=num_workers)\n",
    "    data_train = torch.utils.data.DataLoader(raw_data_train, batch_size=batch_size, num_workers=num_workers)\n",
    "    target_train = torch.utils.data.DataLoader(raw_target_train, batch_size=batch_size, num_workers=num_workers)\n",
    "\n",
    "    # Preparing train process\n",
    "    model = model.to(device)\n",
    "    model.train()\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    \n",
    "    # On the Road... Train\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        loss_train = float(0)\n",
    "        true_counter = float(0)\n",
    "        batch_counter = 0\n",
    "\n",
    "        model.train()\n",
    "\n",
    "        for data, target in zip(data_train, target_train):\n",
    "\n",
    "            batch_counter += 1\n",
    "\n",
    "            data = data.to(device)\n",
    "            target = target.to(device)\n",
    "            \n",
    "            # Feel free to try enable modelzero_grad() on the next line:\n",
    "            # model.zero_grad()\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            loss_train += loss.item()*data.shape[0]\n",
    "            _, indices = torch.max(output, 1)\n",
    "            luck_counter = 0\n",
    "            for _index, _target in zip(indices, target):\n",
    "                if _index == _target:\n",
    "                    true_counter += 1\n",
    "\n",
    "        loss_train = loss_train / len_train\n",
    "        accuracy_train = (true_counter * 100) / len_train\n",
    "\n",
    "        print('Epoch {:2d} train     loss: {:.6f}'.format(epoch+1, loss_train))\n",
    "        print('Epoch {:2d} train accuracy: {:.2f}'.format(epoch+1, accuracy_train))\n",
    "\n",
    "\n",
    "\n",
    "        # Preparing test process\n",
    "        model.eval()\n",
    "\n",
    "        # Test\n",
    "        loss_test = float(0)\n",
    "        true_counter = float(0)\n",
    "\n",
    "        for data, target in zip(data_test, target_test):\n",
    "            data = data.to(device)\n",
    "            target = target.to(device)\n",
    "\n",
    "            #model.zero_grad()\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            loss_test += loss.item()*data.shape[0]\n",
    "\n",
    "            _, indices = torch.max(output, 1)\n",
    "            for _index, _target in zip(indices, target):\n",
    "                if _index == _target:\n",
    "                    true_counter += 1\n",
    "\n",
    "        loss_test = loss_test / len_test\n",
    "        accuracy_test = (true_counter * 100) / len_test\n",
    "\n",
    "        print('    Test loss: {:.6f}'.format(loss_test))\n",
    "        print('Test accuracy: {:.2f}'.format(accuracy_test))\n",
    "                \n",
    "        # Here comes the Sun... and federated and secure learning too\n",
    "        hook = sy.TorchHook(torch)\n",
    "        vw1 = sy.VirtualWorker(hook, id=\"vw1\").add_worker(sy.local_worker)\n",
    "        vw2 = sy.VirtualWorker(hook, id=\"vw2\").add_worker(sy.local_worker)\n",
    "        secure_worker = sy.VirtualWorker(hook, id=\"secure_worker\").add_worker(sy.local_worker)\n",
    "        raw_data_train = raw_data_train.to(device)\n",
    "\n",
    "        \n",
    "        \"\"\"\n",
    "        This part of code necessary to secure learning. Running secure codes gives an empty AssertionError on the date\n",
    "        17.08.2019. It comes from the installation process since pip install couldn't make installation process succesfully. \n",
    "        The current version of PySyft doesn't fit with current version of PyTorch,TorchVision and TensorFlow. If you want to\n",
    "        use secure learning, you have to update these. I hope the next versions will be better. Be careful, \"the name\n",
    "        tf.Session is deprecated. Please use tf.compat.v1.Session instead\"\n",
    "        \n",
    "        Secure Learning part begins:\n",
    "        ---------------------------\n",
    "        \n",
    "        encrypted_model = model.fix_precision().share(vw1, vw2, crypto_provider=secure_worker)\n",
    "        encrypted_data_train = raw_data_train.fix_precision().share(vw1, vw2, crypto_provider=secure_worker)\n",
    "        # encrypted_target_train = raw_target_train.fix_precision().share(vw1, vw2, crypto_provider=secure_worker)\n",
    "        encrypted_prediction = encrypted_model(encrypted_data_train)\n",
    "        encrypted_prediction.get().float_precision()\n",
    "        \n",
    "        ---------------------------\n",
    "        Secure Learning part ends\n",
    "        \"\"\"\n",
    "        \n",
    "        # Federated learning part: don't be confused with the variable names, it's without secure learning.\n",
    "        encrypted_model = model.share(vw1, vw2, crypto_provider=secure_worker)\n",
    "        encrypted_data_train = raw_data_train.share(vw1, vw2, crypto_provider=secure_worker)\n",
    "        encrypted_prediction = encrypted_model(encrypted_data_train)\n",
    "        encrypted_prediction.get()\n",
    "        \n",
    "        # That's all Folks!\n",
    "        # I choose this printing method since this is pythonic :-)\n",
    "        print('{}'.format('Finished.'))\n",
    "        \n",
    "        \n",
    "# Main without \"e\" as this is a program not a state.\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
