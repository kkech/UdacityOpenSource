{"cells":[{"metadata":{"trusted":false},"cell_type":"code","source":"%matplotlib inline\n%config InlineBackend.figure_format = 'retina'\n\nimport matplotlib.pyplot as plt\n\nimport torch\nfrom torch import nn\nfrom torch import optim\nimport torch.nn.functional as F\nfrom torchvision import datasets, transforms, models\nfrom autoaugment import ImageNetPolicy","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"torch.set_default_tensor_type(torch.cuda.FloatTensor)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"data_dir = r'D:\\food\\images'\n\n# TODO: Define transforms for the training data and testing data\ntrain_transforms = transforms.Compose([transforms.RandomRotation(30),\n                                       transforms.RandomResizedCrop(224),\n                                       transforms.RandomHorizontalFlip(),ImageNetPolicy(),\n                                       transforms.ToTensor(),\n                                       transforms.Normalize([0.485, 0.456, 0.406],\n                                                            [0.229, 0.224, 0.225])])\n\ntest_transforms = transforms.Compose([transforms.Resize(255),\n                                      transforms.CenterCrop(224),\n                                      transforms.ToTensor(),\n                                      transforms.Normalize([0.485, 0.456, 0.406],\n                                                           [0.229, 0.224, 0.225])])\n\n# Pass transforms in here, then run the next cell to see how the transforms look\ntrain_data = datasets.ImageFolder(data_dir + r'\\train_noise', transform=train_transforms)\ntest_data = datasets.ImageFolder(data_dir + r'\\valid', transform=test_transforms)\ntestdata=datasets.ImageFolder(data_dir + r'\\test', transform=test_transforms)\n\ntrainloader = torch.utils.data.DataLoader(train_data, batch_size=128, shuffle=True)\ntestloader = torch.utils.data.DataLoader(test_data, batch_size=128)\ntest_loader=torch.utils.data.DataLoader(testdata, batch_size=64)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"len(trainloader)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"model =models.densenet121(pretrained=True)\nmodel","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Freeze parameters so we don't backprop through them\nfor param in model.parameters():\n    param.requires_grad = False\n\nfrom collections import OrderedDict\nclassifier = nn.Sequential(OrderedDict([\n                          ('fc1', nn.Linear(1024, 500)),\n                          ('relu', nn.ReLU()),\n                          ('fc2', nn.Linear(500, 101)),\n                          ('output', nn.LogSoftmax(dim=1))\n                          ]))\n    \nmodel.classifier = classifier","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"import time","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"for device in ['cuda']:\n\n    criterion = nn.NLLLoss()\n    # Only train the classifier parameters, feature parameters are frozen\n    optimizer = optim.Adam(model.classifier.parameters(), lr=0.001)\n\n    model.to(device)\n\n    for ii, (inputs, labels) in enumerate(trainloader):\n\n        # Move input and label tensors to the GPU\n        inputs, labels = inputs.to(device), labels.to(device)\n\n        start = time.time()\n\n        outputs = model.forward(inputs)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n\n        if ii==3:\n            break\n        \n    print(f\"Device = {device}; Time per batch: {(time.time() - start)/3:.3f} seconds\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Use GPU if it's available\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Freeze parameters so we don't backprop through them\nfor param in model.parameters():\n    param.requires_grad = False\n\nmodel.classifier = nn.Sequential(nn.Linear(1024,512),nn.LeakyReLU(),nn.Linear(512,256),nn.LeakyReLU(),nn.Linear(256,101))\n\ncriterion = nn.CrossEntropyLoss()\n\n# Only train the classifier parameters, feature parameters are frozen\n#optimizer = optim.Adam(model.fc.parameters(), lr=0.001)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#optimizer = optim.Adam(model.parameters(), lr=0.00001)\noptimizer = optim.Adam(model.classifier.parameters(), lr=0.001, betas=[0.9, 0.999])   ","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"import numpy as np\nimport time\ndef train(n_epochs,trainloader,testloader, resnet, optimizer, criterion, save_path):\n    \"\"\"returns trained model\"\"\"\n    # initialize tracker for minimum validation loss\n    valid_loss_min = np.Inf \n    running_loss=0\n    \n  \n    for epoch in range(n_epochs):\n        \n        \n        for inputs, labels in trainloader:\n            \n        # Move input and label tensors to the default device\n            inputs, labels = inputs.cuda(), labels.cuda()\n            optimizer.zero_grad()\n            start = time.time()\n            logps = resnet(inputs)\n            loss = criterion(logps, labels)\n            loss.backward()\n            optimizer.step()\n        \n            running_loss += loss.item()\n        \n        \n        resnet.eval()\n        valid_loss=0\n        accuracy=0\n        with torch.no_grad():\n            for inputs, labels in testloader:\n                inputs, labels = inputs.cuda(), labels.cuda()\n                logps = resnet(inputs)\n                batch_loss = criterion(logps, labels)\n                valid_loss += batch_loss.item()\n                    \n                    # Calculate accuracy\n                \n                top_p, top_class = logps.topk(1, dim=1)\n                equals = top_class == labels.view(*top_class.shape)\n                accuracy += torch.mean(equals.type(torch.FloatTensor)).item()\n           \n        \n            if valid_loss <= valid_loss_min:\n                print(\"Validation loss decreased  Saving model\")\n                torch.save(resnet.state_dict(),'food_classifier_densenet121_noise.pt')\n                valid_loss_min=valid_loss\n                \n            \n            print(f\"Device = cuda; Time per batch: {(time.time() - start):.3f} seconds\")       \n            print(f\"Epoch /{n_epochs}.. \"\n                  f\"Train loss: {running_loss/len(trainloader):.3f}.. \"\n                  f\"Test loss: {valid_loss/len(testloader):.3f}.. \"\n                  f\"Test accuracy: {accuracy/len(testloader):.3f}\")\n            running_loss = 0\n            resnet.train()            \n            \n        \n       \n     \n\n\n","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":false},"cell_type":"code","source":"train(30,trainloader,testloader, model, optimizer, criterion,'model_vowel_consonant.pt')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"model.load_state_dict(torch.load('food_classifier_densenet121_noise.pt'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"torch.save(model.state_dict(),'food101.pth')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"model.load_state_dict(torch.load('food101.pth'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"valid_loss=0\naccuracy=0\nwith torch.no_grad():\n  model.eval()\n  for images,labels in test_loader:\n    images,lables=images.cuda(),labels.cuda()\n    logps = model(images)\n    batch_loss = criterion(logps, labels)\n    valid_loss += batch_loss.item()\n    top_p, top_class = logps.topk(1, dim=1)\n    equals = top_class == labels.view(*top_class.shape)\n    accuracy += torch.mean(equals.type(torch.FloatTensor)).item()\nprint(valid_loss/len(test_loader))\nprint(accuracy/len(test_loader))             ","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"model.load_state_dict(torch.load('food_classifier_final_1.pt'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"valid_loss=0\naccuracy=0\nwith torch.no_grad():\n  model.eval()\n  for images,labels in testloader:\n    images,lables=images.cuda(),labels.cuda()\n    logps = model(images)\n    batch_loss = criterion(logps, labels)\n    valid_loss += batch_loss.item()\n    top_p, top_class = logps.topk(1, dim=1)\n    equals = top_class == labels.view(*top_class.shape)\n    accuracy += torch.mean(equals.type(torch.FloatTensor)).item()\nprint(valid_loss/len(testloader))\nprint(accuracy/len(testloader))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"model.load_state_dict(torch.load('food_classifier.pt'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"valid_loss=0\naccuracy=0\nwith torch.no_grad():\n  model.eval()\n  for images,labels in trainloader:\n    images,lables=images.cuda(),labels.cuda()\n    logps = model(images)\n    batch_loss = criterion(logps, labels)\n    valid_loss += batch_loss.item()\n    top_p, top_class = logps.topk(1, dim=1)\n    equals = top_class == labels.view(*top_class.shape)\n    accuracy += torch.mean(equals.type(torch.FloatTensor)).item()\nprint(valid_loss/len(trainloader))\nprint(accuracy/len(trainloader))              ","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"import numpy as np\nimport cv2","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"class_names = [item for item in train_data.classes]","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"class_names[20]","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"from PIL import Image\n# list of class names by index, i.e. a name can be accessed like class_names[0]\n#class_names = [item[4:] for item in train_data.classes]\n\ndef predict_food(img_path):\n    \n    # load the image and return the predicted breed\n    img = Image.open(img_path)\n    # Resize\n    left_margin = (img.width-224)/2\n    bottom_margin = (img.height-224)/2\n    right_margin = left_margin + 224\n    top_margin = bottom_margin + 224\n    img = img.crop((left_margin, bottom_margin, right_margin,   \n                      top_margin))\n    # Normalize\n    img = np.array(img)/255\n    mean = np.array([0.485, 0.456, 0.406]) #provided mean\n    std = np.array([0.229, 0.224, 0.225]) #provided std\n    img = (img - mean)/std\n    \n    # Move color channels to first dimension as expected by PyTorch\n    img = img.transpose((2, 0, 1))\n    img = torch.from_numpy(img).type(torch.cuda.FloatTensor) \n    img.unsqueeze_(0)\n    ps=torch.exp(model(img))\n    top_p, top_class = ps.topk(1, dim=1)\n    \n    \n    return top_class.data.cpu().numpy()[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"img_path=r'D:\\Takoyaki.jpg'\nimg = cv2.imread(img_path)\nplt.imshow(img)\nplt.show()\nclass_names[predict_food(img_path).item()]","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"class_names","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"model=models.vgg19_bn(pretrained=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"for param in model.parameters():\n    param.requires_grad = False\n#model.classifier=nn.Sequential(nn.Linear(25088,1000),nn.ReLU(),nn.Dropout(p=0.3),nn.Linear(1000,101),nn.LogSoftmax(dim=1))\nfrom collections import OrderedDict\nclassifier = nn.Sequential(OrderedDict([\n                                       ('fc1', nn.Linear(25088, 1000)),\n                                       ('relu', nn.ReLU()),\n                                       ('dropout', nn.Dropout(p=0.3)),\n                                       ('fc2', nn.Linear(1000, 101)),\n                                       ('logsoftmax', nn.LogSoftmax(dim=1))\n]))\nmodel.classifier=classifier","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"model.load_state_dict(torch.load('food_classifier_george.pt'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"from PIL import Image\n# list of class names by index, i.e. a name can be accessed like class_names[0]\n#class_names = [item[4:] for item in train_data.classes]\n\ndef predict_food(img_path):\n    \n    # load the image and return the predicted breed\n    img = Image.open(img_path)\n    # Resize\n    left_margin = (img.width-224)/2\n    bottom_margin = (img.height-224)/2\n    right_margin = left_margin + 224\n    top_margin = bottom_margin + 224\n    img = img.crop((left_margin, bottom_margin, right_margin,   \n                      top_margin))\n    # Normalize\n    img = np.array(img)/255\n    mean = np.array([0.485, 0.456, 0.406]) #provided mean\n    std = np.array([0.229, 0.224, 0.225]) #provided std\n    img = (img - mean)/std\n    \n    # Move color channels to first dimension as expected by PyTorch\n    img = img.transpose((2, 0, 1))\n    img = torch.from_numpy(img).type(torch.cuda.FloatTensor) \n    img.unsqueeze_(0)\n    ps=torch.exp(model(img))\n    top_p, top_class = ps.topk(1, dim=1)\n    \n    \n    return top_class","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"img_path=r'D:\\Takoyaki.jpg'\nimg = cv2.imread(img_path)\npredict_food(img_path)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"return_label=predict_food(img_path).item()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"return_label","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"return_label=[return_label,1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"return_label.extend([1] * 99)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"print(return_label)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"len(return_label)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"}},"nbformat":4,"nbformat_minor":1}
