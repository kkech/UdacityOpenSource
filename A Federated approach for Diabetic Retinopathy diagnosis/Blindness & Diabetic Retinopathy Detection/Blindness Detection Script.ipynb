{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "# for dirname, _, filenames in os.walk('/kaggle'):\n",
    "#     print(dirname)\n",
    "    # for filename in filenames:\n",
    "    #     print(os.path.join(dirname, filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/pip/_internal/commands/install.py:243: UserWarning: Disabling all use of wheels due to the use of --build-options / --global-options / --install-options.\n",
      "  cmdoptions.check_install_build_global(options)\n",
      "Created temporary directory: /tmp/pip-ephem-wheel-cache-9zcukja2\n",
      "Created temporary directory: /tmp/pip-req-tracker-niaxbcd1\n",
      "Created requirements tracker '/tmp/pip-req-tracker-niaxbcd1'\n",
      "Created temporary directory: /tmp/pip-install-___66x9d\n",
      "Processing /kaggle/input/nvidia-apex-15aug2019/apex-master/apex-master\n",
      "  Created temporary directory: /tmp/pip-req-build-evab29kl\n",
      "  Added file:///kaggle/input/nvidia-apex-15aug2019/apex-master/apex-master to build tracker '/tmp/pip-req-tracker-niaxbcd1'\n",
      "    Running setup.py (path:/tmp/pip-req-build-evab29kl/setup.py) egg_info for package from file:///kaggle/input/nvidia-apex-15aug2019/apex-master/apex-master\n",
      "    Running command python setup.py egg_info\n",
      "    torch.__version__  =  1.2.0\n",
      "    running egg_info\n",
      "    creating pip-egg-info/apex.egg-info\n",
      "    writing pip-egg-info/apex.egg-info/PKG-INFO\n",
      "    writing dependency_links to pip-egg-info/apex.egg-info/dependency_links.txt\n",
      "    writing top-level names to pip-egg-info/apex.egg-info/top_level.txt\n",
      "    writing manifest file 'pip-egg-info/apex.egg-info/SOURCES.txt'\n",
      "    reading manifest file 'pip-egg-info/apex.egg-info/SOURCES.txt'\n",
      "    writing manifest file 'pip-egg-info/apex.egg-info/SOURCES.txt'\n",
      "    /tmp/pip-req-build-evab29kl/setup.py:33: UserWarning: Option --pyprof not specified. Not installing PyProf dependencies!\n",
      "      warnings.warn(\"Option --pyprof not specified. Not installing PyProf dependencies!\")\n",
      "  Source in /tmp/pip-req-build-evab29kl has version 0.1, which satisfies requirement apex==0.1 from file:///kaggle/input/nvidia-apex-15aug2019/apex-master/apex-master\n",
      "  Removed apex==0.1 from file:///kaggle/input/nvidia-apex-15aug2019/apex-master/apex-master from build tracker '/tmp/pip-req-tracker-niaxbcd1'\n",
      "Skipping bdist_wheel for apex, due to binaries being disabled for it.\n",
      "Installing collected packages: apex\n",
      "  Created temporary directory: /tmp/pip-record-_83x469b\n",
      "    Running command /opt/conda/bin/python -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '\"'\"'/tmp/pip-req-build-evab29kl/setup.py'\"'\"'; __file__='\"'\"'/tmp/pip-req-build-evab29kl/setup.py'\"'\"';f=getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__);code=f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' --cpp_ext --cuda_ext install --record /tmp/pip-record-_83x469b/install-record.txt --single-version-externally-managed --compile\n",
      "    torch.__version__  =  1.2.0\n",
      "    /tmp/pip-req-build-evab29kl/setup.py:33: UserWarning: Option --pyprof not specified. Not installing PyProf dependencies!\n",
      "      warnings.warn(\"Option --pyprof not specified. Not installing PyProf dependencies!\")\n",
      "\n",
      "    Compiling cuda extensions with\n",
      "    nvcc: NVIDIA (R) Cuda compiler driver\n",
      "    Copyright (c) 2005-2018 NVIDIA Corporation\n",
      "    Built on Sat_Aug_25_21:08:01_CDT_2018\n",
      "    Cuda compilation tools, release 10.0, V10.0.130\n",
      "    from /usr/local/cuda/bin\n",
      "\n",
      "    running install\n",
      "    running build\n",
      "    running build_py\n",
      "    creating build\n",
      "    creating build/lib.linux-x86_64-3.6\n",
      "    creating build/lib.linux-x86_64-3.6/apex\n",
      "    copying apex/__init__.py -> build/lib.linux-x86_64-3.6/apex\n",
      "    creating build/lib.linux-x86_64-3.6/apex/pyprof\n",
      "    copying apex/pyprof/__init__.py -> build/lib.linux-x86_64-3.6/apex/pyprof\n",
      "    creating build/lib.linux-x86_64-3.6/apex/normalization\n",
      "    copying apex/normalization/fused_layer_norm.py -> build/lib.linux-x86_64-3.6/apex/normalization\n",
      "    copying apex/normalization/__init__.py -> build/lib.linux-x86_64-3.6/apex/normalization\n",
      "    creating build/lib.linux-x86_64-3.6/apex/parallel\n",
      "    copying apex/parallel/optimized_sync_batchnorm_kernel.py -> build/lib.linux-x86_64-3.6/apex/parallel\n",
      "    copying apex/parallel/optimized_sync_batchnorm.py -> build/lib.linux-x86_64-3.6/apex/parallel\n",
      "    copying apex/parallel/distributed.py -> build/lib.linux-x86_64-3.6/apex/parallel\n",
      "    copying apex/parallel/sync_batchnorm.py -> build/lib.linux-x86_64-3.6/apex/parallel\n",
      "    copying apex/parallel/sync_batchnorm_kernel.py -> build/lib.linux-x86_64-3.6/apex/parallel\n",
      "    copying apex/parallel/multiproc.py -> build/lib.linux-x86_64-3.6/apex/parallel\n",
      "    copying apex/parallel/LARC.py -> build/lib.linux-x86_64-3.6/apex/parallel\n",
      "    copying apex/parallel/__init__.py -> build/lib.linux-x86_64-3.6/apex/parallel\n",
      "    creating build/lib.linux-x86_64-3.6/apex/RNN\n",
      "    copying apex/RNN/models.py -> build/lib.linux-x86_64-3.6/apex/RNN\n",
      "    copying apex/RNN/cells.py -> build/lib.linux-x86_64-3.6/apex/RNN\n",
      "    copying apex/RNN/RNNBackend.py -> build/lib.linux-x86_64-3.6/apex/RNN\n",
      "    copying apex/RNN/__init__.py -> build/lib.linux-x86_64-3.6/apex/RNN\n",
      "    creating build/lib.linux-x86_64-3.6/apex/multi_tensor_apply\n",
      "    copying apex/multi_tensor_apply/__init__.py -> build/lib.linux-x86_64-3.6/apex/multi_tensor_apply\n",
      "    copying apex/multi_tensor_apply/multi_tensor_apply.py -> build/lib.linux-x86_64-3.6/apex/multi_tensor_apply\n",
      "    creating build/lib.linux-x86_64-3.6/apex/amp\n",
      "    copying apex/amp/compat.py -> build/lib.linux-x86_64-3.6/apex/amp\n",
      "    copying apex/amp/amp.py -> build/lib.linux-x86_64-3.6/apex/amp\n",
      "    copying apex/amp/_amp_state.py -> build/lib.linux-x86_64-3.6/apex/amp\n",
      "    copying apex/amp/handle.py -> build/lib.linux-x86_64-3.6/apex/amp\n",
      "    copying apex/amp/_process_optimizer.py -> build/lib.linux-x86_64-3.6/apex/amp\n",
      "    copying apex/amp/rnn_compat.py -> build/lib.linux-x86_64-3.6/apex/amp\n",
      "    copying apex/amp/frontend.py -> build/lib.linux-x86_64-3.6/apex/amp\n",
      "    copying apex/amp/_initialize.py -> build/lib.linux-x86_64-3.6/apex/amp\n",
      "    copying apex/amp/opt.py -> build/lib.linux-x86_64-3.6/apex/amp\n",
      "    copying apex/amp/scaler.py -> build/lib.linux-x86_64-3.6/apex/amp\n",
      "    copying apex/amp/__init__.py -> build/lib.linux-x86_64-3.6/apex/amp\n",
      "    copying apex/amp/utils.py -> build/lib.linux-x86_64-3.6/apex/amp\n",
      "    copying apex/amp/wrap.py -> build/lib.linux-x86_64-3.6/apex/amp\n",
      "    copying apex/amp/__version__.py -> build/lib.linux-x86_64-3.6/apex/amp\n",
      "    creating build/lib.linux-x86_64-3.6/apex/fp16_utils\n",
      "    copying apex/fp16_utils/loss_scaler.py -> build/lib.linux-x86_64-3.6/apex/fp16_utils\n",
      "    copying apex/fp16_utils/fp16util.py -> build/lib.linux-x86_64-3.6/apex/fp16_utils\n",
      "    copying apex/fp16_utils/__init__.py -> build/lib.linux-x86_64-3.6/apex/fp16_utils\n",
      "    copying apex/fp16_utils/fp16_optimizer.py -> build/lib.linux-x86_64-3.6/apex/fp16_utils\n",
      "    creating build/lib.linux-x86_64-3.6/apex/optimizers\n",
      "    copying apex/optimizers/fused_adam.py -> build/lib.linux-x86_64-3.6/apex/optimizers\n",
      "    copying apex/optimizers/__init__.py -> build/lib.linux-x86_64-3.6/apex/optimizers\n",
      "    copying apex/optimizers/fp16_optimizer.py -> build/lib.linux-x86_64-3.6/apex/optimizers\n",
      "    creating build/lib.linux-x86_64-3.6/apex/reparameterization\n",
      "    copying apex/reparameterization/weight_norm.py -> build/lib.linux-x86_64-3.6/apex/reparameterization\n",
      "    copying apex/reparameterization/reparameterization.py -> build/lib.linux-x86_64-3.6/apex/reparameterization\n",
      "    copying apex/reparameterization/__init__.py -> build/lib.linux-x86_64-3.6/apex/reparameterization\n",
      "    creating build/lib.linux-x86_64-3.6/apex/pyprof/nvtx\n",
      "    copying apex/pyprof/nvtx/nvmarker.py -> build/lib.linux-x86_64-3.6/apex/pyprof/nvtx\n",
      "    copying apex/pyprof/nvtx/__init__.py -> build/lib.linux-x86_64-3.6/apex/pyprof/nvtx\n",
      "    creating build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
      "    copying apex/pyprof/prof/blas.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
      "    copying apex/pyprof/prof/utility.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
      "    copying apex/pyprof/prof/linear.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
      "    copying apex/pyprof/prof/reduction.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
      "    copying apex/pyprof/prof/loss.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
      "    copying apex/pyprof/prof/dropout.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
      "    copying apex/pyprof/prof/index_slice_join_mutate.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
      "    copying apex/pyprof/prof/normalization.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
      "    copying apex/pyprof/prof/randomSample.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
      "    copying apex/pyprof/prof/softmax.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
      "    copying apex/pyprof/prof/convert.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
      "    copying apex/pyprof/prof/recurrentCell.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
      "    copying apex/pyprof/prof/activation.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
      "    copying apex/pyprof/prof/pointwise.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
      "    copying apex/pyprof/prof/data.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
      "    copying apex/pyprof/prof/conv.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
      "    copying apex/pyprof/prof/base.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    copying apex/pyprof/prof/output.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
      "    copying apex/pyprof/prof/pooling.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
      "    copying apex/pyprof/prof/usage.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
      "    copying apex/pyprof/prof/embedding.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
      "    copying apex/pyprof/prof/__init__.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
      "    copying apex/pyprof/prof/prof.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
      "    copying apex/pyprof/prof/misc.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
      "    copying apex/pyprof/prof/__main__.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
      "    copying apex/pyprof/prof/optim.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
      "    creating build/lib.linux-x86_64-3.6/apex/pyprof/parse\n",
      "    copying apex/pyprof/parse/parse.py -> build/lib.linux-x86_64-3.6/apex/pyprof/parse\n",
      "    copying apex/pyprof/parse/db.py -> build/lib.linux-x86_64-3.6/apex/pyprof/parse\n",
      "    copying apex/pyprof/parse/kernel.py -> build/lib.linux-x86_64-3.6/apex/pyprof/parse\n",
      "    copying apex/pyprof/parse/nvvp.py -> build/lib.linux-x86_64-3.6/apex/pyprof/parse\n",
      "    copying apex/pyprof/parse/__init__.py -> build/lib.linux-x86_64-3.6/apex/pyprof/parse\n",
      "    copying apex/pyprof/parse/__main__.py -> build/lib.linux-x86_64-3.6/apex/pyprof/parse\n",
      "    creating build/lib.linux-x86_64-3.6/apex/amp/lists\n",
      "    copying apex/amp/lists/torch_overrides.py -> build/lib.linux-x86_64-3.6/apex/amp/lists\n",
      "    copying apex/amp/lists/functional_overrides.py -> build/lib.linux-x86_64-3.6/apex/amp/lists\n",
      "    copying apex/amp/lists/__init__.py -> build/lib.linux-x86_64-3.6/apex/amp/lists\n",
      "    copying apex/amp/lists/tensor_overrides.py -> build/lib.linux-x86_64-3.6/apex/amp/lists\n",
      "    running build_ext\n",
      "    building 'apex_C' extension\n",
      "    creating build/temp.linux-x86_64-3.6\n",
      "    creating build/temp.linux-x86_64-3.6/csrc\n",
      "    gcc -pthread -B /opt/conda/compiler_compat -Wl,--sysroot=/ -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -Wstrict-prototypes -fPIC -I/opt/conda/lib/python3.6/site-packages/torch/include -I/opt/conda/lib/python3.6/site-packages/torch/include/torch/csrc/api/include -I/opt/conda/lib/python3.6/site-packages/torch/include/TH -I/opt/conda/lib/python3.6/site-packages/torch/include/THC -I/opt/conda/include/python3.6m -c csrc/flatten_unflatten.cpp -o build/temp.linux-x86_64-3.6/csrc/flatten_unflatten.o -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=apex_C -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++11\n",
      "    cc1plus: warning: command line option ‘-Wstrict-prototypes’ is valid for C/ObjC but not for C++\n",
      "    g++ -pthread -shared -B /opt/conda/compiler_compat -L/opt/conda/lib -Wl,-rpath=/opt/conda/lib -Wl,--no-as-needed -Wl,--sysroot=/ build/temp.linux-x86_64-3.6/csrc/flatten_unflatten.o -o build/lib.linux-x86_64-3.6/apex_C.cpython-36m-x86_64-linux-gnu.so\n",
      "    building 'amp_C' extension\n",
      "    gcc -pthread -B /opt/conda/compiler_compat -Wl,--sysroot=/ -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -Wstrict-prototypes -fPIC -I/opt/conda/lib/python3.6/site-packages/torch/include -I/opt/conda/lib/python3.6/site-packages/torch/include/torch/csrc/api/include -I/opt/conda/lib/python3.6/site-packages/torch/include/TH -I/opt/conda/lib/python3.6/site-packages/torch/include/THC -I/usr/local/cuda/include -I/opt/conda/include/python3.6m -c csrc/amp_C_frontend.cpp -o build/temp.linux-x86_64-3.6/csrc/amp_C_frontend.o -O3 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++11\n",
      "    cc1plus: warning: command line option ‘-Wstrict-prototypes’ is valid for C/ObjC but not for C++\n",
      "    /usr/local/cuda/bin/nvcc -I/opt/conda/lib/python3.6/site-packages/torch/include -I/opt/conda/lib/python3.6/site-packages/torch/include/torch/csrc/api/include -I/opt/conda/lib/python3.6/site-packages/torch/include/TH -I/opt/conda/lib/python3.6/site-packages/torch/include/THC -I/usr/local/cuda/include -I/opt/conda/include/python3.6m -c csrc/multi_tensor_scale_kernel.cu -o build/temp.linux-x86_64-3.6/csrc/multi_tensor_scale_kernel.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++11\n",
      "    /usr/local/cuda/bin/nvcc -I/opt/conda/lib/python3.6/site-packages/torch/include -I/opt/conda/lib/python3.6/site-packages/torch/include/torch/csrc/api/include -I/opt/conda/lib/python3.6/site-packages/torch/include/TH -I/opt/conda/lib/python3.6/site-packages/torch/include/THC -I/usr/local/cuda/include -I/opt/conda/include/python3.6m -c csrc/multi_tensor_axpby_kernel.cu -o build/temp.linux-x86_64-3.6/csrc/multi_tensor_axpby_kernel.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++11\n",
      "    /usr/local/cuda/bin/nvcc -I/opt/conda/lib/python3.6/site-packages/torch/include -I/opt/conda/lib/python3.6/site-packages/torch/include/torch/csrc/api/include -I/opt/conda/lib/python3.6/site-packages/torch/include/TH -I/opt/conda/lib/python3.6/site-packages/torch/include/THC -I/usr/local/cuda/include -I/opt/conda/include/python3.6m -c csrc/multi_tensor_l2norm_kernel.cu -o build/temp.linux-x86_64-3.6/csrc/multi_tensor_l2norm_kernel.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++11\n",
      "    /usr/local/cuda/bin/nvcc -I/opt/conda/lib/python3.6/site-packages/torch/include -I/opt/conda/lib/python3.6/site-packages/torch/include/torch/csrc/api/include -I/opt/conda/lib/python3.6/site-packages/torch/include/TH -I/opt/conda/lib/python3.6/site-packages/torch/include/THC -I/usr/local/cuda/include -I/opt/conda/include/python3.6m -c csrc/multi_tensor_lamb_stage_1.cu -o build/temp.linux-x86_64-3.6/csrc/multi_tensor_lamb_stage_1.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++11\n",
      "    /usr/local/cuda/bin/nvcc -I/opt/conda/lib/python3.6/site-packages/torch/include -I/opt/conda/lib/python3.6/site-packages/torch/include/torch/csrc/api/include -I/opt/conda/lib/python3.6/site-packages/torch/include/TH -I/opt/conda/lib/python3.6/site-packages/torch/include/THC -I/usr/local/cuda/include -I/opt/conda/include/python3.6m -c csrc/multi_tensor_lamb_stage_2.cu -o build/temp.linux-x86_64-3.6/csrc/multi_tensor_lamb_stage_2.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++11\n",
      "    g++ -pthread -shared -B /opt/conda/compiler_compat -L/opt/conda/lib -Wl,-rpath=/opt/conda/lib -Wl,--no-as-needed -Wl,--sysroot=/ build/temp.linux-x86_64-3.6/csrc/amp_C_frontend.o build/temp.linux-x86_64-3.6/csrc/multi_tensor_scale_kernel.o build/temp.linux-x86_64-3.6/csrc/multi_tensor_axpby_kernel.o build/temp.linux-x86_64-3.6/csrc/multi_tensor_l2norm_kernel.o build/temp.linux-x86_64-3.6/csrc/multi_tensor_lamb_stage_1.o build/temp.linux-x86_64-3.6/csrc/multi_tensor_lamb_stage_2.o -L/usr/local/cuda/lib64 -lcudart -o build/lib.linux-x86_64-3.6/amp_C.cpython-36m-x86_64-linux-gnu.so\n",
      "    building 'fused_adam_cuda' extension\n",
      "    gcc -pthread -B /opt/conda/compiler_compat -Wl,--sysroot=/ -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -Wstrict-prototypes -fPIC -I/opt/conda/lib/python3.6/site-packages/torch/include -I/opt/conda/lib/python3.6/site-packages/torch/include/torch/csrc/api/include -I/opt/conda/lib/python3.6/site-packages/torch/include/TH -I/opt/conda/lib/python3.6/site-packages/torch/include/THC -I/usr/local/cuda/include -I/opt/conda/include/python3.6m -c csrc/fused_adam_cuda.cpp -o build/temp.linux-x86_64-3.6/csrc/fused_adam_cuda.o -O3 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=fused_adam_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++11\n",
      "    cc1plus: warning: command line option ‘-Wstrict-prototypes’ is valid for C/ObjC but not for C++\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    /usr/local/cuda/bin/nvcc -I/opt/conda/lib/python3.6/site-packages/torch/include -I/opt/conda/lib/python3.6/site-packages/torch/include/torch/csrc/api/include -I/opt/conda/lib/python3.6/site-packages/torch/include/TH -I/opt/conda/lib/python3.6/site-packages/torch/include/THC -I/usr/local/cuda/include -I/opt/conda/include/python3.6m -c csrc/fused_adam_cuda_kernel.cu -o build/temp.linux-x86_64-3.6/csrc/fused_adam_cuda_kernel.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -O3 --use_fast_math -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=fused_adam_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++11\n",
      "    g++ -pthread -shared -B /opt/conda/compiler_compat -L/opt/conda/lib -Wl,-rpath=/opt/conda/lib -Wl,--no-as-needed -Wl,--sysroot=/ build/temp.linux-x86_64-3.6/csrc/fused_adam_cuda.o build/temp.linux-x86_64-3.6/csrc/fused_adam_cuda_kernel.o -L/usr/local/cuda/lib64 -lcudart -o build/lib.linux-x86_64-3.6/fused_adam_cuda.cpython-36m-x86_64-linux-gnu.so\n",
      "    building 'syncbn' extension\n",
      "    gcc -pthread -B /opt/conda/compiler_compat -Wl,--sysroot=/ -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -Wstrict-prototypes -fPIC -I/opt/conda/lib/python3.6/site-packages/torch/include -I/opt/conda/lib/python3.6/site-packages/torch/include/torch/csrc/api/include -I/opt/conda/lib/python3.6/site-packages/torch/include/TH -I/opt/conda/lib/python3.6/site-packages/torch/include/THC -I/usr/local/cuda/include -I/opt/conda/include/python3.6m -c csrc/syncbn.cpp -o build/temp.linux-x86_64-3.6/csrc/syncbn.o -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=syncbn -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++11\n",
      "    cc1plus: warning: command line option ‘-Wstrict-prototypes’ is valid for C/ObjC but not for C++\n",
      "    /usr/local/cuda/bin/nvcc -I/opt/conda/lib/python3.6/site-packages/torch/include -I/opt/conda/lib/python3.6/site-packages/torch/include/torch/csrc/api/include -I/opt/conda/lib/python3.6/site-packages/torch/include/TH -I/opt/conda/lib/python3.6/site-packages/torch/include/THC -I/usr/local/cuda/include -I/opt/conda/include/python3.6m -c csrc/welford.cu -o build/temp.linux-x86_64-3.6/csrc/welford.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=syncbn -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++11\n",
      "    g++ -pthread -shared -B /opt/conda/compiler_compat -L/opt/conda/lib -Wl,-rpath=/opt/conda/lib -Wl,--no-as-needed -Wl,--sysroot=/ build/temp.linux-x86_64-3.6/csrc/syncbn.o build/temp.linux-x86_64-3.6/csrc/welford.o -L/usr/local/cuda/lib64 -lcudart -o build/lib.linux-x86_64-3.6/syncbn.cpython-36m-x86_64-linux-gnu.so\n",
      "    building 'fused_layer_norm_cuda' extension\n",
      "    gcc -pthread -B /opt/conda/compiler_compat -Wl,--sysroot=/ -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -Wstrict-prototypes -fPIC -I/opt/conda/lib/python3.6/site-packages/torch/include -I/opt/conda/lib/python3.6/site-packages/torch/include/torch/csrc/api/include -I/opt/conda/lib/python3.6/site-packages/torch/include/TH -I/opt/conda/lib/python3.6/site-packages/torch/include/THC -I/usr/local/cuda/include -I/opt/conda/include/python3.6m -c csrc/layer_norm_cuda.cpp -o build/temp.linux-x86_64-3.6/csrc/layer_norm_cuda.o -O3 -DVERSION_GE_1_1 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=fused_layer_norm_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++11\n",
      "    cc1plus: warning: command line option ‘-Wstrict-prototypes’ is valid for C/ObjC but not for C++\n",
      "    /usr/local/cuda/bin/nvcc -I/opt/conda/lib/python3.6/site-packages/torch/include -I/opt/conda/lib/python3.6/site-packages/torch/include/torch/csrc/api/include -I/opt/conda/lib/python3.6/site-packages/torch/include/TH -I/opt/conda/lib/python3.6/site-packages/torch/include/THC -I/usr/local/cuda/include -I/opt/conda/include/python3.6m -c csrc/layer_norm_cuda_kernel.cu -o build/temp.linux-x86_64-3.6/csrc/layer_norm_cuda_kernel.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -maxrregcount=50 -O3 --use_fast_math -DVERSION_GE_1_1 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=fused_layer_norm_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++11\n",
      "    g++ -pthread -shared -B /opt/conda/compiler_compat -L/opt/conda/lib -Wl,-rpath=/opt/conda/lib -Wl,--no-as-needed -Wl,--sysroot=/ build/temp.linux-x86_64-3.6/csrc/layer_norm_cuda.o build/temp.linux-x86_64-3.6/csrc/layer_norm_cuda_kernel.o -L/usr/local/cuda/lib64 -lcudart -o build/lib.linux-x86_64-3.6/fused_layer_norm_cuda.cpython-36m-x86_64-linux-gnu.so\n",
      "    running install_lib\n",
      "    copying build/lib.linux-x86_64-3.6/fused_adam_cuda.cpython-36m-x86_64-linux-gnu.so -> /opt/conda/lib/python3.6/site-packages\n",
      "    creating /opt/conda/lib/python3.6/site-packages/apex\n",
      "    creating /opt/conda/lib/python3.6/site-packages/apex/pyprof\n",
      "    creating /opt/conda/lib/python3.6/site-packages/apex/pyprof/nvtx\n",
      "    copying build/lib.linux-x86_64-3.6/apex/pyprof/nvtx/nvmarker.py -> /opt/conda/lib/python3.6/site-packages/apex/pyprof/nvtx\n",
      "    copying build/lib.linux-x86_64-3.6/apex/pyprof/nvtx/__init__.py -> /opt/conda/lib/python3.6/site-packages/apex/pyprof/nvtx\n",
      "    creating /opt/conda/lib/python3.6/site-packages/apex/pyprof/prof\n",
      "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/blas.py -> /opt/conda/lib/python3.6/site-packages/apex/pyprof/prof\n",
      "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/utility.py -> /opt/conda/lib/python3.6/site-packages/apex/pyprof/prof\n",
      "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/linear.py -> /opt/conda/lib/python3.6/site-packages/apex/pyprof/prof\n",
      "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/reduction.py -> /opt/conda/lib/python3.6/site-packages/apex/pyprof/prof\n",
      "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/loss.py -> /opt/conda/lib/python3.6/site-packages/apex/pyprof/prof\n",
      "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/dropout.py -> /opt/conda/lib/python3.6/site-packages/apex/pyprof/prof\n",
      "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/index_slice_join_mutate.py -> /opt/conda/lib/python3.6/site-packages/apex/pyprof/prof\n",
      "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/normalization.py -> /opt/conda/lib/python3.6/site-packages/apex/pyprof/prof\n",
      "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/randomSample.py -> /opt/conda/lib/python3.6/site-packages/apex/pyprof/prof\n",
      "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/softmax.py -> /opt/conda/lib/python3.6/site-packages/apex/pyprof/prof\n",
      "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/convert.py -> /opt/conda/lib/python3.6/site-packages/apex/pyprof/prof\n",
      "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/recurrentCell.py -> /opt/conda/lib/python3.6/site-packages/apex/pyprof/prof\n",
      "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/activation.py -> /opt/conda/lib/python3.6/site-packages/apex/pyprof/prof\n",
      "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/pointwise.py -> /opt/conda/lib/python3.6/site-packages/apex/pyprof/prof\n",
      "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/data.py -> /opt/conda/lib/python3.6/site-packages/apex/pyprof/prof\n",
      "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/conv.py -> /opt/conda/lib/python3.6/site-packages/apex/pyprof/prof\n",
      "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/base.py -> /opt/conda/lib/python3.6/site-packages/apex/pyprof/prof\n",
      "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/output.py -> /opt/conda/lib/python3.6/site-packages/apex/pyprof/prof\n",
      "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/pooling.py -> /opt/conda/lib/python3.6/site-packages/apex/pyprof/prof\n",
      "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/usage.py -> /opt/conda/lib/python3.6/site-packages/apex/pyprof/prof\n",
      "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/embedding.py -> /opt/conda/lib/python3.6/site-packages/apex/pyprof/prof\n",
      "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/__init__.py -> /opt/conda/lib/python3.6/site-packages/apex/pyprof/prof\n",
      "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/prof.py -> /opt/conda/lib/python3.6/site-packages/apex/pyprof/prof\n",
      "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/misc.py -> /opt/conda/lib/python3.6/site-packages/apex/pyprof/prof\n",
      "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/__main__.py -> /opt/conda/lib/python3.6/site-packages/apex/pyprof/prof\n",
      "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/optim.py -> /opt/conda/lib/python3.6/site-packages/apex/pyprof/prof\n",
      "    creating /opt/conda/lib/python3.6/site-packages/apex/pyprof/parse\n",
      "    copying build/lib.linux-x86_64-3.6/apex/pyprof/parse/parse.py -> /opt/conda/lib/python3.6/site-packages/apex/pyprof/parse\n",
      "    copying build/lib.linux-x86_64-3.6/apex/pyprof/parse/db.py -> /opt/conda/lib/python3.6/site-packages/apex/pyprof/parse\n",
      "    copying build/lib.linux-x86_64-3.6/apex/pyprof/parse/kernel.py -> /opt/conda/lib/python3.6/site-packages/apex/pyprof/parse\n",
      "    copying build/lib.linux-x86_64-3.6/apex/pyprof/parse/nvvp.py -> /opt/conda/lib/python3.6/site-packages/apex/pyprof/parse\n",
      "    copying build/lib.linux-x86_64-3.6/apex/pyprof/parse/__init__.py -> /opt/conda/lib/python3.6/site-packages/apex/pyprof/parse\n",
      "    copying build/lib.linux-x86_64-3.6/apex/pyprof/parse/__main__.py -> /opt/conda/lib/python3.6/site-packages/apex/pyprof/parse\n",
      "    copying build/lib.linux-x86_64-3.6/apex/pyprof/__init__.py -> /opt/conda/lib/python3.6/site-packages/apex/pyprof\n",
      "    creating /opt/conda/lib/python3.6/site-packages/apex/normalization\n",
      "    copying build/lib.linux-x86_64-3.6/apex/normalization/fused_layer_norm.py -> /opt/conda/lib/python3.6/site-packages/apex/normalization\n",
      "    copying build/lib.linux-x86_64-3.6/apex/normalization/__init__.py -> /opt/conda/lib/python3.6/site-packages/apex/normalization\n",
      "    creating /opt/conda/lib/python3.6/site-packages/apex/parallel\n",
      "    copying build/lib.linux-x86_64-3.6/apex/parallel/optimized_sync_batchnorm_kernel.py -> /opt/conda/lib/python3.6/site-packages/apex/parallel\n",
      "    copying build/lib.linux-x86_64-3.6/apex/parallel/optimized_sync_batchnorm.py -> /opt/conda/lib/python3.6/site-packages/apex/parallel\n",
      "    copying build/lib.linux-x86_64-3.6/apex/parallel/distributed.py -> /opt/conda/lib/python3.6/site-packages/apex/parallel\n",
      "    copying build/lib.linux-x86_64-3.6/apex/parallel/sync_batchnorm.py -> /opt/conda/lib/python3.6/site-packages/apex/parallel\n",
      "    copying build/lib.linux-x86_64-3.6/apex/parallel/sync_batchnorm_kernel.py -> /opt/conda/lib/python3.6/site-packages/apex/parallel\n",
      "    copying build/lib.linux-x86_64-3.6/apex/parallel/multiproc.py -> /opt/conda/lib/python3.6/site-packages/apex/parallel\n",
      "    copying build/lib.linux-x86_64-3.6/apex/parallel/LARC.py -> /opt/conda/lib/python3.6/site-packages/apex/parallel\n",
      "    copying build/lib.linux-x86_64-3.6/apex/parallel/__init__.py -> /opt/conda/lib/python3.6/site-packages/apex/parallel\n",
      "    creating /opt/conda/lib/python3.6/site-packages/apex/RNN\n",
      "    copying build/lib.linux-x86_64-3.6/apex/RNN/models.py -> /opt/conda/lib/python3.6/site-packages/apex/RNN\n",
      "    copying build/lib.linux-x86_64-3.6/apex/RNN/cells.py -> /opt/conda/lib/python3.6/site-packages/apex/RNN\n",
      "    copying build/lib.linux-x86_64-3.6/apex/RNN/RNNBackend.py -> /opt/conda/lib/python3.6/site-packages/apex/RNN\n",
      "    copying build/lib.linux-x86_64-3.6/apex/RNN/__init__.py -> /opt/conda/lib/python3.6/site-packages/apex/RNN\n",
      "    creating /opt/conda/lib/python3.6/site-packages/apex/multi_tensor_apply\n",
      "    copying build/lib.linux-x86_64-3.6/apex/multi_tensor_apply/__init__.py -> /opt/conda/lib/python3.6/site-packages/apex/multi_tensor_apply\n",
      "    copying build/lib.linux-x86_64-3.6/apex/multi_tensor_apply/multi_tensor_apply.py -> /opt/conda/lib/python3.6/site-packages/apex/multi_tensor_apply\n",
      "    creating /opt/conda/lib/python3.6/site-packages/apex/amp\n",
      "    copying build/lib.linux-x86_64-3.6/apex/amp/compat.py -> /opt/conda/lib/python3.6/site-packages/apex/amp\n",
      "    copying build/lib.linux-x86_64-3.6/apex/amp/amp.py -> /opt/conda/lib/python3.6/site-packages/apex/amp\n",
      "    copying build/lib.linux-x86_64-3.6/apex/amp/_amp_state.py -> /opt/conda/lib/python3.6/site-packages/apex/amp\n",
      "    copying build/lib.linux-x86_64-3.6/apex/amp/handle.py -> /opt/conda/lib/python3.6/site-packages/apex/amp\n",
      "    copying build/lib.linux-x86_64-3.6/apex/amp/_process_optimizer.py -> /opt/conda/lib/python3.6/site-packages/apex/amp\n",
      "    copying build/lib.linux-x86_64-3.6/apex/amp/rnn_compat.py -> /opt/conda/lib/python3.6/site-packages/apex/amp\n",
      "    copying build/lib.linux-x86_64-3.6/apex/amp/frontend.py -> /opt/conda/lib/python3.6/site-packages/apex/amp\n",
      "    copying build/lib.linux-x86_64-3.6/apex/amp/_initialize.py -> /opt/conda/lib/python3.6/site-packages/apex/amp\n",
      "    copying build/lib.linux-x86_64-3.6/apex/amp/opt.py -> /opt/conda/lib/python3.6/site-packages/apex/amp\n",
      "    copying build/lib.linux-x86_64-3.6/apex/amp/scaler.py -> /opt/conda/lib/python3.6/site-packages/apex/amp\n",
      "    copying build/lib.linux-x86_64-3.6/apex/amp/__init__.py -> /opt/conda/lib/python3.6/site-packages/apex/amp\n",
      "    copying build/lib.linux-x86_64-3.6/apex/amp/utils.py -> /opt/conda/lib/python3.6/site-packages/apex/amp\n",
      "    creating /opt/conda/lib/python3.6/site-packages/apex/amp/lists\n",
      "    copying build/lib.linux-x86_64-3.6/apex/amp/lists/torch_overrides.py -> /opt/conda/lib/python3.6/site-packages/apex/amp/lists\n",
      "    copying build/lib.linux-x86_64-3.6/apex/amp/lists/functional_overrides.py -> /opt/conda/lib/python3.6/site-packages/apex/amp/lists\n",
      "    copying build/lib.linux-x86_64-3.6/apex/amp/lists/__init__.py -> /opt/conda/lib/python3.6/site-packages/apex/amp/lists\n",
      "    copying build/lib.linux-x86_64-3.6/apex/amp/lists/tensor_overrides.py -> /opt/conda/lib/python3.6/site-packages/apex/amp/lists\n",
      "    copying build/lib.linux-x86_64-3.6/apex/amp/wrap.py -> /opt/conda/lib/python3.6/site-packages/apex/amp\n",
      "    copying build/lib.linux-x86_64-3.6/apex/amp/__version__.py -> /opt/conda/lib/python3.6/site-packages/apex/amp\n",
      "    creating /opt/conda/lib/python3.6/site-packages/apex/fp16_utils\n",
      "    copying build/lib.linux-x86_64-3.6/apex/fp16_utils/loss_scaler.py -> /opt/conda/lib/python3.6/site-packages/apex/fp16_utils\n",
      "    copying build/lib.linux-x86_64-3.6/apex/fp16_utils/fp16util.py -> /opt/conda/lib/python3.6/site-packages/apex/fp16_utils\n",
      "    copying build/lib.linux-x86_64-3.6/apex/fp16_utils/__init__.py -> /opt/conda/lib/python3.6/site-packages/apex/fp16_utils\n",
      "    copying build/lib.linux-x86_64-3.6/apex/fp16_utils/fp16_optimizer.py -> /opt/conda/lib/python3.6/site-packages/apex/fp16_utils\n",
      "    copying build/lib.linux-x86_64-3.6/apex/__init__.py -> /opt/conda/lib/python3.6/site-packages/apex\n",
      "    creating /opt/conda/lib/python3.6/site-packages/apex/optimizers\n",
      "    copying build/lib.linux-x86_64-3.6/apex/optimizers/fused_adam.py -> /opt/conda/lib/python3.6/site-packages/apex/optimizers\n",
      "    copying build/lib.linux-x86_64-3.6/apex/optimizers/__init__.py -> /opt/conda/lib/python3.6/site-packages/apex/optimizers\n",
      "    copying build/lib.linux-x86_64-3.6/apex/optimizers/fp16_optimizer.py -> /opt/conda/lib/python3.6/site-packages/apex/optimizers\n",
      "    creating /opt/conda/lib/python3.6/site-packages/apex/reparameterization\n",
      "    copying build/lib.linux-x86_64-3.6/apex/reparameterization/weight_norm.py -> /opt/conda/lib/python3.6/site-packages/apex/reparameterization\n",
      "    copying build/lib.linux-x86_64-3.6/apex/reparameterization/reparameterization.py -> /opt/conda/lib/python3.6/site-packages/apex/reparameterization\n",
      "    copying build/lib.linux-x86_64-3.6/apex/reparameterization/__init__.py -> /opt/conda/lib/python3.6/site-packages/apex/reparameterization\n",
      "    copying build/lib.linux-x86_64-3.6/syncbn.cpython-36m-x86_64-linux-gnu.so -> /opt/conda/lib/python3.6/site-packages\n",
      "    copying build/lib.linux-x86_64-3.6/apex_C.cpython-36m-x86_64-linux-gnu.so -> /opt/conda/lib/python3.6/site-packages\n",
      "    copying build/lib.linux-x86_64-3.6/amp_C.cpython-36m-x86_64-linux-gnu.so -> /opt/conda/lib/python3.6/site-packages\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    copying build/lib.linux-x86_64-3.6/fused_layer_norm_cuda.cpython-36m-x86_64-linux-gnu.so -> /opt/conda/lib/python3.6/site-packages\n",
      "    byte-compiling /opt/conda/lib/python3.6/site-packages/apex/pyprof/nvtx/nvmarker.py to nvmarker.cpython-36.pyc\n",
      "    byte-compiling /opt/conda/lib/python3.6/site-packages/apex/pyprof/nvtx/__init__.py to __init__.cpython-36.pyc\n",
      "    byte-compiling /opt/conda/lib/python3.6/site-packages/apex/pyprof/prof/blas.py to blas.cpython-36.pyc\n",
      "    byte-compiling /opt/conda/lib/python3.6/site-packages/apex/pyprof/prof/utility.py to utility.cpython-36.pyc\n",
      "    byte-compiling /opt/conda/lib/python3.6/site-packages/apex/pyprof/prof/linear.py to linear.cpython-36.pyc\n",
      "    byte-compiling /opt/conda/lib/python3.6/site-packages/apex/pyprof/prof/reduction.py to reduction.cpython-36.pyc\n",
      "    byte-compiling /opt/conda/lib/python3.6/site-packages/apex/pyprof/prof/loss.py to loss.cpython-36.pyc\n",
      "    byte-compiling /opt/conda/lib/python3.6/site-packages/apex/pyprof/prof/dropout.py to dropout.cpython-36.pyc\n",
      "    byte-compiling /opt/conda/lib/python3.6/site-packages/apex/pyprof/prof/index_slice_join_mutate.py to index_slice_join_mutate.cpython-36.pyc\n",
      "    byte-compiling /opt/conda/lib/python3.6/site-packages/apex/pyprof/prof/normalization.py to normalization.cpython-36.pyc\n",
      "    byte-compiling /opt/conda/lib/python3.6/site-packages/apex/pyprof/prof/randomSample.py to randomSample.cpython-36.pyc\n",
      "    byte-compiling /opt/conda/lib/python3.6/site-packages/apex/pyprof/prof/softmax.py to softmax.cpython-36.pyc\n",
      "    byte-compiling /opt/conda/lib/python3.6/site-packages/apex/pyprof/prof/convert.py to convert.cpython-36.pyc\n",
      "    byte-compiling /opt/conda/lib/python3.6/site-packages/apex/pyprof/prof/recurrentCell.py to recurrentCell.cpython-36.pyc\n",
      "    byte-compiling /opt/conda/lib/python3.6/site-packages/apex/pyprof/prof/activation.py to activation.cpython-36.pyc\n",
      "    byte-compiling /opt/conda/lib/python3.6/site-packages/apex/pyprof/prof/pointwise.py to pointwise.cpython-36.pyc\n",
      "    byte-compiling /opt/conda/lib/python3.6/site-packages/apex/pyprof/prof/data.py to data.cpython-36.pyc\n",
      "    byte-compiling /opt/conda/lib/python3.6/site-packages/apex/pyprof/prof/conv.py to conv.cpython-36.pyc\n",
      "    byte-compiling /opt/conda/lib/python3.6/site-packages/apex/pyprof/prof/base.py to base.cpython-36.pyc\n",
      "    byte-compiling /opt/conda/lib/python3.6/site-packages/apex/pyprof/prof/output.py to output.cpython-36.pyc\n",
      "    byte-compiling /opt/conda/lib/python3.6/site-packages/apex/pyprof/prof/pooling.py to pooling.cpython-36.pyc\n",
      "    byte-compiling /opt/conda/lib/python3.6/site-packages/apex/pyprof/prof/usage.py to usage.cpython-36.pyc\n",
      "    byte-compiling /opt/conda/lib/python3.6/site-packages/apex/pyprof/prof/embedding.py to embedding.cpython-36.pyc\n",
      "    byte-compiling /opt/conda/lib/python3.6/site-packages/apex/pyprof/prof/__init__.py to __init__.cpython-36.pyc\n",
      "    byte-compiling /opt/conda/lib/python3.6/site-packages/apex/pyprof/prof/prof.py to prof.cpython-36.pyc\n",
      "    byte-compiling /opt/conda/lib/python3.6/site-packages/apex/pyprof/prof/misc.py to misc.cpython-36.pyc\n",
      "    byte-compiling /opt/conda/lib/python3.6/site-packages/apex/pyprof/prof/__main__.py to __main__.cpython-36.pyc\n",
      "    byte-compiling /opt/conda/lib/python3.6/site-packages/apex/pyprof/prof/optim.py to optim.cpython-36.pyc\n",
      "    byte-compiling /opt/conda/lib/python3.6/site-packages/apex/pyprof/parse/parse.py to parse.cpython-36.pyc\n",
      "    byte-compiling /opt/conda/lib/python3.6/site-packages/apex/pyprof/parse/db.py to db.cpython-36.pyc\n",
      "    byte-compiling /opt/conda/lib/python3.6/site-packages/apex/pyprof/parse/kernel.py to kernel.cpython-36.pyc\n",
      "    byte-compiling /opt/conda/lib/python3.6/site-packages/apex/pyprof/parse/nvvp.py to nvvp.cpython-36.pyc\n",
      "    byte-compiling /opt/conda/lib/python3.6/site-packages/apex/pyprof/parse/__init__.py to __init__.cpython-36.pyc\n",
      "    byte-compiling /opt/conda/lib/python3.6/site-packages/apex/pyprof/parse/__main__.py to __main__.cpython-36.pyc\n",
      "    byte-compiling /opt/conda/lib/python3.6/site-packages/apex/pyprof/__init__.py to __init__.cpython-36.pyc\n",
      "    byte-compiling /opt/conda/lib/python3.6/site-packages/apex/normalization/fused_layer_norm.py to fused_layer_norm.cpython-36.pyc\n",
      "    byte-compiling /opt/conda/lib/python3.6/site-packages/apex/normalization/__init__.py to __init__.cpython-36.pyc\n",
      "    byte-compiling /opt/conda/lib/python3.6/site-packages/apex/parallel/optimized_sync_batchnorm_kernel.py to optimized_sync_batchnorm_kernel.cpython-36.pyc\n",
      "    byte-compiling /opt/conda/lib/python3.6/site-packages/apex/parallel/optimized_sync_batchnorm.py to optimized_sync_batchnorm.cpython-36.pyc\n",
      "    byte-compiling /opt/conda/lib/python3.6/site-packages/apex/parallel/distributed.py to distributed.cpython-36.pyc\n",
      "    byte-compiling /opt/conda/lib/python3.6/site-packages/apex/parallel/sync_batchnorm.py to sync_batchnorm.cpython-36.pyc\n",
      "    byte-compiling /opt/conda/lib/python3.6/site-packages/apex/parallel/sync_batchnorm_kernel.py to sync_batchnorm_kernel.cpython-36.pyc\n",
      "    byte-compiling /opt/conda/lib/python3.6/site-packages/apex/parallel/multiproc.py to multiproc.cpython-36.pyc\n",
      "    byte-compiling /opt/conda/lib/python3.6/site-packages/apex/parallel/LARC.py to LARC.cpython-36.pyc\n",
      "    byte-compiling /opt/conda/lib/python3.6/site-packages/apex/parallel/__init__.py to __init__.cpython-36.pyc\n",
      "    byte-compiling /opt/conda/lib/python3.6/site-packages/apex/RNN/models.py to models.cpython-36.pyc\n",
      "    byte-compiling /opt/conda/lib/python3.6/site-packages/apex/RNN/cells.py to cells.cpython-36.pyc\n",
      "    byte-compiling /opt/conda/lib/python3.6/site-packages/apex/RNN/RNNBackend.py to RNNBackend.cpython-36.pyc\n",
      "    byte-compiling /opt/conda/lib/python3.6/site-packages/apex/RNN/__init__.py to __init__.cpython-36.pyc\n",
      "    byte-compiling /opt/conda/lib/python3.6/site-packages/apex/multi_tensor_apply/__init__.py to __init__.cpython-36.pyc\n",
      "    byte-compiling /opt/conda/lib/python3.6/site-packages/apex/multi_tensor_apply/multi_tensor_apply.py to multi_tensor_apply.cpython-36.pyc\n",
      "    byte-compiling /opt/conda/lib/python3.6/site-packages/apex/amp/compat.py to compat.cpython-36.pyc\n",
      "    byte-compiling /opt/conda/lib/python3.6/site-packages/apex/amp/amp.py to amp.cpython-36.pyc\n",
      "    byte-compiling /opt/conda/lib/python3.6/site-packages/apex/amp/_amp_state.py to _amp_state.cpython-36.pyc\n",
      "    byte-compiling /opt/conda/lib/python3.6/site-packages/apex/amp/handle.py to handle.cpython-36.pyc\n",
      "    byte-compiling /opt/conda/lib/python3.6/site-packages/apex/amp/_process_optimizer.py to _process_optimizer.cpython-36.pyc\n",
      "    byte-compiling /opt/conda/lib/python3.6/site-packages/apex/amp/rnn_compat.py to rnn_compat.cpython-36.pyc\n",
      "    byte-compiling /opt/conda/lib/python3.6/site-packages/apex/amp/frontend.py to frontend.cpython-36.pyc\n",
      "    byte-compiling /opt/conda/lib/python3.6/site-packages/apex/amp/_initialize.py to _initialize.cpython-36.pyc\n",
      "    byte-compiling /opt/conda/lib/python3.6/site-packages/apex/amp/opt.py to opt.cpython-36.pyc\n",
      "    byte-compiling /opt/conda/lib/python3.6/site-packages/apex/amp/scaler.py to scaler.cpython-36.pyc\n",
      "    byte-compiling /opt/conda/lib/python3.6/site-packages/apex/amp/__init__.py to __init__.cpython-36.pyc\n",
      "    byte-compiling /opt/conda/lib/python3.6/site-packages/apex/amp/utils.py to utils.cpython-36.pyc\n",
      "    byte-compiling /opt/conda/lib/python3.6/site-packages/apex/amp/lists/torch_overrides.py to torch_overrides.cpython-36.pyc\n",
      "    byte-compiling /opt/conda/lib/python3.6/site-packages/apex/amp/lists/functional_overrides.py to functional_overrides.cpython-36.pyc\n",
      "    byte-compiling /opt/conda/lib/python3.6/site-packages/apex/amp/lists/__init__.py to __init__.cpython-36.pyc\n",
      "    byte-compiling /opt/conda/lib/python3.6/site-packages/apex/amp/lists/tensor_overrides.py to tensor_overrides.cpython-36.pyc\n",
      "    byte-compiling /opt/conda/lib/python3.6/site-packages/apex/amp/wrap.py to wrap.cpython-36.pyc\n",
      "    byte-compiling /opt/conda/lib/python3.6/site-packages/apex/amp/__version__.py to __version__.cpython-36.pyc\n",
      "    byte-compiling /opt/conda/lib/python3.6/site-packages/apex/fp16_utils/loss_scaler.py to loss_scaler.cpython-36.pyc\n",
      "    byte-compiling /opt/conda/lib/python3.6/site-packages/apex/fp16_utils/fp16util.py to fp16util.cpython-36.pyc\n",
      "    byte-compiling /opt/conda/lib/python3.6/site-packages/apex/fp16_utils/__init__.py to __init__.cpython-36.pyc\n",
      "    byte-compiling /opt/conda/lib/python3.6/site-packages/apex/fp16_utils/fp16_optimizer.py to fp16_optimizer.cpython-36.pyc\n",
      "    byte-compiling /opt/conda/lib/python3.6/site-packages/apex/__init__.py to __init__.cpython-36.pyc\n",
      "    byte-compiling /opt/conda/lib/python3.6/site-packages/apex/optimizers/fused_adam.py to fused_adam.cpython-36.pyc\n",
      "    byte-compiling /opt/conda/lib/python3.6/site-packages/apex/optimizers/__init__.py to __init__.cpython-36.pyc\n",
      "    byte-compiling /opt/conda/lib/python3.6/site-packages/apex/optimizers/fp16_optimizer.py to fp16_optimizer.cpython-36.pyc\n",
      "    byte-compiling /opt/conda/lib/python3.6/site-packages/apex/reparameterization/weight_norm.py to weight_norm.cpython-36.pyc\n",
      "    byte-compiling /opt/conda/lib/python3.6/site-packages/apex/reparameterization/reparameterization.py to reparameterization.cpython-36.pyc\n",
      "    byte-compiling /opt/conda/lib/python3.6/site-packages/apex/reparameterization/__init__.py to __init__.cpython-36.pyc\n",
      "    running install_egg_info\n",
      "    running egg_info\n",
      "    creating apex.egg-info\n",
      "    writing apex.egg-info/PKG-INFO\n",
      "    writing dependency_links to apex.egg-info/dependency_links.txt\n",
      "    writing top-level names to apex.egg-info/top_level.txt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    writing manifest file 'apex.egg-info/SOURCES.txt'\n",
      "    reading manifest file 'apex.egg-info/SOURCES.txt'\n",
      "    writing manifest file 'apex.egg-info/SOURCES.txt'\n",
      "    Copying apex.egg-info to /opt/conda/lib/python3.6/site-packages/apex-0.1-py3.6.egg-info\n",
      "    running install_scripts\n",
      "    writing list of installed files to '/tmp/pip-record-_83x469b/install-record.txt'\n",
      "  Running setup.py install for apex ... \u001b[?25l\u001b[?25hdone\n",
      "  Removing source in /tmp/pip-req-build-evab29kl\n",
      "Successfully installed apex-0.1\n",
      "Cleaning up...\n",
      "Removed build tracker '/tmp/pip-req-tracker-niaxbcd1'\n",
      "1 location(s) to search for versions of pip:\n",
      "* https://pypi.org/simple/pip/\n",
      "Getting page https://pypi.org/simple/pip/\n",
      "Found index url https://pypi.org/simple\n",
      "Getting credentials from keyring for https://pypi.org/simple\n",
      "Getting credentials from keyring for pypi.org\n",
      "Starting new HTTPS connection (1): pypi.org:443\n",
      "Could not fetch URL https://pypi.org/simple/pip/: connection error: HTTPSConnectionPool(host='pypi.org', port=443): Max retries exceeded with url: /simple/pip/ (Caused by NewConnectionError('<pip._vendor.urllib3.connection.VerifiedHTTPSConnection object at 0x7fcce93053c8>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution',)) - skipping\n",
      "Given no hashes to check 0 links for project 'pip': discarding no candidates\n"
     ]
    }
   ],
   "source": [
    "# Installing Nvidia Apex\n",
    "! pip install -v --no-cache-dir --global-option=\"--cpp_ext\" --global-option=\"--cuda_ext\" ../input/nvidia-apex-15aug2019/apex-master/apex-master"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm import tqdm\n",
    "from apex import amp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading The Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Original data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = Path(\"/kaggle/models/\")\n",
    "sample_path = Path(\"/kaggle/input/aptos2019-blindness-detection/train_images/\")\n",
    "input_path = Path(\"/kaggle/input/aptos2019-blindness-detection/\")\n",
    "test_path = Path(\"/kaggle/input/aptos2019-blindness-detection/test_images/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3662"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.read_csv(input_path/'train.csv')\n",
    "test_df = pd.read_csv(input_path/'test.csv')\n",
    "\n",
    "len(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "type_percents = train_df[\"diagnosis\"].value_counts(normalize=True)\n",
    "\n",
    "sns.barplot(x=type_percents.index, y=type_percents.values*100)\\\n",
    "          .set(xlabel=\"severity of diabetic retinopathy\", ylabel='Percent (%)')    \n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-folds Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Divide the training dataset into 5 folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# skf = StratifiedKFold(n_splits=5)\n",
    "# skf.get_n_splits()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SIZE = 256"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ben's transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaleRadius(image, scale):\n",
    "    x = image[int(image.shape[0]/2),:,:].sum(1)\n",
    "    r = (x>x.mean()/10).sum()/2\n",
    "    s = scale*1.0/r\n",
    "    return cv2.resize(image, (0,0), fx=s, fy=s)\n",
    "\n",
    "def writeToNewFile(image, path, scale):\n",
    "    parent = path.parent\n",
    "    file_name = path.name\n",
    "    parent_name = parent.name+\"_\"+str(scale)\n",
    "    new_path = str(parent.parent/parent_name/file_name)\n",
    "    cv2.imwrite(new_path,image)\n",
    "    \n",
    "scale = 300\n",
    "def load_ben_color(path, scale):\n",
    "    image = cv2.imread(str(path))\n",
    "    a=scaleRadius(image, scale)\n",
    "    b=np.zeros(a.shape)\n",
    "    cv2.circle(b,(int(a.shape[1]/2),int(a.shape[0]/2)),int(scale*0.9),(1,1,1),-1,8,0)\n",
    "    aa=cv2.add(cv2.multiply(cv2.addWeighted(a,4,cv2.GaussianBlur(a,(0,0),scale/30),-4,128), np.asarray(b, a.dtype)), np.asarray(128*(1.0-b), a.dtype))\n",
    "    # aa=a*np.asarray(b, a.dtype)\n",
    "    # writeToNewFile(aa, path, scale)\n",
    "    return aa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i, image_id in enumerate(tqdm(train_df['id_code'])):\n",
    "#     load_ben_color(sample_path/(image_id+\".png\"), scale)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading The Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset, DataLoader, SubsetRandomSampler\n",
    "from torchvision import transforms\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "package_path = '/kaggle/input/efficientnet/efficientnet-pytorch/EfficientNet-PyTorch/'\n",
    "sys.path.append(package_path)\n",
    "from efficientnet_pytorch import EfficientNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_image_size = 512 # EfficientNet.get_image_size('efficientnet-b2')\n",
    "batch_size = 32\n",
    "classes = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Custom Dataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, sample_path, data, transform=None, category=-1, suffix='.png', is_test=False):\n",
    "        self.category = category\n",
    "        self.data = pd.DataFrame(data).reset_index()\n",
    "        self.transform = transform\n",
    "        self.sample_path = sample_path\n",
    "        self.suffix = suffix\n",
    "        self.is_test = is_test\n",
    "        self.O_ins = None\n",
    "        \n",
    "        if self.is_test==False:\n",
    "            self.targets = self.data.diagnosis+1\n",
    "        \n",
    "    def __len__(self):\n",
    "        return(len(self.data))\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.sample_path/(self.data.iloc[idx, :]['id_code'] + self.suffix)\n",
    "        image = load_ben_color(img_name, 360) # cv2.imread(str(img_name))\n",
    "        image = image[:,:,::-1]\n",
    "        if self.is_test==True:\n",
    "            if self.transform:\n",
    "                transformed_image = self.transform(image)\n",
    "                return transformed_image\n",
    "            return image\n",
    "        \n",
    "        label = self.targets[idx]\n",
    "        if self.transform:\n",
    "            transformed_image = self.transform(image)\n",
    "            if self.O_ins is not None:\n",
    "                return transformed_image, label, self.O_ins[idx, :]\n",
    "            return transformed_image, label\n",
    "        \n",
    "        return image, label\n",
    "    \n",
    "def cycle(iterable):\n",
    "    while True:\n",
    "        for x in iterable:\n",
    "            yield x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = [0.4528, 0.3897, 0.2204]\n",
    "std = [0.5857, 0.6561, 0.9010]\n",
    "\n",
    "train_transforms = transforms.Compose([transforms.ToPILImage(),\n",
    "                                       transforms.CenterCrop(512),\n",
    "                                       transforms.RandomHorizontalFlip(),\n",
    "                                       transforms.RandomVerticalFlip(),\n",
    "                                       # transforms.Resize(training_image_size),\n",
    "                                       transforms.ToTensor(), \n",
    "                                       transforms.Normalize(mean, std)])\n",
    "\n",
    "dataset = ImageDataset(sample_path, train_df, transform=train_transforms)\n",
    "train_labels = dataset.targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "channels = 3\n",
    "z_dim = 128\n",
    "v_n_dim = 64\n",
    "embedding_z_dim = 64\n",
    "size_mult = 32\n",
    "\n",
    "epochs = 3\n",
    "\n",
    "is_trained = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Identity(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Identity, self).__init__()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class D(nn.Module):\n",
    "    def __init__(self):\n",
    "        global z_dim\n",
    "        super(D, self).__init__()\n",
    "        self.model = EfficientNet.from_name('efficientnet-b5')\n",
    "        self.model.load_state_dict(torch.load('../input/efficientnet-pytorch/efficientnet-b5-586e6cc6.pth'))\n",
    "        in_features = self.model._fc.in_features\n",
    "        z_dim = in_features\n",
    "        out_features = z_dim\n",
    "        \n",
    "        self.model._fc = Identity()\n",
    "        self.classifier = nn.Sequential(nn.Linear(out_features, classes*2), \n",
    "                                        nn.LogSoftmax(dim=1))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        (x_, is_training_on_test) = x\n",
    "        out1 = self.model(x_)\n",
    "        out2 = self.classifier(out1)\n",
    "        if is_training_on_test==True:\n",
    "            return (out1, out2)\n",
    "        \n",
    "        return out2\n",
    "    \n",
    "    def print_grad(self):\n",
    "        print(self.model.classifier[0].weight.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oversampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class O(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(O, self).__init__()\n",
    "        self.encode = nn.Sequential(\n",
    "            nn.Linear(z_dim, 128, bias=False),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, embedding_z_dim, bias=False),\n",
    "            nn.Tanh())\n",
    "        self.decode = nn.Sequential(\n",
    "            nn.Linear(embedding_z_dim, 128, bias=False),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, classes*2, bias=False),\n",
    "            nn.LogSoftmax(dim=1))\n",
    "        \n",
    "    def forward(self, x, is_embed=False):\n",
    "        if is_embed==False:\n",
    "            output = self.encode(x)\n",
    "        else:\n",
    "            output = self.decode(x)\n",
    "        return output\n",
    "    def print_grad_e(self):\n",
    "        print(self.encode[0].weight.grad)\n",
    "    def print_grad_d(self):\n",
    "        print(self.decode[0].weight.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper conv function\n",
    "def conv(in_channels, out_channels, kernel_size, stride=4, padding=3):\n",
    "    \"\"\"Creates a convolutional layer, with optional batch normalization.\n",
    "    \"\"\"\n",
    "    layers = []\n",
    "    conv_layer = nn.Conv2d(in_channels=in_channels, out_channels=out_channels, \n",
    "                           kernel_size=kernel_size, stride=stride, padding=padding, bias=False)\n",
    "    layers.append(conv_layer)\n",
    "    layers.append(nn.BatchNorm2d(out_channels))\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "# residual block class\n",
    "class ResidualBlock(nn.Module):\n",
    "    \"\"\"Defines a residual block.\n",
    "       This adds an input x to a convolutional layer (applied to x) with the same size input and output.\n",
    "       These blocks allow a model to learn an effective transformation from one domain to another.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        \n",
    "        # define two convolutional layers + batch normalization that will act as our residual function, F(x)\n",
    "        # layers should have the same shape input as output; I suggest a kernel_size of 3\n",
    "        \n",
    "        self.conv_layer1 = conv(in_channels=in_channels, out_channels=out_channels, \n",
    "                                kernel_size=3, stride=1, padding=1)\n",
    "        \n",
    "        self.conv_layer2 = conv(in_channels=out_channels, out_channels=out_channels, \n",
    "                               kernel_size=3, stride=1, padding=1)\n",
    "        \n",
    "        self.is_not_consist = (in_channels != out_channels)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # apply a ReLu activation the outputs of the first layer\n",
    "        # return a summed output, x + resnet_block(x)\n",
    "        out_1 = F.relu(self.conv_layer1(x))\n",
    "        if self.is_not_consist:\n",
    "            return out_1\n",
    "        \n",
    "        out_2 = x + self.conv_layer2(out_1)\n",
    "        return out_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class G(nn.Module):\n",
    "    \n",
    "    def __init__(self, n_res_blocks=5):\n",
    "        super(G, self).__init__()\n",
    "\n",
    "        self.sh_fc = nn.Linear(embedding_z_dim+v_n_dim, 8*8*size_mult*4)\n",
    "        \n",
    "        # 1. Define the encoder part of the generator\n",
    "        \n",
    "        # initial convolutional layer given, below\n",
    "        self.enc = nn.Sequential(conv(3, size_mult, 8), # x, y = 128, depth 32\n",
    "                                 nn.ReLU(),\n",
    "                                 conv(size_mult, size_mult*2, 8), # x, y = 32, depth 64\n",
    "                                 nn.ReLU(),\n",
    "                                 conv(size_mult*2, size_mult*4, 8), # x, y = 8, depth 128\n",
    "                                 nn.ReLU(),\n",
    "                                )\n",
    "        \n",
    "        # 2. Define the resnet part of the generator\n",
    "        # Residual blocks\n",
    "        res_layers = [ResidualBlock(size_mult*8, size_mult*4)]\n",
    "        for layer in range(n_res_blocks):\n",
    "            res_layers.append(ResidualBlock(size_mult*4, size_mult*4))\n",
    "        # use sequential to create these layers\n",
    "        self.res_blocks = nn.Sequential(*res_layers)\n",
    "        \n",
    "        self.dec = nn.Sequential(\n",
    "            nn.ConvTranspose2d(size_mult*4, size_mult*2, 8, 4, 2, bias=False),\n",
    "            nn.BatchNorm2d(size_mult*2),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(size_mult*2, size_mult, 8, 4, 2, bias=False),\n",
    "            nn.BatchNorm2d(size_mult),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(size_mult, channels, 8, 4, 2, bias=False))\n",
    "        \n",
    "    def forward(self, atup):\n",
    "        (ne_vec, x) = atup\n",
    "        \n",
    "        ne_vec = self.sh_fc(ne_vec).view(-1, size_mult*4, 8, 8)\n",
    "        \n",
    "        out = self.enc(x)\n",
    "        out = torch.cat([ne_vec, out], 1)\n",
    "        out = self.res_blocks(out)\n",
    "        out = self.dec(out)\n",
    "        out = torch.tanh(out)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initializing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.\n",
      "\n",
      "Defaults for this optimization level are:\n",
      "enabled                : True\n",
      "opt_level              : O1\n",
      "cast_model_type        : None\n",
      "patch_torch_functions  : True\n",
      "keep_batchnorm_fp32    : None\n",
      "master_weights         : None\n",
      "loss_scale             : dynamic\n",
      "Processing user overrides (additional kwargs that are not None)...\n",
      "After processing overrides, optimization options are:\n",
      "enabled                : True\n",
      "opt_level              : O1\n",
      "cast_model_type        : None\n",
      "patch_torch_functions  : True\n",
      "keep_batchnorm_fp32    : None\n",
      "master_weights         : None\n",
      "loss_scale             : dynamic\n",
      "Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.\n",
      "\n",
      "Defaults for this optimization level are:\n",
      "enabled                : True\n",
      "opt_level              : O1\n",
      "cast_model_type        : None\n",
      "patch_torch_functions  : True\n",
      "keep_batchnorm_fp32    : None\n",
      "master_weights         : None\n",
      "loss_scale             : dynamic\n",
      "Processing user overrides (additional kwargs that are not None)...\n",
      "After processing overrides, optimization options are:\n",
      "enabled                : True\n",
      "opt_level              : O1\n",
      "cast_model_type        : None\n",
      "patch_torch_functions  : True\n",
      "keep_batchnorm_fp32    : None\n",
      "master_weights         : None\n",
      "loss_scale             : dynamic\n",
      "Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.\n",
      "\n",
      "Defaults for this optimization level are:\n",
      "enabled                : True\n",
      "opt_level              : O1\n",
      "cast_model_type        : None\n",
      "patch_torch_functions  : True\n",
      "keep_batchnorm_fp32    : None\n",
      "master_weights         : None\n",
      "loss_scale             : dynamic\n",
      "Processing user overrides (additional kwargs that are not None)...\n",
      "After processing overrides, optimization options are:\n",
      "enabled                : True\n",
      "opt_level              : O1\n",
      "cast_model_type        : None\n",
      "patch_torch_functions  : True\n",
      "keep_batchnorm_fp32    : None\n",
      "master_weights         : None\n",
      "loss_scale             : dynamic\n"
     ]
    }
   ],
   "source": [
    "def weights_init(m):\n",
    "    if type(m) == nn.Linear or type(m) == nn.ConvTranspose2d:\n",
    "        torch.nn.init.xavier_uniform_(m.weight)\n",
    "        if m.bias is not None:\n",
    "            m.bias.data.fill_(0.01)\n",
    "\n",
    "# Use GPU if it's available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "D = D()\n",
    "if os.path.exists(str(D_model_path)):\n",
    "    D.load_state_dict(torch.load(D_model_path))\n",
    "    is_trained = True\n",
    "    \n",
    "O = O()\n",
    "O.apply(weights_init)\n",
    "G = G()\n",
    "G.apply(weights_init)\n",
    "\n",
    "criterionD = nn.NLLLoss()\n",
    "criterionO = nn.MSELoss()\n",
    "criterionG = nn.NLLLoss()\n",
    "\n",
    "optimD = optim.Adam(D.parameters(), lr=0.0005)\n",
    "optimO = optim.Adam(O.parameters())\n",
    "optimG = optim.Adam(G.parameters(), lr=0.0001)\n",
    "\n",
    "D.to(device)\n",
    "O.to(device)\n",
    "G.to(device)\n",
    "\n",
    "D, optimD = amp.initialize(D, optimD, opt_level=\"O1\")\n",
    "O, optimO = amp.initialize(O, optimO, opt_level=\"O1\")\n",
    "G, optimG = amp.initialize(G, optimG, opt_level=\"O1\")\n",
    "\n",
    "accumuated_rnd = 4\n",
    "accumuated_num = int(batch_size/accumuated_rnd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_cb_class = 2\n",
    "mark_mat = torch.FloatTensor(accumuated_num, classes+1, n_cb_class).to(device)\n",
    "mark_mat = Variable(mark_mat)\n",
    "def combine_lsm(lsm_layer, combine_indices, is_g=False):\n",
    "    global mark_mat\n",
    "    n_batch, n_class = lsm_layer.shape\n",
    "    n_batch, n_cb_class = combine_indices.shape\n",
    "    sm_layer = torch.exp(lsm_layer)\n",
    "    \n",
    "    for i in range(n_batch):\n",
    "        eye = torch.eye(n_class)\n",
    "        for j in range(1, n_cb_class):\n",
    "            cb_index = combine_indices[i, j]\n",
    "            eye[:, 0] = eye[:, 0] + eye[:, cb_index]\n",
    "            if cb_index < (n_cb_class-1):\n",
    "                eye = torch.cat([eye[:, :cb_index], eye[:, cb_index+1:]], dim=1)\n",
    "            else:\n",
    "                eye = eye[:,:-1]\n",
    "        mark_mat[i, :, :] = torch.cat([eye[:, 0].view(-1, 1), 1-eye[:, 0].view(-1, 1)], dim=1)\n",
    "    sm_layer = torch.einsum('bj,bjk->bk', sm_layer, mark_mat)\n",
    "    return torch.log(sm_layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 of KFold 3\n",
      "Epoch 1/3.. Dloss: 2.420.. Gloss: 0.000.. \n",
      "Epoch 1/3.. Dloss: 0.985.. Gloss: 0.000.. \n",
      "Epoch 1/3.. Dloss: 0.619.. Gloss: 0.000.. \n",
      "Epoch 1/3.. Dloss: 0.474.. Gloss: 0.000.. \n",
      "Epoch 1/3.. Dloss: 0.445.. Gloss: 0.000.. \n",
      "Epoch 1/3.. Dloss: 0.607.. Gloss: 0.000.. \n",
      "Epoch 1/3.. Dloss: 0.471.. Gloss: 0.000.. \n",
      "Epoch 1/3.. Dloss: 0.428.. Gloss: 0.000.. \n",
      "Epoch 1/3.. Dloss: 0.471.. Gloss: 0.000.. \n",
      "Epoch 1/3.. Dloss: 0.450.. Gloss: 0.000.. \n",
      "Epoch 1/3.. Dloss: 0.460.. Gloss: 0.000.. \n",
      "Epoch 1/3.. Dloss: 0.426.. Gloss: 0.000.. \n",
      "Epoch 1/3.. Dloss: 0.385.. Gloss: 0.000.. \n",
      "Epoch 1/3.. Dloss: 0.557.. Gloss: 0.000.. \n",
      "Epoch 1/3.. Dloss: 0.343.. Gloss: 0.000.. \n",
      "Validation loss: 0.357.. Validation accuracy: 0.873\n",
      "1 of KFold 3\n",
      "Epoch 1/3.. Dloss: 0.410.. Gloss: 0.000.. \n",
      "Epoch 1/3.. Dloss: 0.399.. Gloss: 0.000.. \n",
      "Epoch 1/3.. Dloss: 0.343.. Gloss: 0.000.. \n",
      "Epoch 1/3.. Dloss: 0.343.. Gloss: 0.000.. \n",
      "Epoch 1/3.. Dloss: 0.328.. Gloss: 0.000.. \n",
      "Epoch 1/3.. Dloss: 0.435.. Gloss: 0.000.. \n",
      "Epoch 1/3.. Dloss: 0.286.. Gloss: 0.000.. \n",
      "Epoch 1/3.. Dloss: 0.447.. Gloss: 0.000.. \n",
      "Epoch 1/3.. Dloss: 0.357.. Gloss: 0.000.. \n",
      "Epoch 1/3.. Dloss: 0.364.. Gloss: 0.000.. \n",
      "Epoch 1/3.. Dloss: 0.294.. Gloss: 0.000.. \n",
      "Epoch 1/3.. Dloss: 0.286.. Gloss: 0.000.. \n",
      "Epoch 1/3.. Dloss: 0.335.. Gloss: 0.000.. \n",
      "Epoch 1/3.. Dloss: 0.367.. Gloss: 0.000.. \n",
      "Epoch 1/3.. Dloss: 0.326.. Gloss: 0.000.. \n",
      "Validation loss: 0.298.. Validation accuracy: 0.905\n",
      "2 of KFold 3\n",
      "Epoch 1/3.. Dloss: 0.341.. Gloss: 0.000.. \n",
      "Epoch 1/3.. Dloss: 0.193.. Gloss: 0.000.. \n",
      "Epoch 1/3.. Dloss: 0.367.. Gloss: 0.000.. \n",
      "Epoch 1/3.. Dloss: 0.270.. Gloss: 0.000.. \n",
      "Epoch 1/3.. Dloss: 0.301.. Gloss: 0.000.. \n",
      "Epoch 1/3.. Dloss: 0.361.. Gloss: 0.000.. \n",
      "Epoch 1/3.. Dloss: 0.224.. Gloss: 0.000.. \n",
      "Epoch 1/3.. Dloss: 0.245.. Gloss: 0.000.. \n",
      "Epoch 1/3.. Dloss: 0.310.. Gloss: 0.000.. \n",
      "Epoch 1/3.. Dloss: 0.237.. Gloss: 0.000.. \n",
      "Epoch 1/3.. Dloss: 0.283.. Gloss: 0.000.. \n",
      "Epoch 1/3.. Dloss: 0.334.. Gloss: 0.000.. \n",
      "Epoch 1/3.. Dloss: 0.234.. Gloss: 0.000.. \n",
      "Epoch 1/3.. Dloss: 0.282.. Gloss: 0.000.. \n",
      "Epoch 1/3.. Dloss: 0.348.. Gloss: 0.000.. \n",
      "Validation loss: 0.323.. Validation accuracy: 0.887\n"
     ]
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "skf = StratifiedKFold(n_splits=3)\n",
    "skf.get_n_splits()\n",
    "\n",
    "top_classes = None\n",
    "true_labels = None\n",
    "\n",
    "for fold_ind, (dev_index, val_index) in enumerate(skf.split(dataset, train_labels)):\n",
    "    print('{} of KFold {}'.format(fold_ind, skf.n_splits))\n",
    "    train_sampler = SubsetRandomSampler(dev_index)\n",
    "    valid_sampler = SubsetRandomSampler(val_index)\n",
    "    train_data_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, num_workers=8, \n",
    "                                                    sampler=train_sampler)\n",
    "    valid_data_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, num_workers=8, \n",
    "                                                    sampler=valid_sampler)\n",
    "    \n",
    "    if is_trained == False:\n",
    "        ####\n",
    "        steps = 0\n",
    "        d_running_loss = 0\n",
    "        g_running_loss = 0\n",
    "        print_every = 5\n",
    "\n",
    "        # Train on training data\n",
    "        for epoch in range(epochs):\n",
    "            label_counts = np.zeros((classes, 1))\n",
    "            for inputs, labels in train_data_loader:\n",
    "                steps += 1\n",
    "                # Move input and label tensors to the default device\n",
    "                real_x = inputs\n",
    "                bs = real_x.size(0)\n",
    "                if (bs!=batch_size):\n",
    "                    continue\n",
    "\n",
    "                d_loss=0\n",
    "                g_loss=0\n",
    "                d_loss_accum=0\n",
    "\n",
    "                # D-E1\n",
    "                optimD.zero_grad()\n",
    "                d_loss_accum=0\n",
    "                for batch_num in range(accumuated_rnd):\n",
    "                    logps_d = D((real_x[batch_num*accumuated_num:(batch_num+1)*accumuated_num,:,:,:].to(device), False))\n",
    "                    loss_real = criterionD(logps_d, labels[batch_num*accumuated_num:(batch_num+1)*accumuated_num].to(device))\n",
    "\n",
    "                    with amp.scale_loss(loss_real, optimD) as scaled_loss:\n",
    "                        scaled_loss.backward()\n",
    "\n",
    "                    d_loss_accum += loss_real\n",
    "\n",
    "                optimD.step()\n",
    "                d_loss += d_loss_accum.item() / accumuated_rnd\n",
    "                g_loss += 0\n",
    "\n",
    "\n",
    "                d_running_loss += d_loss\n",
    "                g_running_loss += g_loss\n",
    "\n",
    "                if steps % print_every == 0:\n",
    "                    print(f\"Epoch {epoch+1}/{epochs}.. \"\n",
    "                          f\"Dloss: {d_running_loss/print_every:.3f}.. \"\n",
    "                          f\"Gloss: {g_running_loss/print_every:.3f}.. \")\n",
    "                    d_running_loss = 0\n",
    "                    g_running_loss = 0\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Validation Phase\n",
    "    test_loss = 0\n",
    "    accuracy = 0\n",
    "    D.eval()\n",
    "    with torch.no_grad():\n",
    "        for inputs, val_labels in valid_data_loader:\n",
    "            inputs, val_labels = inputs.to(device), val_labels.to(device)\n",
    "            logps = D((inputs, False))\n",
    "            batch_loss = nn.NLLLoss()(logps, val_labels)\n",
    "\n",
    "            test_loss += batch_loss.item()\n",
    "\n",
    "            # Calculate accuracy\n",
    "            ps = torch.exp(logps)\n",
    "            top_p, top_class = ps.topk(1, dim=1)\n",
    "            \n",
    "            if top_classes is None:\n",
    "                top_classes = top_class\n",
    "                true_labels = val_labels.view(*top_class.shape)\n",
    "            else:\n",
    "                top_classes = torch.cat([top_classes, top_class], dim=0)\n",
    "                true_labels = torch.cat([true_labels, val_labels.view(*top_class.shape)], dim=0)\n",
    "                \n",
    "            equals = top_class == val_labels.view(*top_class.shape)\n",
    "            accuracy += torch.mean(equals.type(torch.FloatTensor)).item()\n",
    "\n",
    "    print(f\"Validation loss: {test_loss/len(valid_data_loader):.3f}.. \"\n",
    "          f\"Validation accuracy: {accuracy/len(valid_data_loader):.3f}\")\n",
    "    D.train()\n",
    "\n",
    "if is_trained == False:\n",
    "    # save the model parameters\n",
    "    D_model_path = os.path.join(model_path, 'D.pkl')\n",
    "    torch.save(D.model.state_dict(), D_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_path = Path(\"/kaggle/input/aptos2019-blindness-detection/test_images/\")\n",
    "test_dataset = ImageDataset(test_path, test_df, transform=train_transforms, is_test=True)\n",
    "test_data_loader = torch.utils.data.DataLoader(test_dataset, batch_size=2, num_workers=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "964it [04:09,  8.44it/s]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "submit = pd.read_csv(input_path/'sample_submission.csv')\n",
    "ans = []\n",
    "label_counts = np.zeros((classes, 1))\n",
    "for i, inputs in tqdm(enumerate(test_data_loader)):\n",
    "    logps = D((inputs.to(device), False))\n",
    "    ps = torch.exp(logps[:,:classes]).detach().cpu().numpy()\n",
    "    top_cls = np.argmax(ps, axis=1)\n",
    "    for ii, label in enumerate(top_cls):\n",
    "        label_counts[label] = label_counts[label] + 1\n",
    "    top_cls[top_cls<0] = 0\n",
    "    ans.extend(top_cls.tolist())\n",
    "    \n",
    "sns.barplot(x=np.array(list(range(classes))), y=label_counts.squeeze())\\\n",
    "      .set(xlabel=\"severity of diabetic retinopathy\", ylabel='Percent (%)')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/opt/conda/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/opt/conda/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/opt/conda/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/opt/conda/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/opt/conda/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/opt/conda/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "submit['diagnosis'] = ans\n",
    "submit.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
