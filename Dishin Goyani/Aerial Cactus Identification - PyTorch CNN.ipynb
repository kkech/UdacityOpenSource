{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "%matplotlib inline\n",
    "import os\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import image as img\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "import torch.optim as optim\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "# import os\n",
    "# for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "#     for filename in filenames:\n",
    "#         print(os.path.join(dirname, filename))\n",
    "\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set all dir\n",
    "data_dir = \"../input/aerial-cactus-identification\"            # main dir\n",
    "train_dir = data_dir + \"/train/train\"                         # train images folder\n",
    "test_dir = data_dir + \"/test/test\"                            # test images folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>has_cactus</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0004be2cfeaba1c0361d39e2b000257b.jpg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000c8a36845c0208e833c79c1bffedd1.jpg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000d1e9a533f62e55c289303b072733d.jpg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0011485b40695e9138e92d0b3fb55128.jpg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0014d7a11e90b62848904c1418fc8cf2.jpg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     id  has_cactus\n",
       "0  0004be2cfeaba1c0361d39e2b000257b.jpg           1\n",
       "1  000c8a36845c0208e833c79c1bffedd1.jpg           1\n",
       "2  000d1e9a533f62e55c289303b072733d.jpg           1\n",
       "3  0011485b40695e9138e92d0b3fb55128.jpg           1\n",
       "4  0014d7a11e90b62848904c1418fc8cf2.jpg           1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read train csv\n",
    "data = pd.read_csv(data_dir + \"/train.csv\")\n",
    "data.head()                                                   # test csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    13136\n",
       "0     4364\n",
       "Name: has_cactus, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check ratio of images with cactus and without cactus\n",
    "data[\"has_cactus\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Number of Images has cactus {1} and without cactus')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# visualize ratio of images with cactus and without cactus\n",
    "plt.hist(data.has_cactus,align='mid',bins=4)\n",
    "plt.xlim(-1, 2)\n",
    "plt.xlabel(\"0: has not cactus and 1: has cactus\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"Number of Images has cactus {1} and without cactus\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inherit Dataset and make changes as we access images through index latter\n",
    "class cactData(Dataset):\n",
    "    def __init__(self, split_data, data_root = './', transform = None):\n",
    "        super().__init__()\n",
    "        self.df = split_data.values                           # set dataframe\n",
    "        self.data_root = data_root                            # images path\n",
    "        self.transform = transform                            # transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)                                   # return total length of dataframe \n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        img_name,label = self.df[index]                       # get image id and label from csv\n",
    "        img_path = os.path.join(self.data_root, img_name)     # set image path\n",
    "        image = img.imread(img_path)                          # read image from given image path\n",
    "        if self.transform is not None:                        # transform image if transform available \n",
    "            image = self.transform(image)\n",
    "        return image, label                                   # return image and image label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data into train and validation set; so training data = 80% and validation data = 20%\n",
    "train, valid = train_test_split(data, stratify=data.has_cactus, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define transforms\n",
    "train_transf = transforms.Compose([transforms.ToPILImage(),\n",
    "                                   transforms.ToTensor()])\n",
    "\n",
    "valid_transf = transforms.Compose([transforms.ToPILImage(),\n",
    "                                  transforms.ToTensor()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  batch size \n",
    "batch_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set cactData cons\n",
    "train_data = cactData(train, train_dir, train_transf)\n",
    "valid_data = cactData(valid, train_dir, valid_transf)\n",
    "\n",
    "# get training and validation data\n",
    "train_loader = DataLoader(dataset=train_data, batch_size=batch_size, shuffle=True)\n",
    "valid_loader = DataLoader(dataset=valid_data, batch_size=batch_size//2, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[0.3569, 0.3608, 0.3725,  ..., 0.4667, 0.5216, 0.5333],\n",
       "          [0.3843, 0.3882, 0.4000,  ..., 0.5686, 0.6157, 0.5137],\n",
       "          [0.3961, 0.4000, 0.4000,  ..., 0.6000, 0.6902, 0.5373],\n",
       "          ...,\n",
       "          [0.4784, 0.4471, 0.3804,  ..., 0.4039, 0.4000, 0.3412],\n",
       "          [0.4863, 0.4353, 0.3765,  ..., 0.3843, 0.4275, 0.3686],\n",
       "          [0.4824, 0.4353, 0.3922,  ..., 0.4196, 0.4745, 0.4353]],\n",
       " \n",
       "         [[0.3098, 0.3137, 0.3255,  ..., 0.3725, 0.4275, 0.4392],\n",
       "          [0.3373, 0.3412, 0.3529,  ..., 0.4745, 0.5216, 0.4196],\n",
       "          [0.3490, 0.3529, 0.3529,  ..., 0.5176, 0.5961, 0.4431],\n",
       "          ...,\n",
       "          [0.4275, 0.3961, 0.3294,  ..., 0.3451, 0.3412, 0.2824],\n",
       "          [0.4353, 0.3843, 0.3255,  ..., 0.3255, 0.3686, 0.3098],\n",
       "          [0.4314, 0.3843, 0.3412,  ..., 0.3608, 0.4157, 0.3765]],\n",
       " \n",
       "         [[0.2627, 0.2667, 0.2784,  ..., 0.3569, 0.4196, 0.4314],\n",
       "          [0.2902, 0.2941, 0.3059,  ..., 0.4588, 0.5137, 0.4118],\n",
       "          [0.3020, 0.3059, 0.3059,  ..., 0.4980, 0.5882, 0.4353],\n",
       "          ...,\n",
       "          [0.3961, 0.3647, 0.2980,  ..., 0.3176, 0.3137, 0.2549],\n",
       "          [0.4039, 0.3529, 0.2941,  ..., 0.2980, 0.3412, 0.2824],\n",
       "          [0.4000, 0.3529, 0.3098,  ..., 0.3333, 0.3882, 0.3490]]]), tensor(0))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test data\n",
    "images, labels = next(iter(train_loader))\n",
    "images[0], labels[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define CNN model\n",
    "class Net(nn.Module): \n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        # Convolutional Neural Networks\n",
    "        self.conv1 = nn.Conv2d(3, 10, 3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(10, 20, 3, padding=1)\n",
    "        \n",
    "        # fully connected network\n",
    "        self.fc1 = nn.Linear(1280, 640)\n",
    "        self.fc2 = nn.Linear(640, 2)\n",
    "        \n",
    "        # pooling and dropout layer\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.dropout = nn.Dropout(0.25)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        \n",
    "        # reshape to fit into fully connected net\n",
    "        x = x.view(x.shape[0],-1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# set hyper parameters\n",
    "num_epochs = 20\n",
    "learning_rate = 0.01\n",
    "momentum = 0.9\n",
    "\n",
    "# check if GPU available\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (fc1): Linear(in_features=1280, out_features=640, bias=True)\n",
      "  (fc2): Linear(in_features=640, out_features=2, bias=True)\n",
      "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (dropout): Dropout(p=0.25, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# model architecture  \n",
    "model = Net().to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify loss function (categorical cross-entropy)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# SGD optimizer\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate,momentum=momentum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 0.506759 \tValidation Loss: 0.429532\n",
      "Validation loss decreased (inf --> 0.429532).  Saving model ...\n",
      "Epoch: 2 \tTraining Loss: 0.271991 \tValidation Loss: 0.179413\n",
      "Validation loss decreased (0.429532 --> 0.179413).  Saving model ...\n",
      "Epoch: 3 \tTraining Loss: 0.184310 \tValidation Loss: 0.567914\n",
      "Epoch: 4 \tTraining Loss: 0.194244 \tValidation Loss: 0.157378\n",
      "Validation loss decreased (0.179413 --> 0.157378).  Saving model ...\n",
      "Epoch: 5 \tTraining Loss: 0.136386 \tValidation Loss: 0.112869\n",
      "Validation loss decreased (0.157378 --> 0.112869).  Saving model ...\n",
      "Epoch: 6 \tTraining Loss: 0.127845 \tValidation Loss: 0.111538\n",
      "Validation loss decreased (0.112869 --> 0.111538).  Saving model ...\n",
      "Epoch: 7 \tTraining Loss: 0.111014 \tValidation Loss: 0.111654\n",
      "Epoch: 8 \tTraining Loss: 0.111486 \tValidation Loss: 0.157831\n",
      "Epoch: 9 \tTraining Loss: 0.105680 \tValidation Loss: 0.091259\n",
      "Validation loss decreased (0.111538 --> 0.091259).  Saving model ...\n",
      "Epoch: 10 \tTraining Loss: 0.094604 \tValidation Loss: 0.089342\n",
      "Validation loss decreased (0.091259 --> 0.089342).  Saving model ...\n",
      "Epoch: 11 \tTraining Loss: 0.096200 \tValidation Loss: 0.087086\n",
      "Validation loss decreased (0.089342 --> 0.087086).  Saving model ...\n",
      "Epoch: 12 \tTraining Loss: 0.094699 \tValidation Loss: 0.124432\n",
      "Epoch: 13 \tTraining Loss: 0.094026 \tValidation Loss: 0.094645\n",
      "Epoch: 14 \tTraining Loss: 0.087855 \tValidation Loss: 0.080670\n",
      "Validation loss decreased (0.087086 --> 0.080670).  Saving model ...\n",
      "Epoch: 15 \tTraining Loss: 0.089995 \tValidation Loss: 0.089475\n",
      "Epoch: 16 \tTraining Loss: 0.081357 \tValidation Loss: 0.079744\n",
      "Validation loss decreased (0.080670 --> 0.079744).  Saving model ...\n",
      "Epoch: 17 \tTraining Loss: 0.083342 \tValidation Loss: 0.080135\n",
      "Epoch: 18 \tTraining Loss: 0.076224 \tValidation Loss: 0.082159\n",
      "Epoch: 19 \tTraining Loss: 0.082018 \tValidation Loss: 0.081641\n",
      "Epoch: 20 \tTraining Loss: 0.078008 \tValidation Loss: 0.082739\n"
     ]
    }
   ],
   "source": [
    "# to track validation loss\n",
    "valid_loss_min = np.Inf\n",
    "\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    # keep track of training and validation loss\n",
    "    train_loss = 0.0\n",
    "    valid_loss = 0.0\n",
    "    \n",
    "    # training model\n",
    "    model.train()\n",
    "    for data, target in train_loader:\n",
    "        # move tensors to GPU if CUDA is available\n",
    "        data = data.to(device)\n",
    "        target = target.to(device)\n",
    "        \n",
    "        # clear the gradients of all optimized variables\n",
    "        optimizer.zero_grad()\n",
    "        # forward pass: compute predicted outputs by passing inputs to the model\n",
    "        output = model(data)\n",
    "        # calculate the batch loss\n",
    "        loss = criterion(output, target)\n",
    "        # backward pass: compute gradient of the loss with respect to model parameters\n",
    "        loss.backward()\n",
    "        # perform a single optimization step (parameter update)\n",
    "        optimizer.step()\n",
    "        # update training loss\n",
    "        train_loss += loss.item() * data.size(0)\n",
    "        \n",
    "    # validate the model\n",
    "    model.eval()\n",
    "    for data, target in valid_loader:\n",
    "        # move tensors to GPU if CUDA is available\n",
    "        data = data.to(device)\n",
    "        target = target.to(device)\n",
    "        \n",
    "        # forward pass: compute predicted outputs by passing inputs to the model\n",
    "        output = model(data)\n",
    "        # calculate the batch loss\n",
    "        loss = criterion(output, target)\n",
    "        # update average validation loss \n",
    "        valid_loss += loss.item() * data.size(0)\n",
    "    \n",
    "    # calculate average losses\n",
    "    train_loss = train_loss/len(train_loader.sampler)\n",
    "    valid_loss = valid_loss/len(valid_loader.sampler)\n",
    "        \n",
    "    # print training/validation statistics \n",
    "    print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(\n",
    "        epoch, train_loss, valid_loss))\n",
    "    \n",
    "    # save model if validation loss has decreased\n",
    "    if valid_loss <= valid_loss_min:\n",
    "        print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n",
    "        valid_loss_min,\n",
    "        valid_loss))\n",
    "        torch.save(model.state_dict(), 'model_cifar.pt')\n",
    "        valid_loss_min = valid_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# set best model params\n",
    "model.load_state_dict(torch.load('model_cifar.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 97.08571428571429 %\n"
     ]
    }
   ],
   "source": [
    "# check accuracy on validation dataset\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for data, target in valid_loader:\n",
    "        # move tensors to GPU if CUDA is available\n",
    "        data = data.to(device)\n",
    "        target = target.to(device)\n",
    "        \n",
    "        # forward pass: compute predicted outputs by passing inputs to the model\n",
    "        outputs = model(data)\n",
    "        \n",
    "        # convert output probabilities to predicted class\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        \n",
    "        total += target.size(0)\n",
    "        correct += (predicted == target).sum().item()\n",
    "    print('Test Accuracy: {} %'.format(100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare data for submission\n",
    "df_submission = pd.read_csv(data_dir + \"/sample_submission.csv\")\n",
    "\n",
    "test_data = cactData(df_submission, test_dir, transform=valid_transf)\n",
    "test_loader = DataLoader(dataset = test_data, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/opt/conda/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/opt/conda/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/opt/conda/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/opt/conda/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/opt/conda/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/opt/conda/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "pred_label = []\n",
    "for data, target in test_loader:\n",
    "    # move tensors to GPU if CUDA is available\n",
    "    data, target = data.cuda(), target.cuda()\n",
    "    \n",
    "    # forward pass\n",
    "    output = model(data)\n",
    "    \n",
    "    prob = output[:,1].detach().cpu().numpy()\n",
    "    for p in prob:\n",
    "        pred_label.append(p)\n",
    "\n",
    "# Set predicted labels to submission\n",
    "df_submission['has_cactus'] = pred_label\n",
    "df_submission.to_csv('submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
