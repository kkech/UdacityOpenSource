{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Federated_learning_pysyft_pytorch.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "nMfVhYG9W6Yn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Install dependencies\n",
        "!pip install syft\n",
        "!pip install torch"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ex25_Y71Xnpz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "outputId": "386d6669-8d50-4a20-cc9a-775a9232b6d6"
      },
      "source": [
        "# import dependencies\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "import syft as sy\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0820 14:56:16.005809 139905971980160 secure_random.py:26] Falling back to insecure randomness since the required custom op could not be found for the installed version of TensorFlow. Fix this by compiling custom ops. Missing file was '/usr/local/lib/python3.6/dist-packages/tf_encrypted/operations/secure_random/secure_random_module_tf_1.14.0.so'\n",
            "W0820 14:56:16.025116 139905971980160 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/tf_encrypted/session.py:26: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ptfdoYsBYCbh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# make workers\n",
        "hook = sy.TorchHook(torch)\n",
        "bob =  sy.VirtualWorker(hook, id=\"bob\")\n",
        "alice =  sy.VirtualWorker(hook, id=\"alice\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R3VpxripYN9N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# define the arguments used in the model\n",
        "class Arguments():\n",
        "    def __init__(self):\n",
        "        self.batch_size = 64\n",
        "        self.test_batch_size = 1000\n",
        "        self.epochs = 10\n",
        "        self.lr = 0.01\n",
        "        self.momentum = 0.5\n",
        "        self.no_cuda = False\n",
        "        self.seed = 1\n",
        "        self.log_interval = 10\n",
        "        self.save_model = False\n",
        "\n",
        "args = Arguments()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PHZqbwACYt4Y",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "9a32ba15-4895-4cd8-f39e-264902617d04"
      },
      "source": [
        "torch.manual_seed(args.seed)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7f3e421d2cf0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iVDzH4k1ZJBM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "use_cuda = not args.no_cuda and torch.cuda.is_available()\n",
        "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "kwargs = {'num_workers': 1, 'pin_memory': True} if use_cuda else {}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O0s44h3GZU0n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "federated_train_loader = sy.FederatedDataLoader( # <-- this is now a FederatedDataLoader \n",
        "    datasets.MNIST('../data', train=True, download=True,\n",
        "                   transform=transforms.Compose([\n",
        "                       transforms.ToTensor(),\n",
        "                       transforms.Normalize((0.1307,), (0.3081,))\n",
        "                   ]))\n",
        "    .federate((bob, alice)), \n",
        "    batch_size=args.batch_size, shuffle=True, **kwargs)\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    datasets.MNIST('../data', train=False, transform=transforms.Compose([\n",
        "                       transforms.ToTensor(),\n",
        "                       transforms.Normalize((0.1307,), (0.3081,))\n",
        "                   ])),\n",
        "    batch_size=args.test_batch_size, shuffle=True, **kwargs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NtIzqHRWbdMw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 20, 5, 1)\n",
        "        self.conv2 = nn.Conv2d(20, 50, 5, 1)\n",
        "        self.fc1 = nn.Linear(4*4*50, 500)\n",
        "        self.fc2 = nn.Linear(500, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = F.max_pool2d(x, 2, 2)\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = F.max_pool2d(x, 2, 2)\n",
        "        x = x.view(-1, 4*4*50)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return F.log_softmax(x, dim=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aP3rZcieb-2u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "model = Net()\n",
        "model = model.to(device)  #pushing the model into available device.\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SIl1FCPScJOW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "c6ef71ce-5e26-4dd6-8992-8429a0e7bc04"
      },
      "source": [
        "\n",
        "for epoch in range(1, args.epochs + 1):\n",
        "    # Train the model\n",
        "    model.train()\n",
        "    for batch_idx, (data, target) in enumerate(federated_train_loader): # iterate through each worker's dataset\n",
        "        \n",
        "        model.send(data.location) #send the model to the right location ; data.location returns the worker name in which the data is present\n",
        "        \n",
        "        data, target = data.to(device), target.to(device) # pushing both the data and target labels onto the available device.\n",
        "        \n",
        "        optimizer.zero_grad() # 1) erase previous gradients (if they exist)\n",
        "        output = model(data)  # 2) make a prediction\n",
        "        loss = F.nll_loss(output, target)  # 3) calculate how much we missed\n",
        "        loss.backward()  # 4) figure out which weights caused us to miss\n",
        "        optimizer.step()  # 5) change those weights\n",
        "        model.get()  # get the model back (with gradients)\n",
        "        \n",
        "        if batch_idx % args.log_interval == 0:\n",
        "            loss = loss.get() #get the loss back\n",
        "            print('Epoch: {} [Training: {:.0f}%]\\tLoss: {:.6f}'.format(epoch, 100. * batch_idx / len(federated_train_loader), loss.item()))\n",
        "    # Test the model\n",
        "    model.eval()  \n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "      for data, target in test_loader:\n",
        "        data, target = data.to(device), target.to(device) \n",
        "        output = model(data) # Getting a prediction\n",
        "\n",
        "        test_loss += F.nll_loss(output, target, reduction='sum').item() #updating test loss\n",
        "        pred = output.argmax(1, keepdim=True) # get the index of the max log-probability \n",
        "\n",
        "        correct += pred.eq(target.view_as(pred)).sum().item() #correct pred in the current test set.\n",
        "\n",
        "    test_loss /= len(test_loader.dataset) \n",
        "\n",
        "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(test_loss, correct, len(test_loader.dataset), 100. * correct / len(test_loader.dataset)))\n",
        "\n",
        "    torch.save(model.state_dict(), \"mnist_cnn.pt\")\n",
        "\n",
        "    \n",
        "    "
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 1 [Training: 0%]\tLoss: 2.305134\n",
            "Epoch: 1 [Training: 1%]\tLoss: 2.273475\n",
            "Epoch: 1 [Training: 2%]\tLoss: 2.216174\n",
            "Epoch: 1 [Training: 3%]\tLoss: 2.156802\n",
            "Epoch: 1 [Training: 4%]\tLoss: 2.139429\n",
            "Epoch: 1 [Training: 5%]\tLoss: 2.053059\n",
            "Epoch: 1 [Training: 6%]\tLoss: 1.896587\n",
            "Epoch: 1 [Training: 7%]\tLoss: 1.917239\n",
            "Epoch: 1 [Training: 9%]\tLoss: 1.655076\n",
            "Epoch: 1 [Training: 10%]\tLoss: 1.440329\n",
            "Epoch: 1 [Training: 11%]\tLoss: 1.231347\n",
            "Epoch: 1 [Training: 12%]\tLoss: 0.983715\n",
            "Epoch: 1 [Training: 13%]\tLoss: 0.867023\n",
            "Epoch: 1 [Training: 14%]\tLoss: 0.890953\n",
            "Epoch: 1 [Training: 15%]\tLoss: 0.861902\n",
            "Epoch: 1 [Training: 16%]\tLoss: 0.654317\n",
            "Epoch: 1 [Training: 17%]\tLoss: 0.587091\n",
            "Epoch: 1 [Training: 18%]\tLoss: 0.693011\n",
            "Epoch: 1 [Training: 19%]\tLoss: 0.593099\n",
            "Epoch: 1 [Training: 20%]\tLoss: 0.531986\n",
            "Epoch: 1 [Training: 21%]\tLoss: 0.400264\n",
            "Epoch: 1 [Training: 22%]\tLoss: 0.455322\n",
            "Epoch: 1 [Training: 23%]\tLoss: 0.438987\n",
            "Epoch: 1 [Training: 25%]\tLoss: 0.398361\n",
            "Epoch: 1 [Training: 26%]\tLoss: 0.371324\n",
            "Epoch: 1 [Training: 27%]\tLoss: 0.289150\n",
            "Epoch: 1 [Training: 28%]\tLoss: 0.416670\n",
            "Epoch: 1 [Training: 29%]\tLoss: 0.304617\n",
            "Epoch: 1 [Training: 30%]\tLoss: 0.367789\n",
            "Epoch: 1 [Training: 31%]\tLoss: 0.386265\n",
            "Epoch: 1 [Training: 32%]\tLoss: 0.314097\n",
            "Epoch: 1 [Training: 33%]\tLoss: 0.238745\n",
            "Epoch: 1 [Training: 34%]\tLoss: 0.534601\n",
            "Epoch: 1 [Training: 35%]\tLoss: 0.369211\n",
            "Epoch: 1 [Training: 36%]\tLoss: 0.464785\n",
            "Epoch: 1 [Training: 37%]\tLoss: 0.279363\n",
            "Epoch: 1 [Training: 38%]\tLoss: 0.238020\n",
            "Epoch: 1 [Training: 39%]\tLoss: 0.182981\n",
            "Epoch: 1 [Training: 41%]\tLoss: 0.321955\n",
            "Epoch: 1 [Training: 42%]\tLoss: 0.187752\n",
            "Epoch: 1 [Training: 43%]\tLoss: 0.286318\n",
            "Epoch: 1 [Training: 44%]\tLoss: 0.365828\n",
            "Epoch: 1 [Training: 45%]\tLoss: 0.523223\n",
            "Epoch: 1 [Training: 46%]\tLoss: 0.147624\n",
            "Epoch: 1 [Training: 47%]\tLoss: 0.131381\n",
            "Epoch: 1 [Training: 48%]\tLoss: 0.224323\n",
            "Epoch: 1 [Training: 49%]\tLoss: 0.274111\n",
            "Epoch: 1 [Training: 50%]\tLoss: 0.215077\n",
            "Epoch: 1 [Training: 51%]\tLoss: 0.143916\n",
            "Epoch: 1 [Training: 52%]\tLoss: 0.291434\n",
            "Epoch: 1 [Training: 53%]\tLoss: 0.230507\n",
            "Epoch: 1 [Training: 54%]\tLoss: 0.268499\n",
            "Epoch: 1 [Training: 55%]\tLoss: 0.205015\n",
            "Epoch: 1 [Training: 57%]\tLoss: 0.147734\n",
            "Epoch: 1 [Training: 58%]\tLoss: 0.187934\n",
            "Epoch: 1 [Training: 59%]\tLoss: 0.171631\n",
            "Epoch: 1 [Training: 60%]\tLoss: 0.478283\n",
            "Epoch: 1 [Training: 61%]\tLoss: 0.303621\n",
            "Epoch: 1 [Training: 62%]\tLoss: 0.243553\n",
            "Epoch: 1 [Training: 63%]\tLoss: 0.263034\n",
            "Epoch: 1 [Training: 64%]\tLoss: 0.239960\n",
            "Epoch: 1 [Training: 65%]\tLoss: 0.456759\n",
            "Epoch: 1 [Training: 66%]\tLoss: 0.200484\n",
            "Epoch: 1 [Training: 67%]\tLoss: 0.256438\n",
            "Epoch: 1 [Training: 68%]\tLoss: 0.196973\n",
            "Epoch: 1 [Training: 69%]\tLoss: 0.270759\n",
            "Epoch: 1 [Training: 70%]\tLoss: 0.191681\n",
            "Epoch: 1 [Training: 71%]\tLoss: 0.118813\n",
            "Epoch: 1 [Training: 72%]\tLoss: 0.176458\n",
            "Epoch: 1 [Training: 74%]\tLoss: 0.174667\n",
            "Epoch: 1 [Training: 75%]\tLoss: 0.193119\n",
            "Epoch: 1 [Training: 76%]\tLoss: 0.349294\n",
            "Epoch: 1 [Training: 77%]\tLoss: 0.220412\n",
            "Epoch: 1 [Training: 78%]\tLoss: 0.191737\n",
            "Epoch: 1 [Training: 79%]\tLoss: 0.155758\n",
            "Epoch: 1 [Training: 80%]\tLoss: 0.323814\n",
            "Epoch: 1 [Training: 81%]\tLoss: 0.246998\n",
            "Epoch: 1 [Training: 82%]\tLoss: 0.254214\n",
            "Epoch: 1 [Training: 83%]\tLoss: 0.273938\n",
            "Epoch: 1 [Training: 84%]\tLoss: 0.266370\n",
            "Epoch: 1 [Training: 85%]\tLoss: 0.150609\n",
            "Epoch: 1 [Training: 86%]\tLoss: 0.129327\n",
            "Epoch: 1 [Training: 87%]\tLoss: 0.276623\n",
            "Epoch: 1 [Training: 88%]\tLoss: 0.212820\n",
            "Epoch: 1 [Training: 90%]\tLoss: 0.182833\n",
            "Epoch: 1 [Training: 91%]\tLoss: 0.143227\n",
            "Epoch: 1 [Training: 92%]\tLoss: 0.118357\n",
            "Epoch: 1 [Training: 93%]\tLoss: 0.223088\n",
            "Epoch: 1 [Training: 94%]\tLoss: 0.214281\n",
            "Epoch: 1 [Training: 95%]\tLoss: 0.085184\n",
            "Epoch: 1 [Training: 96%]\tLoss: 0.081163\n",
            "Epoch: 1 [Training: 97%]\tLoss: 0.119667\n",
            "Epoch: 1 [Training: 98%]\tLoss: 0.172809\n",
            "Epoch: 1 [Training: 99%]\tLoss: 0.143191\n",
            "\n",
            "Test set: Average loss: 0.1575, Accuracy: 9511/10000 (95%)\n",
            "\n",
            "Epoch: 2 [Training: 0%]\tLoss: 0.103221\n",
            "Epoch: 2 [Training: 1%]\tLoss: 0.244422\n",
            "Epoch: 2 [Training: 2%]\tLoss: 0.402155\n",
            "Epoch: 2 [Training: 3%]\tLoss: 0.105845\n",
            "Epoch: 2 [Training: 4%]\tLoss: 0.349342\n",
            "Epoch: 2 [Training: 5%]\tLoss: 0.211654\n",
            "Epoch: 2 [Training: 6%]\tLoss: 0.147023\n",
            "Epoch: 2 [Training: 7%]\tLoss: 0.192190\n",
            "Epoch: 2 [Training: 9%]\tLoss: 0.103523\n",
            "Epoch: 2 [Training: 10%]\tLoss: 0.148321\n",
            "Epoch: 2 [Training: 11%]\tLoss: 0.148263\n",
            "Epoch: 2 [Training: 12%]\tLoss: 0.060192\n",
            "Epoch: 2 [Training: 13%]\tLoss: 0.108846\n",
            "Epoch: 2 [Training: 14%]\tLoss: 0.152352\n",
            "Epoch: 2 [Training: 15%]\tLoss: 0.152677\n",
            "Epoch: 2 [Training: 16%]\tLoss: 0.111557\n",
            "Epoch: 2 [Training: 17%]\tLoss: 0.186860\n",
            "Epoch: 2 [Training: 18%]\tLoss: 0.138993\n",
            "Epoch: 2 [Training: 19%]\tLoss: 0.118514\n",
            "Epoch: 2 [Training: 20%]\tLoss: 0.221352\n",
            "Epoch: 2 [Training: 21%]\tLoss: 0.132464\n",
            "Epoch: 2 [Training: 22%]\tLoss: 0.063139\n",
            "Epoch: 2 [Training: 23%]\tLoss: 0.124831\n",
            "Epoch: 2 [Training: 25%]\tLoss: 0.163303\n",
            "Epoch: 2 [Training: 26%]\tLoss: 0.089060\n",
            "Epoch: 2 [Training: 27%]\tLoss: 0.148574\n",
            "Epoch: 2 [Training: 28%]\tLoss: 0.046008\n",
            "Epoch: 2 [Training: 29%]\tLoss: 0.156941\n",
            "Epoch: 2 [Training: 30%]\tLoss: 0.132319\n",
            "Epoch: 2 [Training: 31%]\tLoss: 0.099179\n",
            "Epoch: 2 [Training: 32%]\tLoss: 0.159745\n",
            "Epoch: 2 [Training: 33%]\tLoss: 0.054083\n",
            "Epoch: 2 [Training: 34%]\tLoss: 0.210852\n",
            "Epoch: 2 [Training: 35%]\tLoss: 0.157456\n",
            "Epoch: 2 [Training: 36%]\tLoss: 0.115076\n",
            "Epoch: 2 [Training: 37%]\tLoss: 0.144074\n",
            "Epoch: 2 [Training: 38%]\tLoss: 0.229732\n",
            "Epoch: 2 [Training: 39%]\tLoss: 0.074357\n",
            "Epoch: 2 [Training: 41%]\tLoss: 0.181132\n",
            "Epoch: 2 [Training: 42%]\tLoss: 0.197346\n",
            "Epoch: 2 [Training: 43%]\tLoss: 0.067677\n",
            "Epoch: 2 [Training: 44%]\tLoss: 0.065908\n",
            "Epoch: 2 [Training: 45%]\tLoss: 0.206655\n",
            "Epoch: 2 [Training: 46%]\tLoss: 0.115207\n",
            "Epoch: 2 [Training: 47%]\tLoss: 0.153855\n",
            "Epoch: 2 [Training: 48%]\tLoss: 0.079507\n",
            "Epoch: 2 [Training: 49%]\tLoss: 0.126364\n",
            "Epoch: 2 [Training: 50%]\tLoss: 0.038445\n",
            "Epoch: 2 [Training: 51%]\tLoss: 0.063148\n",
            "Epoch: 2 [Training: 52%]\tLoss: 0.074466\n",
            "Epoch: 2 [Training: 53%]\tLoss: 0.056466\n",
            "Epoch: 2 [Training: 54%]\tLoss: 0.158469\n",
            "Epoch: 2 [Training: 55%]\tLoss: 0.163727\n",
            "Epoch: 2 [Training: 57%]\tLoss: 0.109543\n",
            "Epoch: 2 [Training: 58%]\tLoss: 0.156446\n",
            "Epoch: 2 [Training: 59%]\tLoss: 0.072912\n",
            "Epoch: 2 [Training: 60%]\tLoss: 0.130330\n",
            "Epoch: 2 [Training: 61%]\tLoss: 0.074369\n",
            "Epoch: 2 [Training: 62%]\tLoss: 0.093282\n",
            "Epoch: 2 [Training: 63%]\tLoss: 0.090162\n",
            "Epoch: 2 [Training: 64%]\tLoss: 0.161415\n",
            "Epoch: 2 [Training: 65%]\tLoss: 0.128906\n",
            "Epoch: 2 [Training: 66%]\tLoss: 0.049100\n",
            "Epoch: 2 [Training: 67%]\tLoss: 0.074256\n",
            "Epoch: 2 [Training: 68%]\tLoss: 0.113933\n",
            "Epoch: 2 [Training: 69%]\tLoss: 0.122479\n",
            "Epoch: 2 [Training: 70%]\tLoss: 0.153609\n",
            "Epoch: 2 [Training: 71%]\tLoss: 0.138244\n",
            "Epoch: 2 [Training: 72%]\tLoss: 0.107693\n",
            "Epoch: 2 [Training: 74%]\tLoss: 0.048192\n",
            "Epoch: 2 [Training: 75%]\tLoss: 0.102147\n",
            "Epoch: 2 [Training: 76%]\tLoss: 0.116528\n",
            "Epoch: 2 [Training: 77%]\tLoss: 0.085225\n",
            "Epoch: 2 [Training: 78%]\tLoss: 0.072129\n",
            "Epoch: 2 [Training: 79%]\tLoss: 0.198065\n",
            "Epoch: 2 [Training: 80%]\tLoss: 0.101033\n",
            "Epoch: 2 [Training: 81%]\tLoss: 0.082122\n",
            "Epoch: 2 [Training: 82%]\tLoss: 0.095091\n",
            "Epoch: 2 [Training: 83%]\tLoss: 0.154688\n",
            "Epoch: 2 [Training: 84%]\tLoss: 0.106988\n",
            "Epoch: 2 [Training: 85%]\tLoss: 0.062860\n",
            "Epoch: 2 [Training: 86%]\tLoss: 0.032838\n",
            "Epoch: 2 [Training: 87%]\tLoss: 0.167880\n",
            "Epoch: 2 [Training: 88%]\tLoss: 0.069700\n",
            "Epoch: 2 [Training: 90%]\tLoss: 0.073842\n",
            "Epoch: 2 [Training: 91%]\tLoss: 0.049357\n",
            "Epoch: 2 [Training: 92%]\tLoss: 0.098377\n",
            "Epoch: 2 [Training: 93%]\tLoss: 0.113492\n",
            "Epoch: 2 [Training: 94%]\tLoss: 0.091218\n",
            "Epoch: 2 [Training: 95%]\tLoss: 0.041372\n",
            "Epoch: 2 [Training: 96%]\tLoss: 0.111734\n",
            "Epoch: 2 [Training: 97%]\tLoss: 0.083618\n",
            "Epoch: 2 [Training: 98%]\tLoss: 0.064205\n",
            "Epoch: 2 [Training: 99%]\tLoss: 0.069066\n",
            "\n",
            "Test set: Average loss: 0.0899, Accuracy: 9735/10000 (97%)\n",
            "\n",
            "Epoch: 3 [Training: 0%]\tLoss: 0.081291\n",
            "Epoch: 3 [Training: 1%]\tLoss: 0.115016\n",
            "Epoch: 3 [Training: 2%]\tLoss: 0.148862\n",
            "Epoch: 3 [Training: 3%]\tLoss: 0.080039\n",
            "Epoch: 3 [Training: 4%]\tLoss: 0.141026\n",
            "Epoch: 3 [Training: 5%]\tLoss: 0.090113\n",
            "Epoch: 3 [Training: 6%]\tLoss: 0.184774\n",
            "Epoch: 3 [Training: 7%]\tLoss: 0.286939\n",
            "Epoch: 3 [Training: 9%]\tLoss: 0.261452\n",
            "Epoch: 3 [Training: 10%]\tLoss: 0.072767\n",
            "Epoch: 3 [Training: 11%]\tLoss: 0.129465\n",
            "Epoch: 3 [Training: 12%]\tLoss: 0.095007\n",
            "Epoch: 3 [Training: 13%]\tLoss: 0.078837\n",
            "Epoch: 3 [Training: 14%]\tLoss: 0.039845\n",
            "Epoch: 3 [Training: 15%]\tLoss: 0.056576\n",
            "Epoch: 3 [Training: 16%]\tLoss: 0.206170\n",
            "Epoch: 3 [Training: 17%]\tLoss: 0.121223\n",
            "Epoch: 3 [Training: 18%]\tLoss: 0.132383\n",
            "Epoch: 3 [Training: 19%]\tLoss: 0.249777\n",
            "Epoch: 3 [Training: 20%]\tLoss: 0.165334\n",
            "Epoch: 3 [Training: 21%]\tLoss: 0.030447\n",
            "Epoch: 3 [Training: 22%]\tLoss: 0.121717\n",
            "Epoch: 3 [Training: 23%]\tLoss: 0.089990\n",
            "Epoch: 3 [Training: 25%]\tLoss: 0.171353\n",
            "Epoch: 3 [Training: 26%]\tLoss: 0.178415\n",
            "Epoch: 3 [Training: 27%]\tLoss: 0.157047\n",
            "Epoch: 3 [Training: 28%]\tLoss: 0.067125\n",
            "Epoch: 3 [Training: 29%]\tLoss: 0.150552\n",
            "Epoch: 3 [Training: 30%]\tLoss: 0.075970\n",
            "Epoch: 3 [Training: 31%]\tLoss: 0.017012\n",
            "Epoch: 3 [Training: 32%]\tLoss: 0.028152\n",
            "Epoch: 3 [Training: 33%]\tLoss: 0.043230\n",
            "Epoch: 3 [Training: 34%]\tLoss: 0.030028\n",
            "Epoch: 3 [Training: 35%]\tLoss: 0.046844\n",
            "Epoch: 3 [Training: 36%]\tLoss: 0.119822\n",
            "Epoch: 3 [Training: 37%]\tLoss: 0.148301\n",
            "Epoch: 3 [Training: 38%]\tLoss: 0.055311\n",
            "Epoch: 3 [Training: 39%]\tLoss: 0.110982\n",
            "Epoch: 3 [Training: 41%]\tLoss: 0.054740\n",
            "Epoch: 3 [Training: 42%]\tLoss: 0.034426\n",
            "Epoch: 3 [Training: 43%]\tLoss: 0.204520\n",
            "Epoch: 3 [Training: 44%]\tLoss: 0.036480\n",
            "Epoch: 3 [Training: 45%]\tLoss: 0.291176\n",
            "Epoch: 3 [Training: 46%]\tLoss: 0.095636\n",
            "Epoch: 3 [Training: 47%]\tLoss: 0.087709\n",
            "Epoch: 3 [Training: 48%]\tLoss: 0.055141\n",
            "Epoch: 3 [Training: 49%]\tLoss: 0.157873\n",
            "Epoch: 3 [Training: 50%]\tLoss: 0.095859\n",
            "Epoch: 3 [Training: 51%]\tLoss: 0.064245\n",
            "Epoch: 3 [Training: 52%]\tLoss: 0.020321\n",
            "Epoch: 3 [Training: 53%]\tLoss: 0.041318\n",
            "Epoch: 3 [Training: 54%]\tLoss: 0.025072\n",
            "Epoch: 3 [Training: 55%]\tLoss: 0.051386\n",
            "Epoch: 3 [Training: 57%]\tLoss: 0.058133\n",
            "Epoch: 3 [Training: 58%]\tLoss: 0.071311\n",
            "Epoch: 3 [Training: 59%]\tLoss: 0.096919\n",
            "Epoch: 3 [Training: 60%]\tLoss: 0.166712\n",
            "Epoch: 3 [Training: 61%]\tLoss: 0.039723\n",
            "Epoch: 3 [Training: 62%]\tLoss: 0.143135\n",
            "Epoch: 3 [Training: 63%]\tLoss: 0.028540\n",
            "Epoch: 3 [Training: 64%]\tLoss: 0.034051\n",
            "Epoch: 3 [Training: 65%]\tLoss: 0.071771\n",
            "Epoch: 3 [Training: 66%]\tLoss: 0.019140\n",
            "Epoch: 3 [Training: 67%]\tLoss: 0.082193\n",
            "Epoch: 3 [Training: 68%]\tLoss: 0.018273\n",
            "Epoch: 3 [Training: 69%]\tLoss: 0.118543\n",
            "Epoch: 3 [Training: 70%]\tLoss: 0.169696\n",
            "Epoch: 3 [Training: 71%]\tLoss: 0.055946\n",
            "Epoch: 3 [Training: 72%]\tLoss: 0.052105\n",
            "Epoch: 3 [Training: 74%]\tLoss: 0.054504\n",
            "Epoch: 3 [Training: 75%]\tLoss: 0.026263\n",
            "Epoch: 3 [Training: 76%]\tLoss: 0.080822\n",
            "Epoch: 3 [Training: 77%]\tLoss: 0.074233\n",
            "Epoch: 3 [Training: 78%]\tLoss: 0.068628\n",
            "Epoch: 3 [Training: 79%]\tLoss: 0.116148\n",
            "Epoch: 3 [Training: 80%]\tLoss: 0.024500\n",
            "Epoch: 3 [Training: 81%]\tLoss: 0.187242\n",
            "Epoch: 3 [Training: 82%]\tLoss: 0.249473\n",
            "Epoch: 3 [Training: 83%]\tLoss: 0.099400\n",
            "Epoch: 3 [Training: 84%]\tLoss: 0.078691\n",
            "Epoch: 3 [Training: 85%]\tLoss: 0.147570\n",
            "Epoch: 3 [Training: 86%]\tLoss: 0.026502\n",
            "Epoch: 3 [Training: 87%]\tLoss: 0.112970\n",
            "Epoch: 3 [Training: 88%]\tLoss: 0.049495\n",
            "Epoch: 3 [Training: 90%]\tLoss: 0.055414\n",
            "Epoch: 3 [Training: 91%]\tLoss: 0.080249\n",
            "Epoch: 3 [Training: 92%]\tLoss: 0.050256\n",
            "Epoch: 3 [Training: 93%]\tLoss: 0.067579\n",
            "Epoch: 3 [Training: 94%]\tLoss: 0.019180\n",
            "Epoch: 3 [Training: 95%]\tLoss: 0.078065\n",
            "Epoch: 3 [Training: 96%]\tLoss: 0.057820\n",
            "Epoch: 3 [Training: 97%]\tLoss: 0.055631\n",
            "Epoch: 3 [Training: 98%]\tLoss: 0.017225\n",
            "Epoch: 3 [Training: 99%]\tLoss: 0.463102\n",
            "\n",
            "Test set: Average loss: 0.0738, Accuracy: 9758/10000 (98%)\n",
            "\n",
            "Epoch: 4 [Training: 0%]\tLoss: 0.146829\n",
            "Epoch: 4 [Training: 1%]\tLoss: 0.140821\n",
            "Epoch: 4 [Training: 2%]\tLoss: 0.027331\n",
            "Epoch: 4 [Training: 3%]\tLoss: 0.065596\n",
            "Epoch: 4 [Training: 4%]\tLoss: 0.088037\n",
            "Epoch: 4 [Training: 5%]\tLoss: 0.096970\n",
            "Epoch: 4 [Training: 6%]\tLoss: 0.045302\n",
            "Epoch: 4 [Training: 7%]\tLoss: 0.090846\n",
            "Epoch: 4 [Training: 9%]\tLoss: 0.103507\n",
            "Epoch: 4 [Training: 10%]\tLoss: 0.101460\n",
            "Epoch: 4 [Training: 11%]\tLoss: 0.018280\n",
            "Epoch: 4 [Training: 12%]\tLoss: 0.076733\n",
            "Epoch: 4 [Training: 13%]\tLoss: 0.038305\n",
            "Epoch: 4 [Training: 14%]\tLoss: 0.030344\n",
            "Epoch: 4 [Training: 15%]\tLoss: 0.026687\n",
            "Epoch: 4 [Training: 16%]\tLoss: 0.068318\n",
            "Epoch: 4 [Training: 17%]\tLoss: 0.031968\n",
            "Epoch: 4 [Training: 18%]\tLoss: 0.078717\n",
            "Epoch: 4 [Training: 19%]\tLoss: 0.116785\n",
            "Epoch: 4 [Training: 20%]\tLoss: 0.107049\n",
            "Epoch: 4 [Training: 21%]\tLoss: 0.033536\n",
            "Epoch: 4 [Training: 22%]\tLoss: 0.105321\n",
            "Epoch: 4 [Training: 23%]\tLoss: 0.228651\n",
            "Epoch: 4 [Training: 25%]\tLoss: 0.015653\n",
            "Epoch: 4 [Training: 26%]\tLoss: 0.047861\n",
            "Epoch: 4 [Training: 27%]\tLoss: 0.065255\n",
            "Epoch: 4 [Training: 28%]\tLoss: 0.145419\n",
            "Epoch: 4 [Training: 29%]\tLoss: 0.048203\n",
            "Epoch: 4 [Training: 30%]\tLoss: 0.038587\n",
            "Epoch: 4 [Training: 31%]\tLoss: 0.045567\n",
            "Epoch: 4 [Training: 32%]\tLoss: 0.075156\n",
            "Epoch: 4 [Training: 33%]\tLoss: 0.041997\n",
            "Epoch: 4 [Training: 34%]\tLoss: 0.035451\n",
            "Epoch: 4 [Training: 35%]\tLoss: 0.083796\n",
            "Epoch: 4 [Training: 36%]\tLoss: 0.028792\n",
            "Epoch: 4 [Training: 37%]\tLoss: 0.061869\n",
            "Epoch: 4 [Training: 38%]\tLoss: 0.024896\n",
            "Epoch: 4 [Training: 39%]\tLoss: 0.112962\n",
            "Epoch: 4 [Training: 41%]\tLoss: 0.088006\n",
            "Epoch: 4 [Training: 42%]\tLoss: 0.143771\n",
            "Epoch: 4 [Training: 43%]\tLoss: 0.098854\n",
            "Epoch: 4 [Training: 44%]\tLoss: 0.158239\n",
            "Epoch: 4 [Training: 45%]\tLoss: 0.175964\n",
            "Epoch: 4 [Training: 46%]\tLoss: 0.027592\n",
            "Epoch: 4 [Training: 47%]\tLoss: 0.062299\n",
            "Epoch: 4 [Training: 48%]\tLoss: 0.093517\n",
            "Epoch: 4 [Training: 49%]\tLoss: 0.153039\n",
            "Epoch: 4 [Training: 50%]\tLoss: 0.024540\n",
            "Epoch: 4 [Training: 51%]\tLoss: 0.018805\n",
            "Epoch: 4 [Training: 52%]\tLoss: 0.144859\n",
            "Epoch: 4 [Training: 53%]\tLoss: 0.021880\n",
            "Epoch: 4 [Training: 54%]\tLoss: 0.150391\n",
            "Epoch: 4 [Training: 55%]\tLoss: 0.056411\n",
            "Epoch: 4 [Training: 57%]\tLoss: 0.085769\n",
            "Epoch: 4 [Training: 58%]\tLoss: 0.029585\n",
            "Epoch: 4 [Training: 59%]\tLoss: 0.065254\n",
            "Epoch: 4 [Training: 60%]\tLoss: 0.097678\n",
            "Epoch: 4 [Training: 61%]\tLoss: 0.009927\n",
            "Epoch: 4 [Training: 62%]\tLoss: 0.057545\n",
            "Epoch: 4 [Training: 63%]\tLoss: 0.080230\n",
            "Epoch: 4 [Training: 64%]\tLoss: 0.044069\n",
            "Epoch: 4 [Training: 65%]\tLoss: 0.025078\n",
            "Epoch: 4 [Training: 66%]\tLoss: 0.092041\n",
            "Epoch: 4 [Training: 67%]\tLoss: 0.049970\n",
            "Epoch: 4 [Training: 68%]\tLoss: 0.012687\n",
            "Epoch: 4 [Training: 69%]\tLoss: 0.133378\n",
            "Epoch: 4 [Training: 70%]\tLoss: 0.135398\n",
            "Epoch: 4 [Training: 71%]\tLoss: 0.033415\n",
            "Epoch: 4 [Training: 72%]\tLoss: 0.025962\n",
            "Epoch: 4 [Training: 74%]\tLoss: 0.057950\n",
            "Epoch: 4 [Training: 75%]\tLoss: 0.059852\n",
            "Epoch: 4 [Training: 76%]\tLoss: 0.078188\n",
            "Epoch: 4 [Training: 77%]\tLoss: 0.020599\n",
            "Epoch: 4 [Training: 78%]\tLoss: 0.044933\n",
            "Epoch: 4 [Training: 79%]\tLoss: 0.068190\n",
            "Epoch: 4 [Training: 80%]\tLoss: 0.040647\n",
            "Epoch: 4 [Training: 81%]\tLoss: 0.027091\n",
            "Epoch: 4 [Training: 82%]\tLoss: 0.131731\n",
            "Epoch: 4 [Training: 83%]\tLoss: 0.166192\n",
            "Epoch: 4 [Training: 84%]\tLoss: 0.022503\n",
            "Epoch: 4 [Training: 85%]\tLoss: 0.088655\n",
            "Epoch: 4 [Training: 86%]\tLoss: 0.067753\n",
            "Epoch: 4 [Training: 87%]\tLoss: 0.016120\n",
            "Epoch: 4 [Training: 88%]\tLoss: 0.076061\n",
            "Epoch: 4 [Training: 90%]\tLoss: 0.064018\n",
            "Epoch: 4 [Training: 91%]\tLoss: 0.149623\n",
            "Epoch: 4 [Training: 92%]\tLoss: 0.083822\n",
            "Epoch: 4 [Training: 93%]\tLoss: 0.073588\n",
            "Epoch: 4 [Training: 94%]\tLoss: 0.047142\n",
            "Epoch: 4 [Training: 95%]\tLoss: 0.056050\n",
            "Epoch: 4 [Training: 96%]\tLoss: 0.061888\n",
            "Epoch: 4 [Training: 97%]\tLoss: 0.024935\n",
            "Epoch: 4 [Training: 98%]\tLoss: 0.056401\n",
            "Epoch: 4 [Training: 99%]\tLoss: 0.017513\n",
            "\n",
            "Test set: Average loss: 0.0548, Accuracy: 9813/10000 (98%)\n",
            "\n",
            "Epoch: 5 [Training: 0%]\tLoss: 0.082590\n",
            "Epoch: 5 [Training: 1%]\tLoss: 0.191632\n",
            "Epoch: 5 [Training: 2%]\tLoss: 0.027748\n",
            "Epoch: 5 [Training: 3%]\tLoss: 0.040117\n",
            "Epoch: 5 [Training: 4%]\tLoss: 0.027606\n",
            "Epoch: 5 [Training: 5%]\tLoss: 0.013729\n",
            "Epoch: 5 [Training: 6%]\tLoss: 0.028699\n",
            "Epoch: 5 [Training: 7%]\tLoss: 0.149101\n",
            "Epoch: 5 [Training: 9%]\tLoss: 0.051579\n",
            "Epoch: 5 [Training: 10%]\tLoss: 0.035378\n",
            "Epoch: 5 [Training: 11%]\tLoss: 0.110916\n",
            "Epoch: 5 [Training: 12%]\tLoss: 0.087541\n",
            "Epoch: 5 [Training: 13%]\tLoss: 0.063442\n",
            "Epoch: 5 [Training: 14%]\tLoss: 0.069622\n",
            "Epoch: 5 [Training: 15%]\tLoss: 0.153325\n",
            "Epoch: 5 [Training: 16%]\tLoss: 0.022147\n",
            "Epoch: 5 [Training: 17%]\tLoss: 0.021511\n",
            "Epoch: 5 [Training: 18%]\tLoss: 0.017715\n",
            "Epoch: 5 [Training: 19%]\tLoss: 0.165380\n",
            "Epoch: 5 [Training: 20%]\tLoss: 0.160860\n",
            "Epoch: 5 [Training: 21%]\tLoss: 0.127810\n",
            "Epoch: 5 [Training: 22%]\tLoss: 0.046082\n",
            "Epoch: 5 [Training: 23%]\tLoss: 0.027302\n",
            "Epoch: 5 [Training: 25%]\tLoss: 0.065605\n",
            "Epoch: 5 [Training: 26%]\tLoss: 0.101586\n",
            "Epoch: 5 [Training: 27%]\tLoss: 0.028328\n",
            "Epoch: 5 [Training: 28%]\tLoss: 0.019454\n",
            "Epoch: 5 [Training: 29%]\tLoss: 0.054320\n",
            "Epoch: 5 [Training: 30%]\tLoss: 0.040218\n",
            "Epoch: 5 [Training: 31%]\tLoss: 0.057851\n",
            "Epoch: 5 [Training: 32%]\tLoss: 0.034714\n",
            "Epoch: 5 [Training: 33%]\tLoss: 0.036103\n",
            "Epoch: 5 [Training: 34%]\tLoss: 0.031931\n",
            "Epoch: 5 [Training: 35%]\tLoss: 0.097715\n",
            "Epoch: 5 [Training: 36%]\tLoss: 0.080613\n",
            "Epoch: 5 [Training: 37%]\tLoss: 0.042281\n",
            "Epoch: 5 [Training: 38%]\tLoss: 0.032395\n",
            "Epoch: 5 [Training: 39%]\tLoss: 0.022558\n",
            "Epoch: 5 [Training: 41%]\tLoss: 0.066808\n",
            "Epoch: 5 [Training: 42%]\tLoss: 0.026322\n",
            "Epoch: 5 [Training: 43%]\tLoss: 0.083409\n",
            "Epoch: 5 [Training: 44%]\tLoss: 0.151458\n",
            "Epoch: 5 [Training: 45%]\tLoss: 0.022107\n",
            "Epoch: 5 [Training: 46%]\tLoss: 0.101277\n",
            "Epoch: 5 [Training: 47%]\tLoss: 0.024551\n",
            "Epoch: 5 [Training: 48%]\tLoss: 0.017720\n",
            "Epoch: 5 [Training: 49%]\tLoss: 0.105302\n",
            "Epoch: 5 [Training: 50%]\tLoss: 0.047151\n",
            "Epoch: 5 [Training: 51%]\tLoss: 0.039595\n",
            "Epoch: 5 [Training: 52%]\tLoss: 0.053231\n",
            "Epoch: 5 [Training: 53%]\tLoss: 0.094888\n",
            "Epoch: 5 [Training: 54%]\tLoss: 0.069773\n",
            "Epoch: 5 [Training: 55%]\tLoss: 0.054388\n",
            "Epoch: 5 [Training: 57%]\tLoss: 0.052219\n",
            "Epoch: 5 [Training: 58%]\tLoss: 0.113465\n",
            "Epoch: 5 [Training: 59%]\tLoss: 0.133883\n",
            "Epoch: 5 [Training: 60%]\tLoss: 0.079842\n",
            "Epoch: 5 [Training: 61%]\tLoss: 0.037761\n",
            "Epoch: 5 [Training: 62%]\tLoss: 0.046184\n",
            "Epoch: 5 [Training: 63%]\tLoss: 0.020413\n",
            "Epoch: 5 [Training: 64%]\tLoss: 0.029371\n",
            "Epoch: 5 [Training: 65%]\tLoss: 0.138446\n",
            "Epoch: 5 [Training: 66%]\tLoss: 0.073264\n",
            "Epoch: 5 [Training: 67%]\tLoss: 0.011653\n",
            "Epoch: 5 [Training: 68%]\tLoss: 0.031342\n",
            "Epoch: 5 [Training: 69%]\tLoss: 0.354484\n",
            "Epoch: 5 [Training: 70%]\tLoss: 0.026142\n",
            "Epoch: 5 [Training: 71%]\tLoss: 0.032137\n",
            "Epoch: 5 [Training: 72%]\tLoss: 0.022094\n",
            "Epoch: 5 [Training: 74%]\tLoss: 0.017510\n",
            "Epoch: 5 [Training: 75%]\tLoss: 0.041650\n",
            "Epoch: 5 [Training: 76%]\tLoss: 0.009573\n",
            "Epoch: 5 [Training: 77%]\tLoss: 0.023011\n",
            "Epoch: 5 [Training: 78%]\tLoss: 0.040824\n",
            "Epoch: 5 [Training: 79%]\tLoss: 0.039383\n",
            "Epoch: 5 [Training: 80%]\tLoss: 0.015553\n",
            "Epoch: 5 [Training: 81%]\tLoss: 0.009420\n",
            "Epoch: 5 [Training: 82%]\tLoss: 0.040539\n",
            "Epoch: 5 [Training: 83%]\tLoss: 0.062981\n",
            "Epoch: 5 [Training: 84%]\tLoss: 0.019704\n",
            "Epoch: 5 [Training: 85%]\tLoss: 0.050176\n",
            "Epoch: 5 [Training: 86%]\tLoss: 0.039655\n",
            "Epoch: 5 [Training: 87%]\tLoss: 0.020420\n",
            "Epoch: 5 [Training: 88%]\tLoss: 0.049979\n",
            "Epoch: 5 [Training: 90%]\tLoss: 0.016948\n",
            "Epoch: 5 [Training: 91%]\tLoss: 0.032157\n",
            "Epoch: 5 [Training: 92%]\tLoss: 0.063825\n",
            "Epoch: 5 [Training: 93%]\tLoss: 0.097401\n",
            "Epoch: 5 [Training: 94%]\tLoss: 0.012206\n",
            "Epoch: 5 [Training: 95%]\tLoss: 0.016439\n",
            "Epoch: 5 [Training: 96%]\tLoss: 0.027165\n",
            "Epoch: 5 [Training: 97%]\tLoss: 0.045493\n",
            "Epoch: 5 [Training: 98%]\tLoss: 0.014956\n",
            "Epoch: 5 [Training: 99%]\tLoss: 0.061822\n",
            "\n",
            "Test set: Average loss: 0.0460, Accuracy: 9849/10000 (98%)\n",
            "\n",
            "Epoch: 6 [Training: 0%]\tLoss: 0.052845\n",
            "Epoch: 6 [Training: 1%]\tLoss: 0.041022\n",
            "Epoch: 6 [Training: 2%]\tLoss: 0.101479\n",
            "Epoch: 6 [Training: 3%]\tLoss: 0.101472\n",
            "Epoch: 6 [Training: 4%]\tLoss: 0.060263\n",
            "Epoch: 6 [Training: 5%]\tLoss: 0.025262\n",
            "Epoch: 6 [Training: 6%]\tLoss: 0.011245\n",
            "Epoch: 6 [Training: 7%]\tLoss: 0.032220\n",
            "Epoch: 6 [Training: 9%]\tLoss: 0.063896\n",
            "Epoch: 6 [Training: 10%]\tLoss: 0.090635\n",
            "Epoch: 6 [Training: 11%]\tLoss: 0.011883\n",
            "Epoch: 6 [Training: 12%]\tLoss: 0.048127\n",
            "Epoch: 6 [Training: 13%]\tLoss: 0.111328\n",
            "Epoch: 6 [Training: 14%]\tLoss: 0.049169\n",
            "Epoch: 6 [Training: 15%]\tLoss: 0.026860\n",
            "Epoch: 6 [Training: 16%]\tLoss: 0.014489\n",
            "Epoch: 6 [Training: 17%]\tLoss: 0.082701\n",
            "Epoch: 6 [Training: 18%]\tLoss: 0.014496\n",
            "Epoch: 6 [Training: 19%]\tLoss: 0.086852\n",
            "Epoch: 6 [Training: 20%]\tLoss: 0.037308\n",
            "Epoch: 6 [Training: 21%]\tLoss: 0.038465\n",
            "Epoch: 6 [Training: 22%]\tLoss: 0.036762\n",
            "Epoch: 6 [Training: 23%]\tLoss: 0.032877\n",
            "Epoch: 6 [Training: 25%]\tLoss: 0.034338\n",
            "Epoch: 6 [Training: 26%]\tLoss: 0.097659\n",
            "Epoch: 6 [Training: 27%]\tLoss: 0.065059\n",
            "Epoch: 6 [Training: 28%]\tLoss: 0.072403\n",
            "Epoch: 6 [Training: 29%]\tLoss: 0.053820\n",
            "Epoch: 6 [Training: 30%]\tLoss: 0.006761\n",
            "Epoch: 6 [Training: 31%]\tLoss: 0.074201\n",
            "Epoch: 6 [Training: 32%]\tLoss: 0.042853\n",
            "Epoch: 6 [Training: 33%]\tLoss: 0.034724\n",
            "Epoch: 6 [Training: 34%]\tLoss: 0.049317\n",
            "Epoch: 6 [Training: 35%]\tLoss: 0.073565\n",
            "Epoch: 6 [Training: 36%]\tLoss: 0.084093\n",
            "Epoch: 6 [Training: 37%]\tLoss: 0.114023\n",
            "Epoch: 6 [Training: 38%]\tLoss: 0.048715\n",
            "Epoch: 6 [Training: 39%]\tLoss: 0.074322\n",
            "Epoch: 6 [Training: 41%]\tLoss: 0.002777\n",
            "Epoch: 6 [Training: 42%]\tLoss: 0.007704\n",
            "Epoch: 6 [Training: 43%]\tLoss: 0.014966\n",
            "Epoch: 6 [Training: 44%]\tLoss: 0.013714\n",
            "Epoch: 6 [Training: 45%]\tLoss: 0.019984\n",
            "Epoch: 6 [Training: 46%]\tLoss: 0.044726\n",
            "Epoch: 6 [Training: 47%]\tLoss: 0.051556\n",
            "Epoch: 6 [Training: 48%]\tLoss: 0.036293\n",
            "Epoch: 6 [Training: 49%]\tLoss: 0.036134\n",
            "Epoch: 6 [Training: 50%]\tLoss: 0.022428\n",
            "Epoch: 6 [Training: 51%]\tLoss: 0.087016\n",
            "Epoch: 6 [Training: 52%]\tLoss: 0.036529\n",
            "Epoch: 6 [Training: 53%]\tLoss: 0.062339\n",
            "Epoch: 6 [Training: 54%]\tLoss: 0.010435\n",
            "Epoch: 6 [Training: 55%]\tLoss: 0.037346\n",
            "Epoch: 6 [Training: 57%]\tLoss: 0.031494\n",
            "Epoch: 6 [Training: 58%]\tLoss: 0.026880\n",
            "Epoch: 6 [Training: 59%]\tLoss: 0.054342\n",
            "Epoch: 6 [Training: 60%]\tLoss: 0.033832\n",
            "Epoch: 6 [Training: 61%]\tLoss: 0.048298\n",
            "Epoch: 6 [Training: 62%]\tLoss: 0.082996\n",
            "Epoch: 6 [Training: 63%]\tLoss: 0.027112\n",
            "Epoch: 6 [Training: 64%]\tLoss: 0.006616\n",
            "Epoch: 6 [Training: 65%]\tLoss: 0.046024\n",
            "Epoch: 6 [Training: 66%]\tLoss: 0.024264\n",
            "Epoch: 6 [Training: 67%]\tLoss: 0.076249\n",
            "Epoch: 6 [Training: 68%]\tLoss: 0.018658\n",
            "Epoch: 6 [Training: 69%]\tLoss: 0.022150\n",
            "Epoch: 6 [Training: 70%]\tLoss: 0.041769\n",
            "Epoch: 6 [Training: 71%]\tLoss: 0.061550\n",
            "Epoch: 6 [Training: 72%]\tLoss: 0.081969\n",
            "Epoch: 6 [Training: 74%]\tLoss: 0.007849\n",
            "Epoch: 6 [Training: 75%]\tLoss: 0.118383\n",
            "Epoch: 6 [Training: 76%]\tLoss: 0.005613\n",
            "Epoch: 6 [Training: 77%]\tLoss: 0.064405\n",
            "Epoch: 6 [Training: 78%]\tLoss: 0.032996\n",
            "Epoch: 6 [Training: 79%]\tLoss: 0.224602\n",
            "Epoch: 6 [Training: 80%]\tLoss: 0.051831\n",
            "Epoch: 6 [Training: 81%]\tLoss: 0.129895\n",
            "Epoch: 6 [Training: 82%]\tLoss: 0.005333\n",
            "Epoch: 6 [Training: 83%]\tLoss: 0.024883\n",
            "Epoch: 6 [Training: 84%]\tLoss: 0.005739\n",
            "Epoch: 6 [Training: 85%]\tLoss: 0.019321\n",
            "Epoch: 6 [Training: 86%]\tLoss: 0.031440\n",
            "Epoch: 6 [Training: 87%]\tLoss: 0.164235\n",
            "Epoch: 6 [Training: 88%]\tLoss: 0.017333\n",
            "Epoch: 6 [Training: 90%]\tLoss: 0.053846\n",
            "Epoch: 6 [Training: 91%]\tLoss: 0.066497\n",
            "Epoch: 6 [Training: 92%]\tLoss: 0.015297\n",
            "Epoch: 6 [Training: 93%]\tLoss: 0.024572\n",
            "Epoch: 6 [Training: 94%]\tLoss: 0.021320\n",
            "Epoch: 6 [Training: 95%]\tLoss: 0.107925\n",
            "Epoch: 6 [Training: 96%]\tLoss: 0.099601\n",
            "Epoch: 6 [Training: 97%]\tLoss: 0.069870\n",
            "Epoch: 6 [Training: 98%]\tLoss: 0.090656\n",
            "Epoch: 6 [Training: 99%]\tLoss: 0.012264\n",
            "\n",
            "Test set: Average loss: 0.0441, Accuracy: 9859/10000 (99%)\n",
            "\n",
            "Epoch: 7 [Training: 0%]\tLoss: 0.047999\n",
            "Epoch: 7 [Training: 1%]\tLoss: 0.034098\n",
            "Epoch: 7 [Training: 2%]\tLoss: 0.104362\n",
            "Epoch: 7 [Training: 3%]\tLoss: 0.022044\n",
            "Epoch: 7 [Training: 4%]\tLoss: 0.052735\n",
            "Epoch: 7 [Training: 5%]\tLoss: 0.013907\n",
            "Epoch: 7 [Training: 6%]\tLoss: 0.012739\n",
            "Epoch: 7 [Training: 7%]\tLoss: 0.007364\n",
            "Epoch: 7 [Training: 9%]\tLoss: 0.008462\n",
            "Epoch: 7 [Training: 10%]\tLoss: 0.040412\n",
            "Epoch: 7 [Training: 11%]\tLoss: 0.051907\n",
            "Epoch: 7 [Training: 12%]\tLoss: 0.045256\n",
            "Epoch: 7 [Training: 13%]\tLoss: 0.109332\n",
            "Epoch: 7 [Training: 14%]\tLoss: 0.032891\n",
            "Epoch: 7 [Training: 15%]\tLoss: 0.011673\n",
            "Epoch: 7 [Training: 16%]\tLoss: 0.027032\n",
            "Epoch: 7 [Training: 17%]\tLoss: 0.021082\n",
            "Epoch: 7 [Training: 18%]\tLoss: 0.054524\n",
            "Epoch: 7 [Training: 19%]\tLoss: 0.075890\n",
            "Epoch: 7 [Training: 20%]\tLoss: 0.028709\n",
            "Epoch: 7 [Training: 21%]\tLoss: 0.071308\n",
            "Epoch: 7 [Training: 22%]\tLoss: 0.019882\n",
            "Epoch: 7 [Training: 23%]\tLoss: 0.004901\n",
            "Epoch: 7 [Training: 25%]\tLoss: 0.029999\n",
            "Epoch: 7 [Training: 26%]\tLoss: 0.027513\n",
            "Epoch: 7 [Training: 27%]\tLoss: 0.102506\n",
            "Epoch: 7 [Training: 28%]\tLoss: 0.080353\n",
            "Epoch: 7 [Training: 29%]\tLoss: 0.163230\n",
            "Epoch: 7 [Training: 30%]\tLoss: 0.015849\n",
            "Epoch: 7 [Training: 31%]\tLoss: 0.030652\n",
            "Epoch: 7 [Training: 32%]\tLoss: 0.091138\n",
            "Epoch: 7 [Training: 33%]\tLoss: 0.041317\n",
            "Epoch: 7 [Training: 34%]\tLoss: 0.067018\n",
            "Epoch: 7 [Training: 35%]\tLoss: 0.063134\n",
            "Epoch: 7 [Training: 36%]\tLoss: 0.235911\n",
            "Epoch: 7 [Training: 37%]\tLoss: 0.012818\n",
            "Epoch: 7 [Training: 38%]\tLoss: 0.014684\n",
            "Epoch: 7 [Training: 39%]\tLoss: 0.021896\n",
            "Epoch: 7 [Training: 41%]\tLoss: 0.013434\n",
            "Epoch: 7 [Training: 42%]\tLoss: 0.083485\n",
            "Epoch: 7 [Training: 43%]\tLoss: 0.034001\n",
            "Epoch: 7 [Training: 44%]\tLoss: 0.199663\n",
            "Epoch: 7 [Training: 45%]\tLoss: 0.021518\n",
            "Epoch: 7 [Training: 46%]\tLoss: 0.040660\n",
            "Epoch: 7 [Training: 47%]\tLoss: 0.005133\n",
            "Epoch: 7 [Training: 48%]\tLoss: 0.012901\n",
            "Epoch: 7 [Training: 49%]\tLoss: 0.009707\n",
            "Epoch: 7 [Training: 50%]\tLoss: 0.065735\n",
            "Epoch: 7 [Training: 51%]\tLoss: 0.009388\n",
            "Epoch: 7 [Training: 52%]\tLoss: 0.011764\n",
            "Epoch: 7 [Training: 53%]\tLoss: 0.010889\n",
            "Epoch: 7 [Training: 54%]\tLoss: 0.058238\n",
            "Epoch: 7 [Training: 55%]\tLoss: 0.008698\n",
            "Epoch: 7 [Training: 57%]\tLoss: 0.016098\n",
            "Epoch: 7 [Training: 58%]\tLoss: 0.024058\n",
            "Epoch: 7 [Training: 59%]\tLoss: 0.034820\n",
            "Epoch: 7 [Training: 60%]\tLoss: 0.140354\n",
            "Epoch: 7 [Training: 61%]\tLoss: 0.029954\n",
            "Epoch: 7 [Training: 62%]\tLoss: 0.008681\n",
            "Epoch: 7 [Training: 63%]\tLoss: 0.033407\n",
            "Epoch: 7 [Training: 64%]\tLoss: 0.043035\n",
            "Epoch: 7 [Training: 65%]\tLoss: 0.037521\n",
            "Epoch: 7 [Training: 66%]\tLoss: 0.177179\n",
            "Epoch: 7 [Training: 67%]\tLoss: 0.035829\n",
            "Epoch: 7 [Training: 68%]\tLoss: 0.068473\n",
            "Epoch: 7 [Training: 69%]\tLoss: 0.015905\n",
            "Epoch: 7 [Training: 70%]\tLoss: 0.012116\n",
            "Epoch: 7 [Training: 71%]\tLoss: 0.005309\n",
            "Epoch: 7 [Training: 72%]\tLoss: 0.009330\n",
            "Epoch: 7 [Training: 74%]\tLoss: 0.009162\n",
            "Epoch: 7 [Training: 75%]\tLoss: 0.056317\n",
            "Epoch: 7 [Training: 76%]\tLoss: 0.002745\n",
            "Epoch: 7 [Training: 77%]\tLoss: 0.059630\n",
            "Epoch: 7 [Training: 78%]\tLoss: 0.023628\n",
            "Epoch: 7 [Training: 79%]\tLoss: 0.088340\n",
            "Epoch: 7 [Training: 80%]\tLoss: 0.044709\n",
            "Epoch: 7 [Training: 81%]\tLoss: 0.131274\n",
            "Epoch: 7 [Training: 82%]\tLoss: 0.030067\n",
            "Epoch: 7 [Training: 83%]\tLoss: 0.070452\n",
            "Epoch: 7 [Training: 84%]\tLoss: 0.040097\n",
            "Epoch: 7 [Training: 85%]\tLoss: 0.247079\n",
            "Epoch: 7 [Training: 86%]\tLoss: 0.037277\n",
            "Epoch: 7 [Training: 87%]\tLoss: 0.030579\n",
            "Epoch: 7 [Training: 88%]\tLoss: 0.113283\n",
            "Epoch: 7 [Training: 90%]\tLoss: 0.033389\n",
            "Epoch: 7 [Training: 91%]\tLoss: 0.022107\n",
            "Epoch: 7 [Training: 92%]\tLoss: 0.044931\n",
            "Epoch: 7 [Training: 93%]\tLoss: 0.021567\n",
            "Epoch: 7 [Training: 94%]\tLoss: 0.082374\n",
            "Epoch: 7 [Training: 95%]\tLoss: 0.077737\n",
            "Epoch: 7 [Training: 96%]\tLoss: 0.176802\n",
            "Epoch: 7 [Training: 97%]\tLoss: 0.029829\n",
            "Epoch: 7 [Training: 98%]\tLoss: 0.071788\n",
            "Epoch: 7 [Training: 99%]\tLoss: 0.026903\n",
            "\n",
            "Test set: Average loss: 0.0446, Accuracy: 9862/10000 (99%)\n",
            "\n",
            "Epoch: 8 [Training: 0%]\tLoss: 0.056659\n",
            "Epoch: 8 [Training: 1%]\tLoss: 0.003103\n",
            "Epoch: 8 [Training: 2%]\tLoss: 0.008533\n",
            "Epoch: 8 [Training: 3%]\tLoss: 0.038179\n",
            "Epoch: 8 [Training: 4%]\tLoss: 0.011796\n",
            "Epoch: 8 [Training: 5%]\tLoss: 0.092766\n",
            "Epoch: 8 [Training: 6%]\tLoss: 0.075729\n",
            "Epoch: 8 [Training: 7%]\tLoss: 0.037842\n",
            "Epoch: 8 [Training: 9%]\tLoss: 0.062266\n",
            "Epoch: 8 [Training: 10%]\tLoss: 0.080880\n",
            "Epoch: 8 [Training: 11%]\tLoss: 0.019784\n",
            "Epoch: 8 [Training: 12%]\tLoss: 0.074304\n",
            "Epoch: 8 [Training: 13%]\tLoss: 0.017966\n",
            "Epoch: 8 [Training: 14%]\tLoss: 0.068654\n",
            "Epoch: 8 [Training: 15%]\tLoss: 0.008715\n",
            "Epoch: 8 [Training: 16%]\tLoss: 0.011010\n",
            "Epoch: 8 [Training: 17%]\tLoss: 0.008458\n",
            "Epoch: 8 [Training: 18%]\tLoss: 0.144776\n",
            "Epoch: 8 [Training: 19%]\tLoss: 0.032439\n",
            "Epoch: 8 [Training: 20%]\tLoss: 0.033375\n",
            "Epoch: 8 [Training: 21%]\tLoss: 0.010426\n",
            "Epoch: 8 [Training: 22%]\tLoss: 0.135810\n",
            "Epoch: 8 [Training: 23%]\tLoss: 0.059960\n",
            "Epoch: 8 [Training: 25%]\tLoss: 0.023616\n",
            "Epoch: 8 [Training: 26%]\tLoss: 0.008220\n",
            "Epoch: 8 [Training: 27%]\tLoss: 0.007761\n",
            "Epoch: 8 [Training: 28%]\tLoss: 0.002568\n",
            "Epoch: 8 [Training: 29%]\tLoss: 0.031869\n",
            "Epoch: 8 [Training: 30%]\tLoss: 0.014213\n",
            "Epoch: 8 [Training: 31%]\tLoss: 0.046544\n",
            "Epoch: 8 [Training: 32%]\tLoss: 0.024362\n",
            "Epoch: 8 [Training: 33%]\tLoss: 0.113858\n",
            "Epoch: 8 [Training: 34%]\tLoss: 0.030146\n",
            "Epoch: 8 [Training: 35%]\tLoss: 0.003008\n",
            "Epoch: 8 [Training: 36%]\tLoss: 0.030363\n",
            "Epoch: 8 [Training: 37%]\tLoss: 0.025760\n",
            "Epoch: 8 [Training: 38%]\tLoss: 0.094172\n",
            "Epoch: 8 [Training: 39%]\tLoss: 0.015257\n",
            "Epoch: 8 [Training: 41%]\tLoss: 0.073744\n",
            "Epoch: 8 [Training: 42%]\tLoss: 0.046612\n",
            "Epoch: 8 [Training: 43%]\tLoss: 0.158675\n",
            "Epoch: 8 [Training: 44%]\tLoss: 0.059743\n",
            "Epoch: 8 [Training: 45%]\tLoss: 0.002846\n",
            "Epoch: 8 [Training: 46%]\tLoss: 0.050142\n",
            "Epoch: 8 [Training: 47%]\tLoss: 0.044632\n",
            "Epoch: 8 [Training: 48%]\tLoss: 0.002485\n",
            "Epoch: 8 [Training: 49%]\tLoss: 0.022649\n",
            "Epoch: 8 [Training: 50%]\tLoss: 0.033663\n",
            "Epoch: 8 [Training: 51%]\tLoss: 0.041518\n",
            "Epoch: 8 [Training: 52%]\tLoss: 0.023581\n",
            "Epoch: 8 [Training: 53%]\tLoss: 0.194519\n",
            "Epoch: 8 [Training: 54%]\tLoss: 0.058854\n",
            "Epoch: 8 [Training: 55%]\tLoss: 0.023630\n",
            "Epoch: 8 [Training: 57%]\tLoss: 0.061561\n",
            "Epoch: 8 [Training: 58%]\tLoss: 0.016703\n",
            "Epoch: 8 [Training: 59%]\tLoss: 0.039166\n",
            "Epoch: 8 [Training: 60%]\tLoss: 0.025113\n",
            "Epoch: 8 [Training: 61%]\tLoss: 0.031350\n",
            "Epoch: 8 [Training: 62%]\tLoss: 0.027913\n",
            "Epoch: 8 [Training: 63%]\tLoss: 0.044543\n",
            "Epoch: 8 [Training: 64%]\tLoss: 0.068506\n",
            "Epoch: 8 [Training: 65%]\tLoss: 0.008879\n",
            "Epoch: 8 [Training: 66%]\tLoss: 0.012677\n",
            "Epoch: 8 [Training: 67%]\tLoss: 0.045703\n",
            "Epoch: 8 [Training: 68%]\tLoss: 0.115659\n",
            "Epoch: 8 [Training: 69%]\tLoss: 0.040274\n",
            "Epoch: 8 [Training: 70%]\tLoss: 0.046704\n",
            "Epoch: 8 [Training: 71%]\tLoss: 0.002334\n",
            "Epoch: 8 [Training: 72%]\tLoss: 0.051884\n",
            "Epoch: 8 [Training: 74%]\tLoss: 0.019379\n",
            "Epoch: 8 [Training: 75%]\tLoss: 0.146744\n",
            "Epoch: 8 [Training: 76%]\tLoss: 0.019365\n",
            "Epoch: 8 [Training: 77%]\tLoss: 0.055802\n",
            "Epoch: 8 [Training: 78%]\tLoss: 0.148652\n",
            "Epoch: 8 [Training: 79%]\tLoss: 0.007601\n",
            "Epoch: 8 [Training: 80%]\tLoss: 0.028705\n",
            "Epoch: 8 [Training: 81%]\tLoss: 0.114998\n",
            "Epoch: 8 [Training: 82%]\tLoss: 0.004698\n",
            "Epoch: 8 [Training: 83%]\tLoss: 0.014061\n",
            "Epoch: 8 [Training: 84%]\tLoss: 0.043863\n",
            "Epoch: 8 [Training: 85%]\tLoss: 0.021254\n",
            "Epoch: 8 [Training: 86%]\tLoss: 0.041628\n",
            "Epoch: 8 [Training: 87%]\tLoss: 0.063689\n",
            "Epoch: 8 [Training: 88%]\tLoss: 0.003844\n",
            "Epoch: 8 [Training: 90%]\tLoss: 0.037571\n",
            "Epoch: 8 [Training: 91%]\tLoss: 0.011502\n",
            "Epoch: 8 [Training: 92%]\tLoss: 0.019327\n",
            "Epoch: 8 [Training: 93%]\tLoss: 0.010487\n",
            "Epoch: 8 [Training: 94%]\tLoss: 0.036807\n",
            "Epoch: 8 [Training: 95%]\tLoss: 0.004992\n",
            "Epoch: 8 [Training: 96%]\tLoss: 0.081713\n",
            "Epoch: 8 [Training: 97%]\tLoss: 0.007085\n",
            "Epoch: 8 [Training: 98%]\tLoss: 0.012849\n",
            "Epoch: 8 [Training: 99%]\tLoss: 0.138579\n",
            "\n",
            "Test set: Average loss: 0.0362, Accuracy: 9886/10000 (99%)\n",
            "\n",
            "Epoch: 9 [Training: 0%]\tLoss: 0.005221\n",
            "Epoch: 9 [Training: 1%]\tLoss: 0.084376\n",
            "Epoch: 9 [Training: 2%]\tLoss: 0.061507\n",
            "Epoch: 9 [Training: 3%]\tLoss: 0.011011\n",
            "Epoch: 9 [Training: 4%]\tLoss: 0.094140\n",
            "Epoch: 9 [Training: 5%]\tLoss: 0.014418\n",
            "Epoch: 9 [Training: 6%]\tLoss: 0.019707\n",
            "Epoch: 9 [Training: 7%]\tLoss: 0.011745\n",
            "Epoch: 9 [Training: 9%]\tLoss: 0.013157\n",
            "Epoch: 9 [Training: 10%]\tLoss: 0.009392\n",
            "Epoch: 9 [Training: 11%]\tLoss: 0.010081\n",
            "Epoch: 9 [Training: 12%]\tLoss: 0.035735\n",
            "Epoch: 9 [Training: 13%]\tLoss: 0.062374\n",
            "Epoch: 9 [Training: 14%]\tLoss: 0.011431\n",
            "Epoch: 9 [Training: 15%]\tLoss: 0.011509\n",
            "Epoch: 9 [Training: 16%]\tLoss: 0.017745\n",
            "Epoch: 9 [Training: 17%]\tLoss: 0.014124\n",
            "Epoch: 9 [Training: 18%]\tLoss: 0.037441\n",
            "Epoch: 9 [Training: 19%]\tLoss: 0.070276\n",
            "Epoch: 9 [Training: 20%]\tLoss: 0.135452\n",
            "Epoch: 9 [Training: 21%]\tLoss: 0.026995\n",
            "Epoch: 9 [Training: 22%]\tLoss: 0.006879\n",
            "Epoch: 9 [Training: 23%]\tLoss: 0.015284\n",
            "Epoch: 9 [Training: 25%]\tLoss: 0.033568\n",
            "Epoch: 9 [Training: 26%]\tLoss: 0.021353\n",
            "Epoch: 9 [Training: 27%]\tLoss: 0.010064\n",
            "Epoch: 9 [Training: 28%]\tLoss: 0.072461\n",
            "Epoch: 9 [Training: 29%]\tLoss: 0.040855\n",
            "Epoch: 9 [Training: 30%]\tLoss: 0.097195\n",
            "Epoch: 9 [Training: 31%]\tLoss: 0.056922\n",
            "Epoch: 9 [Training: 32%]\tLoss: 0.022991\n",
            "Epoch: 9 [Training: 33%]\tLoss: 0.001831\n",
            "Epoch: 9 [Training: 34%]\tLoss: 0.011748\n",
            "Epoch: 9 [Training: 35%]\tLoss: 0.007182\n",
            "Epoch: 9 [Training: 36%]\tLoss: 0.009269\n",
            "Epoch: 9 [Training: 37%]\tLoss: 0.074973\n",
            "Epoch: 9 [Training: 38%]\tLoss: 0.071696\n",
            "Epoch: 9 [Training: 39%]\tLoss: 0.016536\n",
            "Epoch: 9 [Training: 41%]\tLoss: 0.010128\n",
            "Epoch: 9 [Training: 42%]\tLoss: 0.030944\n",
            "Epoch: 9 [Training: 43%]\tLoss: 0.034434\n",
            "Epoch: 9 [Training: 44%]\tLoss: 0.023989\n",
            "Epoch: 9 [Training: 45%]\tLoss: 0.034639\n",
            "Epoch: 9 [Training: 46%]\tLoss: 0.069216\n",
            "Epoch: 9 [Training: 47%]\tLoss: 0.026672\n",
            "Epoch: 9 [Training: 48%]\tLoss: 0.127856\n",
            "Epoch: 9 [Training: 49%]\tLoss: 0.015918\n",
            "Epoch: 9 [Training: 50%]\tLoss: 0.003149\n",
            "Epoch: 9 [Training: 51%]\tLoss: 0.044178\n",
            "Epoch: 9 [Training: 52%]\tLoss: 0.008925\n",
            "Epoch: 9 [Training: 53%]\tLoss: 0.002760\n",
            "Epoch: 9 [Training: 54%]\tLoss: 0.049925\n",
            "Epoch: 9 [Training: 55%]\tLoss: 0.097224\n",
            "Epoch: 9 [Training: 57%]\tLoss: 0.011822\n",
            "Epoch: 9 [Training: 58%]\tLoss: 0.028268\n",
            "Epoch: 9 [Training: 59%]\tLoss: 0.004451\n",
            "Epoch: 9 [Training: 60%]\tLoss: 0.029053\n",
            "Epoch: 9 [Training: 61%]\tLoss: 0.034851\n",
            "Epoch: 9 [Training: 62%]\tLoss: 0.042928\n",
            "Epoch: 9 [Training: 63%]\tLoss: 0.007711\n",
            "Epoch: 9 [Training: 64%]\tLoss: 0.010666\n",
            "Epoch: 9 [Training: 65%]\tLoss: 0.088333\n",
            "Epoch: 9 [Training: 66%]\tLoss: 0.005372\n",
            "Epoch: 9 [Training: 67%]\tLoss: 0.021169\n",
            "Epoch: 9 [Training: 68%]\tLoss: 0.024116\n",
            "Epoch: 9 [Training: 69%]\tLoss: 0.007750\n",
            "Epoch: 9 [Training: 70%]\tLoss: 0.031291\n",
            "Epoch: 9 [Training: 71%]\tLoss: 0.022429\n",
            "Epoch: 9 [Training: 72%]\tLoss: 0.037185\n",
            "Epoch: 9 [Training: 74%]\tLoss: 0.004419\n",
            "Epoch: 9 [Training: 75%]\tLoss: 0.006216\n",
            "Epoch: 9 [Training: 76%]\tLoss: 0.030606\n",
            "Epoch: 9 [Training: 77%]\tLoss: 0.049544\n",
            "Epoch: 9 [Training: 78%]\tLoss: 0.009908\n",
            "Epoch: 9 [Training: 79%]\tLoss: 0.156226\n",
            "Epoch: 9 [Training: 80%]\tLoss: 0.013750\n",
            "Epoch: 9 [Training: 81%]\tLoss: 0.026456\n",
            "Epoch: 9 [Training: 82%]\tLoss: 0.033159\n",
            "Epoch: 9 [Training: 83%]\tLoss: 0.006207\n",
            "Epoch: 9 [Training: 84%]\tLoss: 0.010933\n",
            "Epoch: 9 [Training: 85%]\tLoss: 0.014837\n",
            "Epoch: 9 [Training: 86%]\tLoss: 0.072524\n",
            "Epoch: 9 [Training: 87%]\tLoss: 0.020404\n",
            "Epoch: 9 [Training: 88%]\tLoss: 0.023534\n",
            "Epoch: 9 [Training: 90%]\tLoss: 0.019514\n",
            "Epoch: 9 [Training: 91%]\tLoss: 0.017296\n",
            "Epoch: 9 [Training: 92%]\tLoss: 0.013314\n",
            "Epoch: 9 [Training: 93%]\tLoss: 0.006574\n",
            "Epoch: 9 [Training: 94%]\tLoss: 0.074066\n",
            "Epoch: 9 [Training: 95%]\tLoss: 0.084104\n",
            "Epoch: 9 [Training: 96%]\tLoss: 0.010870\n",
            "Epoch: 9 [Training: 97%]\tLoss: 0.006524\n",
            "Epoch: 9 [Training: 98%]\tLoss: 0.039178\n",
            "Epoch: 9 [Training: 99%]\tLoss: 0.006324\n",
            "\n",
            "Test set: Average loss: 0.0344, Accuracy: 9891/10000 (99%)\n",
            "\n",
            "Epoch: 10 [Training: 0%]\tLoss: 0.004203\n",
            "Epoch: 10 [Training: 1%]\tLoss: 0.009176\n",
            "Epoch: 10 [Training: 2%]\tLoss: 0.011273\n",
            "Epoch: 10 [Training: 3%]\tLoss: 0.030270\n",
            "Epoch: 10 [Training: 4%]\tLoss: 0.004841\n",
            "Epoch: 10 [Training: 5%]\tLoss: 0.009510\n",
            "Epoch: 10 [Training: 6%]\tLoss: 0.029526\n",
            "Epoch: 10 [Training: 7%]\tLoss: 0.009872\n",
            "Epoch: 10 [Training: 9%]\tLoss: 0.020855\n",
            "Epoch: 10 [Training: 10%]\tLoss: 0.002077\n",
            "Epoch: 10 [Training: 11%]\tLoss: 0.056733\n",
            "Epoch: 10 [Training: 12%]\tLoss: 0.006966\n",
            "Epoch: 10 [Training: 13%]\tLoss: 0.088129\n",
            "Epoch: 10 [Training: 14%]\tLoss: 0.049797\n",
            "Epoch: 10 [Training: 15%]\tLoss: 0.031724\n",
            "Epoch: 10 [Training: 16%]\tLoss: 0.031981\n",
            "Epoch: 10 [Training: 17%]\tLoss: 0.037559\n",
            "Epoch: 10 [Training: 18%]\tLoss: 0.004699\n",
            "Epoch: 10 [Training: 19%]\tLoss: 0.037131\n",
            "Epoch: 10 [Training: 20%]\tLoss: 0.007529\n",
            "Epoch: 10 [Training: 21%]\tLoss: 0.002339\n",
            "Epoch: 10 [Training: 22%]\tLoss: 0.009185\n",
            "Epoch: 10 [Training: 23%]\tLoss: 0.018396\n",
            "Epoch: 10 [Training: 25%]\tLoss: 0.029989\n",
            "Epoch: 10 [Training: 26%]\tLoss: 0.002562\n",
            "Epoch: 10 [Training: 27%]\tLoss: 0.057425\n",
            "Epoch: 10 [Training: 28%]\tLoss: 0.103454\n",
            "Epoch: 10 [Training: 29%]\tLoss: 0.035339\n",
            "Epoch: 10 [Training: 30%]\tLoss: 0.014449\n",
            "Epoch: 10 [Training: 31%]\tLoss: 0.017456\n",
            "Epoch: 10 [Training: 32%]\tLoss: 0.146149\n",
            "Epoch: 10 [Training: 33%]\tLoss: 0.022699\n",
            "Epoch: 10 [Training: 34%]\tLoss: 0.006262\n",
            "Epoch: 10 [Training: 35%]\tLoss: 0.034385\n",
            "Epoch: 10 [Training: 36%]\tLoss: 0.018905\n",
            "Epoch: 10 [Training: 37%]\tLoss: 0.003387\n",
            "Epoch: 10 [Training: 38%]\tLoss: 0.189797\n",
            "Epoch: 10 [Training: 39%]\tLoss: 0.018028\n",
            "Epoch: 10 [Training: 41%]\tLoss: 0.015050\n",
            "Epoch: 10 [Training: 42%]\tLoss: 0.009782\n",
            "Epoch: 10 [Training: 43%]\tLoss: 0.023960\n",
            "Epoch: 10 [Training: 44%]\tLoss: 0.011002\n",
            "Epoch: 10 [Training: 45%]\tLoss: 0.012849\n",
            "Epoch: 10 [Training: 46%]\tLoss: 0.007212\n",
            "Epoch: 10 [Training: 47%]\tLoss: 0.026730\n",
            "Epoch: 10 [Training: 48%]\tLoss: 0.008088\n",
            "Epoch: 10 [Training: 49%]\tLoss: 0.011871\n",
            "Epoch: 10 [Training: 50%]\tLoss: 0.081546\n",
            "Epoch: 10 [Training: 51%]\tLoss: 0.013652\n",
            "Epoch: 10 [Training: 52%]\tLoss: 0.013216\n",
            "Epoch: 10 [Training: 53%]\tLoss: 0.022778\n",
            "Epoch: 10 [Training: 54%]\tLoss: 0.027806\n",
            "Epoch: 10 [Training: 55%]\tLoss: 0.096831\n",
            "Epoch: 10 [Training: 57%]\tLoss: 0.010408\n",
            "Epoch: 10 [Training: 58%]\tLoss: 0.033577\n",
            "Epoch: 10 [Training: 59%]\tLoss: 0.048404\n",
            "Epoch: 10 [Training: 60%]\tLoss: 0.005352\n",
            "Epoch: 10 [Training: 61%]\tLoss: 0.017015\n",
            "Epoch: 10 [Training: 62%]\tLoss: 0.009835\n",
            "Epoch: 10 [Training: 63%]\tLoss: 0.112933\n",
            "Epoch: 10 [Training: 64%]\tLoss: 0.028350\n",
            "Epoch: 10 [Training: 65%]\tLoss: 0.041417\n",
            "Epoch: 10 [Training: 66%]\tLoss: 0.030024\n",
            "Epoch: 10 [Training: 67%]\tLoss: 0.078473\n",
            "Epoch: 10 [Training: 68%]\tLoss: 0.084505\n",
            "Epoch: 10 [Training: 69%]\tLoss: 0.057242\n",
            "Epoch: 10 [Training: 70%]\tLoss: 0.014510\n",
            "Epoch: 10 [Training: 71%]\tLoss: 0.037008\n",
            "Epoch: 10 [Training: 72%]\tLoss: 0.016122\n",
            "Epoch: 10 [Training: 74%]\tLoss: 0.004908\n",
            "Epoch: 10 [Training: 75%]\tLoss: 0.068341\n",
            "Epoch: 10 [Training: 76%]\tLoss: 0.015999\n",
            "Epoch: 10 [Training: 77%]\tLoss: 0.004810\n",
            "Epoch: 10 [Training: 78%]\tLoss: 0.172246\n",
            "Epoch: 10 [Training: 79%]\tLoss: 0.007947\n",
            "Epoch: 10 [Training: 80%]\tLoss: 0.076611\n",
            "Epoch: 10 [Training: 81%]\tLoss: 0.010773\n",
            "Epoch: 10 [Training: 82%]\tLoss: 0.071406\n",
            "Epoch: 10 [Training: 83%]\tLoss: 0.043203\n",
            "Epoch: 10 [Training: 84%]\tLoss: 0.191546\n",
            "Epoch: 10 [Training: 85%]\tLoss: 0.008314\n",
            "Epoch: 10 [Training: 86%]\tLoss: 0.004271\n",
            "Epoch: 10 [Training: 87%]\tLoss: 0.009764\n",
            "Epoch: 10 [Training: 88%]\tLoss: 0.018046\n",
            "Epoch: 10 [Training: 90%]\tLoss: 0.100702\n",
            "Epoch: 10 [Training: 91%]\tLoss: 0.061744\n",
            "Epoch: 10 [Training: 92%]\tLoss: 0.051636\n",
            "Epoch: 10 [Training: 93%]\tLoss: 0.003043\n",
            "Epoch: 10 [Training: 94%]\tLoss: 0.026447\n",
            "Epoch: 10 [Training: 95%]\tLoss: 0.033198\n",
            "Epoch: 10 [Training: 96%]\tLoss: 0.013565\n",
            "Epoch: 10 [Training: 97%]\tLoss: 0.048527\n",
            "Epoch: 10 [Training: 98%]\tLoss: 0.107387\n",
            "Epoch: 10 [Training: 99%]\tLoss: 0.007521\n",
            "\n",
            "Test set: Average loss: 0.0376, Accuracy: 9872/10000 (99%)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MwiIz_xCeoVL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}