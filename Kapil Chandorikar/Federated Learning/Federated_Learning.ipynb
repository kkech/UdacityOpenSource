{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We’ll implement the federated learning approach to train a simple neural network on the MNIST dataset using the two workers: Jake and John."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Import the libraries and modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tf_encrypted:Falling back to insecure randomness since the required custom op could not be found for the installed version of TensorFlow (1.12.0). Fix this by compiling custom ops.\n",
      "WARNING:syft:Keras (Tensorflow) not available.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import syft as sy\n",
    "import torchvision\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initializing the hook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "hook = sy.TorchHook(torch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is done to override PyTorch’s methods to execute commands on one worker that are called on tensors controlled by the local worker. It also allows us to move tensors between workers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Create virtual workers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "jake = sy.VirtualWorker(hook, id=\"jake\")\n",
    "john = sy.VirtualWorker(hook, id=\"john\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Virtual workers are entities present on our local machine. They are used to model the behavior of actual workers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, ), (0.5, )),\n",
    "])\n",
    "\n",
    "train_set = datasets.MNIST(\n",
    "    \"~/.pytorch/MNIST_data/\", train=True, download=True, transform=transform)\n",
    "test_set = datasets.MNIST(\n",
    "    \"~/.pytorch/MNIST_data/\", train=False, download=True, transform=transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In real-life applications, the data is present on client devices. To replicate the scenario, we send data to the VirtualWorkers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "federated_train_loader = sy.FederatedDataLoader(\n",
    "    train_set.federate((jake, john)), batch_size=64, shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_set, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that we have created the training dataset differently. The `train_set.federate((jake, john))` creates a _FederatedDataset_ wherein the train_set is split among Jake and John (our two VirtualWorkers). The _FederatedDataset_ class is intended to be used like the PyTorch’s _Dataset_ class. Pass the created _FederatedDataset_ to a federated data loader _FederatedDataLoader_ to iterate over it in a federated manner. The batches then come from different devices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Build the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        self.fc1 = nn.Linear(784, 500)\n",
    "        self.fc2 = nn.Linear(500, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 784)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "\n",
    "model = Model()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Train the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the data is present on the client device, we obtain its location through the location attribute. The important additions to the code are the steps to get back the improved model and the value of the loss from the client devices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  1 [    0/60032 (  0%)]\tLoss: 2.354304\n",
      "Epoch:  1 [ 6400/60032 ( 11%)]\tLoss: 1.327208\n",
      "Epoch:  1 [12800/60032 ( 21%)]\tLoss: 0.842845\n",
      "Epoch:  1 [19200/60032 ( 32%)]\tLoss: 0.687354\n",
      "Epoch:  1 [25600/60032 ( 43%)]\tLoss: 0.611549\n",
      "Epoch:  1 [32000/60032 ( 53%)]\tLoss: 0.485309\n",
      "Epoch:  1 [38400/60032 ( 64%)]\tLoss: 0.494988\n",
      "Epoch:  1 [44800/60032 ( 75%)]\tLoss: 0.476802\n",
      "Epoch:  1 [51200/60032 ( 85%)]\tLoss: 0.443146\n",
      "Epoch:  1 [57600/60032 ( 96%)]\tLoss: 0.525410\n",
      "Epoch:  2 [    0/60032 (  0%)]\tLoss: 0.465071\n",
      "Epoch:  2 [ 6400/60032 ( 11%)]\tLoss: 0.527767\n",
      "Epoch:  2 [12800/60032 ( 21%)]\tLoss: 0.302260\n",
      "Epoch:  2 [19200/60032 ( 32%)]\tLoss: 0.468767\n",
      "Epoch:  2 [25600/60032 ( 43%)]\tLoss: 0.353053\n",
      "Epoch:  2 [32000/60032 ( 53%)]\tLoss: 0.211528\n",
      "Epoch:  2 [38400/60032 ( 64%)]\tLoss: 0.373569\n",
      "Epoch:  2 [44800/60032 ( 75%)]\tLoss: 0.294314\n",
      "Epoch:  2 [51200/60032 ( 85%)]\tLoss: 0.273752\n",
      "Epoch:  2 [57600/60032 ( 96%)]\tLoss: 0.224646\n",
      "Epoch:  3 [    0/60032 (  0%)]\tLoss: 0.235165\n",
      "Epoch:  3 [ 6400/60032 ( 11%)]\tLoss: 0.354446\n",
      "Epoch:  3 [12800/60032 ( 21%)]\tLoss: 0.240451\n",
      "Epoch:  3 [19200/60032 ( 32%)]\tLoss: 0.346054\n",
      "Epoch:  3 [25600/60032 ( 43%)]\tLoss: 0.198855\n",
      "Epoch:  3 [32000/60032 ( 53%)]\tLoss: 0.264971\n",
      "Epoch:  3 [38400/60032 ( 64%)]\tLoss: 0.278613\n",
      "Epoch:  3 [44800/60032 ( 75%)]\tLoss: 0.404485\n",
      "Epoch:  3 [51200/60032 ( 85%)]\tLoss: 0.277293\n",
      "Epoch:  3 [57600/60032 ( 96%)]\tLoss: 0.197943\n",
      "Epoch:  4 [    0/60032 (  0%)]\tLoss: 0.482322\n",
      "Epoch:  4 [ 6400/60032 ( 11%)]\tLoss: 0.201497\n",
      "Epoch:  4 [12800/60032 ( 21%)]\tLoss: 0.537550\n",
      "Epoch:  4 [19200/60032 ( 32%)]\tLoss: 0.243341\n",
      "Epoch:  4 [25600/60032 ( 43%)]\tLoss: 0.367303\n",
      "Epoch:  4 [32000/60032 ( 53%)]\tLoss: 0.239866\n",
      "Epoch:  4 [38400/60032 ( 64%)]\tLoss: 0.317137\n",
      "Epoch:  4 [44800/60032 ( 75%)]\tLoss: 0.239223\n",
      "Epoch:  4 [51200/60032 ( 85%)]\tLoss: 0.193008\n",
      "Epoch:  4 [57600/60032 ( 96%)]\tLoss: 0.256767\n",
      "Epoch:  5 [    0/60032 (  0%)]\tLoss: 0.151723\n",
      "Epoch:  5 [ 6400/60032 ( 11%)]\tLoss: 0.249813\n",
      "Epoch:  5 [12800/60032 ( 21%)]\tLoss: 0.383207\n",
      "Epoch:  5 [19200/60032 ( 32%)]\tLoss: 0.204487\n",
      "Epoch:  5 [25600/60032 ( 43%)]\tLoss: 0.301314\n",
      "Epoch:  5 [32000/60032 ( 53%)]\tLoss: 0.265877\n",
      "Epoch:  5 [38400/60032 ( 64%)]\tLoss: 0.316862\n",
      "Epoch:  5 [44800/60032 ( 75%)]\tLoss: 0.227827\n",
      "Epoch:  5 [51200/60032 ( 85%)]\tLoss: 0.208022\n",
      "Epoch:  5 [57600/60032 ( 96%)]\tLoss: 0.248349\n",
      "Epoch:  6 [    0/60032 (  0%)]\tLoss: 0.355509\n",
      "Epoch:  6 [ 6400/60032 ( 11%)]\tLoss: 0.162790\n",
      "Epoch:  6 [12800/60032 ( 21%)]\tLoss: 0.294868\n",
      "Epoch:  6 [19200/60032 ( 32%)]\tLoss: 0.279968\n",
      "Epoch:  6 [25600/60032 ( 43%)]\tLoss: 0.172214\n",
      "Epoch:  6 [32000/60032 ( 53%)]\tLoss: 0.217400\n",
      "Epoch:  6 [38400/60032 ( 64%)]\tLoss: 0.356856\n",
      "Epoch:  6 [44800/60032 ( 75%)]\tLoss: 0.178677\n",
      "Epoch:  6 [51200/60032 ( 85%)]\tLoss: 0.188243\n",
      "Epoch:  6 [57600/60032 ( 96%)]\tLoss: 0.349360\n",
      "Epoch:  7 [    0/60032 (  0%)]\tLoss: 0.366114\n",
      "Epoch:  7 [ 6400/60032 ( 11%)]\tLoss: 0.226837\n",
      "Epoch:  7 [12800/60032 ( 21%)]\tLoss: 0.291263\n",
      "Epoch:  7 [19200/60032 ( 32%)]\tLoss: 0.292544\n",
      "Epoch:  7 [25600/60032 ( 43%)]\tLoss: 0.220504\n",
      "Epoch:  7 [32000/60032 ( 53%)]\tLoss: 0.321393\n",
      "Epoch:  7 [38400/60032 ( 64%)]\tLoss: 0.078403\n",
      "Epoch:  7 [44800/60032 ( 75%)]\tLoss: 0.239258\n",
      "Epoch:  7 [51200/60032 ( 85%)]\tLoss: 0.218696\n",
      "Epoch:  7 [57600/60032 ( 96%)]\tLoss: 0.268840\n",
      "Epoch:  8 [    0/60032 (  0%)]\tLoss: 0.228177\n",
      "Epoch:  8 [ 6400/60032 ( 11%)]\tLoss: 0.240549\n",
      "Epoch:  8 [12800/60032 ( 21%)]\tLoss: 0.157163\n",
      "Epoch:  8 [19200/60032 ( 32%)]\tLoss: 0.105103\n",
      "Epoch:  8 [25600/60032 ( 43%)]\tLoss: 0.167602\n",
      "Epoch:  8 [32000/60032 ( 53%)]\tLoss: 0.149110\n",
      "Epoch:  8 [38400/60032 ( 64%)]\tLoss: 0.194888\n",
      "Epoch:  8 [44800/60032 ( 75%)]\tLoss: 0.209314\n",
      "Epoch:  8 [51200/60032 ( 85%)]\tLoss: 0.246336\n",
      "Epoch:  8 [57600/60032 ( 96%)]\tLoss: 0.198457\n",
      "Epoch:  9 [    0/60032 (  0%)]\tLoss: 0.090303\n",
      "Epoch:  9 [ 6400/60032 ( 11%)]\tLoss: 0.167137\n",
      "Epoch:  9 [12800/60032 ( 21%)]\tLoss: 0.143732\n",
      "Epoch:  9 [19200/60032 ( 32%)]\tLoss: 0.364659\n",
      "Epoch:  9 [25600/60032 ( 43%)]\tLoss: 0.349170\n",
      "Epoch:  9 [32000/60032 ( 53%)]\tLoss: 0.176962\n",
      "Epoch:  9 [38400/60032 ( 64%)]\tLoss: 0.144449\n",
      "Epoch:  9 [44800/60032 ( 75%)]\tLoss: 0.105261\n",
      "Epoch:  9 [51200/60032 ( 85%)]\tLoss: 0.105341\n",
      "Epoch:  9 [57600/60032 ( 96%)]\tLoss: 0.197013\n",
      "Epoch: 10 [    0/60032 (  0%)]\tLoss: 0.244392\n",
      "Epoch: 10 [ 6400/60032 ( 11%)]\tLoss: 0.295712\n",
      "Epoch: 10 [12800/60032 ( 21%)]\tLoss: 0.176596\n",
      "Epoch: 10 [19200/60032 ( 32%)]\tLoss: 0.159424\n",
      "Epoch: 10 [25600/60032 ( 43%)]\tLoss: 0.323807\n",
      "Epoch: 10 [32000/60032 ( 53%)]\tLoss: 0.188543\n",
      "Epoch: 10 [38400/60032 ( 64%)]\tLoss: 0.128433\n",
      "Epoch: 10 [44800/60032 ( 75%)]\tLoss: 0.188250\n",
      "Epoch: 10 [51200/60032 ( 85%)]\tLoss: 0.069927\n",
      "Epoch: 10 [57600/60032 ( 96%)]\tLoss: 0.210007\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(0, 10):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(federated_train_loader):\n",
    "        \n",
    "        # send the model to the client device where the data is present\n",
    "        model.send(data.location)\n",
    "        \n",
    "        # training the model\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # get back the improved model\n",
    "        model.get()\n",
    "        \n",
    "        if batch_idx % 100 == 0:\n",
    "            \n",
    "            # get back the loss\n",
    "            loss = loss.get()\n",
    "            \n",
    "            print('Epoch: {:2d} [{:5d}/{:5d} ({:3.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch + 1, batch_idx * 64,\n",
    "                len(federated_train_loader) * 64,\n",
    "                100. * batch_idx / len(federated_train_loader), loss.item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### 6. Test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set: Average loss: 0.1781, Accuracy: 9484/10000 (95%)\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "test_loss = 0\n",
    "correct = 0\n",
    "with torch.no_grad():\n",
    "    for data, target in test_loader:\n",
    "        \n",
    "        output = model(data)\n",
    "        test_loss += F.nll_loss(output, target, reduction='sum').item()\n",
    "        \n",
    "        # get the index of the max log-probability\n",
    "        pred = output.argmax(1, keepdim=True)\n",
    "        correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "test_loss /= len(test_loader.dataset)\n",
    "\n",
    "print('Test set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)'.format(\n",
    "    test_loss, correct, len(test_loader.dataset),\n",
    "    100. * correct / len(test_loader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_names = [str(x) for x in range(0, 10)]\n",
    "label_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_correct = list(0. for i in range(10))\n",
    "class_total = list(0. for i in range(10))\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data in test_loader:\n",
    "        images, labels = data\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        c = (predicted == labels).squeeze()\n",
    "        \n",
    "        for i in range(10):\n",
    "            label = labels[i]\n",
    "            class_correct[label] += c[i].item()\n",
    "            class_total[label] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracies\n",
      "    0 : 99 %\n",
      "    1 : 99 %\n",
      "    2 : 94 %\n",
      "    3 : 94 %\n",
      "    4 : 90 %\n",
      "    5 : 96 %\n",
      "    6 : 97 %\n",
      "    7 : 94 %\n",
      "    8 : 94 %\n",
      "    9 : 94 %\n"
     ]
    }
   ],
   "source": [
    "print(\"Test Accuracies\")\n",
    "for i in range(10):\n",
    "    print('%5s : %2d %%' % (label_names[i],\n",
    "                            100 * class_correct[i] / class_total[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That’s it. We have trained a model using the federated learning approach. When compared to traditional training, it takes more time to train a model using the federated approach."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 GPU",
   "language": "python",
   "name": "tf-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
